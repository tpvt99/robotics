from hw5_tf2.logger import loggerfrom hw5_tf2.utils.serializable import Serializableimport numpy as npimport tensorflow as tffrom hw5_tf2.utils import utilsclass FirstOrderOptimizer(Serializable):    """    Optimizer for first order methods (SGD, Adam)    Args:        tf_optimizer_cls (tf.train.optimizer): desired tensorflow optimzier for training        tf_optimizer_args (dict or None): arguments for the optimizer        learning_rate (float): learning rate        max_epochs: number of maximum epochs for training        tolerance (float): tolerance for early stopping. If the loss fucntion decreases less than the specified tolerance        after an epoch, then the training stops.        num_minibatches (int): number of mini-batches for performing the gradient step. The mini-batch size is        batch size//num_minibatches.        verbose (bool): Whether to log or not the optimization process    """    def __init__(            self,            tf_optimizer_cls=tf.keras.optimizers.Adam,            tf_optimizer_args=None,            learning_rate=1e-3,            max_epochs=1,            tolerance=1e-6,            num_minibatches=1,            verbose=False    ):        Serializable.quick_init(self, locals())        self._policy = None        if tf_optimizer_args is None:            tf_optimizer_args = dict()        tf_optimizer_args['learning_rate'] = learning_rate        self._tf_optimizer = tf_optimizer_cls(**tf_optimizer_args)        self._max_epochs = max_epochs        self._tolerance = tolerance        self._verbose = verbose        self._num_minibatches = num_minibatches        self._loss_object = None    def build_optimizer(self, loss, target, *args, **kwargs):        """        Sets the objective function and target weights for the optimize function        Args:            loss (tf_op) : minimization objective            target (policy) : Policy whose values we are optimizing over            input_ph_dict (dict) : dict containing the placeholders of the computation graph corresponding to loss        """        assert isinstance(loss, tf.keras.losses.Loss)        assert hasattr(target, 'get_params')        self._policy = target        self._loss_object = loss    '''    def loss(self, input_val_dict):        """        Computes the value of the loss for given inputs        Args:            input_val_dict (dict): dict containing the values to be fed into the computation graph        Returns:            (float): value of the loss        """        feed_dict = self.create_feed_dict(input_val_dict)        loss = sess.run(self._loss, feed_dict=feed_dict)        return loss    '''    def loss(self, input_val_dict):        pass    def optimize(self, observations, actions):        """        Carries out the optimization step        Args:            observations (tf.Tensor or numpy): dict containing the values to be fed into the computation graph        Returns:            (float) loss before optimization        """        # Todo: reimplement minibatches        loss_before_opt = None        for epoch in range(self._max_epochs):            if self._verbose:                logger.log("Epoch %d" % epoch)            with tf.GradientTape() as tape:                predicted_actions = self._policy(observations)                loss = self._loss_object(predicted_actions, actions)            gradients = tape.gradient(loss, self._policy.get_params()) # or _policy.trainable_variables            self._tf_optimizer.apply_gradients(zip(gradients, self._policy.get_params()))            if not loss_before_opt:                loss_before_opt = loss        return loss_before_opt    '''    def create_feed_dict(self, input_val_dict):        return utils.create_feed_dict(placeholder_dict=self._input_ph_dict, value_dict=input_val_dict)    '''    def create_feed_dict(self, input_val_dict):        pass