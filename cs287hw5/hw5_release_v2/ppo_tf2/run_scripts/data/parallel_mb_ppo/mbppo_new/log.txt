Logging to /home/tpvt96/ai_course/robotics/cs287hw5/hw5_release_v2/ppo_tf2/run_scripts/data/parallel_mb_ppo/mbppo_new/

 ---------------- Iteration 0 ----------------
Obtaining random samples from the environment...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 4.1473  valid loss: 3.4655  valid_loss_mov_avg: 5.1809
Training NNDynamicsModel - finished epoch 1 -- train loss: 2.5531  valid loss: 3.3109  valid_loss_mov_avg: 5.1622
Training NNDynamicsModel - finished epoch 2 -- train loss: 1.9796  valid loss: 3.3654  valid_loss_mov_avg: 5.1443
Training NNDynamicsModel - finished epoch 3 -- train loss: 1.7655  valid loss: 3.4041  valid_loss_mov_avg: 5.1269
Training NNDynamicsModel - finished epoch 4 -- train loss: 1.4151  valid loss: 3.3621  valid_loss_mov_avg: 5.1092
Training NNDynamicsModel - finished epoch 5 -- train loss: 1.1448  valid loss: 3.3333  valid_loss_mov_avg: 5.0915
Training NNDynamicsModel - finished epoch 6 -- train loss: 1.0413  valid loss: 3.3263  valid_loss_mov_avg: 5.0738
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.9903  valid loss: 3.3134  valid_loss_mov_avg: 5.0562
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.8704  valid loss: 3.2833  valid_loss_mov_avg: 5.0385
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.8174  valid loss: 3.2728  valid_loss_mov_avg: 5.0208
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.7523  valid loss: 3.2680  valid_loss_mov_avg: 5.0033
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.9123  valid loss: 3.2321  valid_loss_mov_avg: 4.9856
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.8595  valid loss: 3.2257  valid_loss_mov_avg: 4.9680
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.8570  valid loss: 3.2195  valid_loss_mov_avg: 4.9505
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.7166  valid loss: 3.1827  valid_loss_mov_avg: 4.9328
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.6378  valid loss: 3.1783  valid_loss_mov_avg: 4.9153
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.5926  valid loss: 3.1606  valid_loss_mov_avg: 4.8977
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.4998  valid loss: 3.1392  valid_loss_mov_avg: 4.8801
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.5957  valid loss: 3.1330  valid_loss_mov_avg: 4.8627
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.5232  valid loss: 3.0991  valid_loss_mov_avg: 4.8450
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.5452  valid loss: 3.1034  valid_loss_mov_avg: 4.8276
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.5166  valid loss: 3.0764  valid_loss_mov_avg: 4.8101
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.4853  valid loss: 3.0538  valid_loss_mov_avg: 4.7925
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.4516  valid loss: 3.0452  valid_loss_mov_avg: 4.7751
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.4751  valid loss: 3.0250  valid_loss_mov_avg: 4.7576
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3936  valid loss: 3.0020  valid_loss_mov_avg: 4.7400
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3935  valid loss: 2.9917  valid_loss_mov_avg: 4.7225
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3891  valid loss: 2.9701  valid_loss_mov_avg: 4.7050
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.4489  valid loss: 2.9481  valid_loss_mov_avg: 4.6874
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.4372  valid loss: 2.9417  valid_loss_mov_avg: 4.6700
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.4814  valid loss: 2.9093  valid_loss_mov_avg: 4.6524
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3362  valid loss: 2.8954  valid_loss_mov_avg: 4.6348
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3113  valid loss: 2.8723  valid_loss_mov_avg: 4.6172
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3849  valid loss: 2.8603  valid_loss_mov_avg: 4.5996
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.4145  valid loss: 2.8315  valid_loss_mov_avg: 4.5819
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3474  valid loss: 2.8154  valid_loss_mov_avg: 4.5643
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.5447  valid loss: 2.7950  valid_loss_mov_avg: 4.5466
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.4875  valid loss: 2.7702  valid_loss_mov_avg: 4.5288
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3699  valid loss: 2.7533  valid_loss_mov_avg: 4.5111
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.5015  valid loss: 2.7268  valid_loss_mov_avg: 4.4932
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.4283  valid loss: 2.7072  valid_loss_mov_avg: 4.4754
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3532  valid loss: 2.6839  valid_loss_mov_avg: 4.4574
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3698  valid loss: 2.6685  valid_loss_mov_avg: 4.4395
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.5002  valid loss: 2.6421  valid_loss_mov_avg: 4.4216
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.5177  valid loss: 2.6219  valid_loss_mov_avg: 4.4036
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.4034  valid loss: 2.5922  valid_loss_mov_avg: 4.3855
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3569  valid loss: 2.5802  valid_loss_mov_avg: 4.3674
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3599  valid loss: 2.5528  valid_loss_mov_avg: 4.3493
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3333  valid loss: 2.5431  valid_loss_mov_avg: 4.3312
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3700  valid loss: 2.4941  valid_loss_mov_avg: 4.3128

 ---------------- Grad-Step 0 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 1 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 2 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 3 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 4 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 5 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 6 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 7 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 8 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 9 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.155         |
| Data-EnvSampler-Poli... | 0.393         |
| Data-EnvTrajs-Averag... | -68.1         |
| Data-EnvTrajs-MaxReturn | -41.7         |
| Data-EnvTrajs-MinReturn | -84.2         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 15.1          |
| Data-TimeEnvSampleProc  | 0.000558      |
| Data-TimeEnvSampling    | 0.562         |
| Iteration               | 0             |
| ItrTime                 | 68.1          |
| LossAfter               | -0.020045495  |
| LossBefore              | 2.4437905e-09 |
| Model-TimeModelFit      | 3.65          |
| ModelSampler-n_times... | 40000         |
| Policy-AverageAbsPol... | 0.73005885    |
| Policy-AverageDiscou... | 68.4          |
| Policy-AveragePolicyStd | 1.0034077     |
| Policy-AverageReturn    | 739           |
| Policy-MaxReturn        | 1.1e+04       |
| Policy-MinReturn        | -8.48e+03     |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 3.8e+03       |
| Policy-TimeAlgoOpt      | 1.14          |
| Policy-TimeSampleProc   | 0.153         |
| Policy-TimeSampling     | 62.5          |
| Policy-TimeStep         | 63.8          |
| Time                    | 68.1          |
| n_timesteps             | 1000          |
-------------------------------------------

 ---------------- Iteration 1 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 1.7397  valid loss: 2.7308  valid_loss_mov_avg: 4.0826
Training NNDynamicsModel - finished epoch 1 -- train loss: 1.5324  valid loss: 2.6493  valid_loss_mov_avg: 4.0682
Training NNDynamicsModel - finished epoch 2 -- train loss: 1.3450  valid loss: 2.6040  valid_loss_mov_avg: 4.0536
Training NNDynamicsModel - finished epoch 3 -- train loss: 1.2390  valid loss: 2.5090  valid_loss_mov_avg: 4.0381
Training NNDynamicsModel - finished epoch 4 -- train loss: 1.1485  valid loss: 2.5010  valid_loss_mov_avg: 4.0228
Training NNDynamicsModel - finished epoch 5 -- train loss: 1.0502  valid loss: 2.4381  valid_loss_mov_avg: 4.0069
Training NNDynamicsModel - finished epoch 6 -- train loss: 1.0183  valid loss: 2.4079  valid_loss_mov_avg: 3.9909
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.9845  valid loss: 2.3682  valid_loss_mov_avg: 3.9747
Training NNDynamicsModel - finished epoch 8 -- train loss: 1.0541  valid loss: 2.3946  valid_loss_mov_avg: 3.9589
Training NNDynamicsModel - finished epoch 9 -- train loss: 1.0112  valid loss: 2.2588  valid_loss_mov_avg: 3.9419
Training NNDynamicsModel - finished epoch 10 -- train loss: 1.0517  valid loss: 2.3138  valid_loss_mov_avg: 3.9256
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.9627  valid loss: 2.1780  valid_loss_mov_avg: 3.9081
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.8829  valid loss: 2.2648  valid_loss_mov_avg: 3.8917
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.8256  valid loss: 2.1230  valid_loss_mov_avg: 3.8740
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.7716  valid loss: 2.1421  valid_loss_mov_avg: 3.8567
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.7656  valid loss: 2.0465  valid_loss_mov_avg: 3.8386
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.7010  valid loss: 2.0815  valid_loss_mov_avg: 3.8210
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.7382  valid loss: 2.0044  valid_loss_mov_avg: 3.8029
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.7023  valid loss: 1.9762  valid_loss_mov_avg: 3.7846
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.6264  valid loss: 1.9868  valid_loss_mov_avg: 3.7666
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.6201  valid loss: 1.8764  valid_loss_mov_avg: 3.7477
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.6472  valid loss: 1.9655  valid_loss_mov_avg: 3.7299
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.5868  valid loss: 1.8060  valid_loss_mov_avg: 3.7107
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.6696  valid loss: 1.9326  valid_loss_mov_avg: 3.6929
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.6495  valid loss: 1.7677  valid_loss_mov_avg: 3.6736
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.7258  valid loss: 1.8593  valid_loss_mov_avg: 3.6555
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.6141  valid loss: 1.7584  valid_loss_mov_avg: 3.6365
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.6157  valid loss: 1.7948  valid_loss_mov_avg: 3.6181
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.6088  valid loss: 1.7382  valid_loss_mov_avg: 3.5993
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.5392  valid loss: 1.7248  valid_loss_mov_avg: 3.5806
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.5536  valid loss: 1.7345  valid_loss_mov_avg: 3.5621
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.4929  valid loss: 1.7139  valid_loss_mov_avg: 3.5436
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.5140  valid loss: 1.6950  valid_loss_mov_avg: 3.5251
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.4953  valid loss: 1.7059  valid_loss_mov_avg: 3.5069
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.4757  valid loss: 1.6681  valid_loss_mov_avg: 3.4885
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.4380  valid loss: 1.6760  valid_loss_mov_avg: 3.4704
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.4663  valid loss: 1.6642  valid_loss_mov_avg: 3.4524
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.4778  valid loss: 1.6668  valid_loss_mov_avg: 3.4345
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.4619  valid loss: 1.6569  valid_loss_mov_avg: 3.4167
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.4242  valid loss: 1.6418  valid_loss_mov_avg: 3.3990
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.4278  valid loss: 1.6434  valid_loss_mov_avg: 3.3814
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.4978  valid loss: 1.6394  valid_loss_mov_avg: 3.3640
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.4714  valid loss: 1.6342  valid_loss_mov_avg: 3.3467
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.4792  valid loss: 1.6338  valid_loss_mov_avg: 3.3296
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.4561  valid loss: 1.6320  valid_loss_mov_avg: 3.3126
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.4364  valid loss: 1.6198  valid_loss_mov_avg: 3.2957
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.4865  valid loss: 1.6171  valid_loss_mov_avg: 3.2789
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3994  valid loss: 1.6260  valid_loss_mov_avg: 3.2624
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3909  valid loss: 1.6068  valid_loss_mov_avg: 3.2458
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.4196  valid loss: 1.6241  valid_loss_mov_avg: 3.2296

 ---------------- Grad-Step 10 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 11 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 12 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 13 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 14 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 15 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 16 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 17 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 18 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 19 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.416          |
| Data-EnvSampler-Poli... | 2.05           |
| Data-EnvTrajs-Averag... | -48.8          |
| Data-EnvTrajs-MaxReturn | 12.4           |
| Data-EnvTrajs-MinReturn | -122           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 45.1           |
| Data-TimeEnvSampleProc  | 0.000559       |
| Data-TimeEnvSampling    | 3.53           |
| Iteration               | 1              |
| ItrTime                 | 74.1           |
| LossAfter               | -0.016127337   |
| LossBefore              | -1.6689301e-09 |
| Model-TimeModelFit      | 8.38           |
| ModelSampler-n_times... | 80000          |
| Policy-AverageAbsPol... | 0.7003544      |
| Policy-AverageDiscou... | -38            |
| Policy-AveragePolicyStd | 0.98242944     |
| Policy-AverageReturn    | -85.7          |
| Policy-MaxReturn        | -18            |
| Policy-MinReturn        | -157           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 33.8           |
| Policy-TimeAlgoOpt      | 1.18           |
| Policy-TimeSampleProc   | 0.138          |
| Policy-TimeSampling     | 60.9           |
| Policy-TimeStep         | 62.2           |
| Time                    | 142            |
| n_timesteps             | 2000           |
--------------------------------------------

 ---------------- Iteration 2 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.9973  valid loss: 1.6184  valid_loss_mov_avg: 2.4195
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.9013  valid loss: 1.5882  valid_loss_mov_avg: 2.4112
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.8570  valid loss: 1.5903  valid_loss_mov_avg: 2.4030
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.8335  valid loss: 1.5809  valid_loss_mov_avg: 2.3947
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.7787  valid loss: 1.5616  valid_loss_mov_avg: 2.3864
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.7140  valid loss: 1.5465  valid_loss_mov_avg: 2.3780
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.6981  valid loss: 1.5521  valid_loss_mov_avg: 2.3697
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.6860  valid loss: 1.5524  valid_loss_mov_avg: 2.3616
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.7080  valid loss: 1.5466  valid_loss_mov_avg: 2.3534
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.6406  valid loss: 1.5279  valid_loss_mov_avg: 2.3452
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.6248  valid loss: 1.5339  valid_loss_mov_avg: 2.3371
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.5748  valid loss: 1.5204  valid_loss_mov_avg: 2.3289
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.5535  valid loss: 1.5221  valid_loss_mov_avg: 2.3208
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.5676  valid loss: 1.5237  valid_loss_mov_avg: 2.3129
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.5583  valid loss: 1.5283  valid_loss_mov_avg: 2.3050
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.5439  valid loss: 1.5120  valid_loss_mov_avg: 2.2971
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.5359  valid loss: 1.5361  valid_loss_mov_avg: 2.2895
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.5079  valid loss: 1.5025  valid_loss_mov_avg: 2.2816
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.4701  valid loss: 1.5086  valid_loss_mov_avg: 2.2739
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.5090  valid loss: 1.5163  valid_loss_mov_avg: 2.2663
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.4978  valid loss: 1.5305  valid_loss_mov_avg: 2.2589
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.4973  valid loss: 1.5091  valid_loss_mov_avg: 2.2514
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.5076  valid loss: 1.5318  valid_loss_mov_avg: 2.2442
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.4972  valid loss: 1.5057  valid_loss_mov_avg: 2.2369
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.5255  valid loss: 1.5194  valid_loss_mov_avg: 2.2297
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.5175  valid loss: 1.5176  valid_loss_mov_avg: 2.2226
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.5022  valid loss: 1.5079  valid_loss_mov_avg: 2.2154
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.4714  valid loss: 1.5096  valid_loss_mov_avg: 2.2084
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.4701  valid loss: 1.5153  valid_loss_mov_avg: 2.2014
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.4544  valid loss: 1.5185  valid_loss_mov_avg: 2.1946
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.4097  valid loss: 1.4964  valid_loss_mov_avg: 2.1876
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3841  valid loss: 1.4996  valid_loss_mov_avg: 2.1807
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3876  valid loss: 1.5141  valid_loss_mov_avg: 2.1741
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.4149  valid loss: 1.5169  valid_loss_mov_avg: 2.1675
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.4004  valid loss: 1.5031  valid_loss_mov_avg: 2.1609
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3754  valid loss: 1.5132  valid_loss_mov_avg: 2.1544
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3594  valid loss: 1.5010  valid_loss_mov_avg: 2.1478
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3912  valid loss: 1.5235  valid_loss_mov_avg: 2.1416
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.4002  valid loss: 1.5174  valid_loss_mov_avg: 2.1354
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3711  valid loss: 1.5007  valid_loss_mov_avg: 2.1290
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3645  valid loss: 1.5018  valid_loss_mov_avg: 2.1227
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3564  valid loss: 1.4877  valid_loss_mov_avg: 2.1164
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3715  valid loss: 1.4934  valid_loss_mov_avg: 2.1102
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3612  valid loss: 1.5016  valid_loss_mov_avg: 2.1041
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3653  valid loss: 1.4926  valid_loss_mov_avg: 2.0980
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3340  valid loss: 1.5014  valid_loss_mov_avg: 2.0920
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3341  valid loss: 1.5070  valid_loss_mov_avg: 2.0861
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3213  valid loss: 1.4998  valid_loss_mov_avg: 2.0803
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3483  valid loss: 1.4990  valid_loss_mov_avg: 2.0745
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3347  valid loss: 1.4909  valid_loss_mov_avg: 2.0686

 ---------------- Grad-Step 20 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 21 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 22 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 23 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 24 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 25 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 26 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 27 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 28 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 29 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.385         |
| Data-EnvSampler-Poli... | 1.82          |
| Data-EnvTrajs-Averag... | -96.1         |
| Data-EnvTrajs-MaxReturn | -35.4         |
| Data-EnvTrajs-MinReturn | -163          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 52.8          |
| Data-TimeEnvSampleProc  | 0.000548      |
| Data-TimeEnvSampling    | 3.25          |
| Iteration               | 2             |
| ItrTime                 | 81.4          |
| LossAfter               | -0.018015686  |
| LossBefore              | 3.5762786e-09 |
| Model-TimeModelFit      | 12.8          |
| ModelSampler-n_times... | 120000        |
| Policy-AverageAbsPol... | 0.7015476     |
| Policy-AverageDiscou... | -238          |
| Policy-AveragePolicyStd | 0.9617234     |
| Policy-AverageReturn    | -956          |
| Policy-MaxReturn        | -127          |
| Policy-MinReturn        | -1.02e+04     |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 2.44e+03      |
| Policy-TimeAlgoOpt      | 1.16          |
| Policy-TimeSampleProc   | 0.174         |
| Policy-TimeSampling     | 64            |
| Policy-TimeStep         | 65.3          |
| Time                    | 224           |
| n_timesteps             | 3000          |
-------------------------------------------

 ---------------- Iteration 3 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.7251  valid loss: 1.5043  valid_loss_mov_avg: 2.2489
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.7593  valid loss: 1.4848  valid_loss_mov_avg: 2.2412
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.6820  valid loss: 1.4514  valid_loss_mov_avg: 2.2333
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.6576  valid loss: 1.4654  valid_loss_mov_avg: 2.2257
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.6246  valid loss: 1.4633  valid_loss_mov_avg: 2.2180
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.5835  valid loss: 1.4693  valid_loss_mov_avg: 2.2106
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.6274  valid loss: 1.4533  valid_loss_mov_avg: 2.2030
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.5915  valid loss: 1.4659  valid_loss_mov_avg: 2.1956
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.5531  valid loss: 1.4385  valid_loss_mov_avg: 2.1880
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.5497  valid loss: 1.4371  valid_loss_mov_avg: 2.1805
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.5307  valid loss: 1.4453  valid_loss_mov_avg: 2.1732
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.5214  valid loss: 1.4305  valid_loss_mov_avg: 2.1658
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.5044  valid loss: 1.4408  valid_loss_mov_avg: 2.1585
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.4825  valid loss: 1.4263  valid_loss_mov_avg: 2.1512
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.4816  valid loss: 1.4245  valid_loss_mov_avg: 2.1439
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.4747  valid loss: 1.4233  valid_loss_mov_avg: 2.1367
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.4832  valid loss: 1.4401  valid_loss_mov_avg: 2.1297
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.4497  valid loss: 1.4357  valid_loss_mov_avg: 2.1228
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.4860  valid loss: 1.4540  valid_loss_mov_avg: 2.1161
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.4998  valid loss: 1.4795  valid_loss_mov_avg: 2.1097
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.5017  valid loss: 1.4341  valid_loss_mov_avg: 2.1030
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.4904  valid loss: 1.4478  valid_loss_mov_avg: 2.0964
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.4720  valid loss: 1.4319  valid_loss_mov_avg: 2.0898
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.4311  valid loss: 1.4203  valid_loss_mov_avg: 2.0831
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.4105  valid loss: 1.4139  valid_loss_mov_avg: 2.0764
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3925  valid loss: 1.4251  valid_loss_mov_avg: 2.0699
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.4292  valid loss: 1.4170  valid_loss_mov_avg: 2.0634
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.4034  valid loss: 1.4216  valid_loss_mov_avg: 2.0569
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.4075  valid loss: 1.4146  valid_loss_mov_avg: 2.0505
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3854  valid loss: 1.4308  valid_loss_mov_avg: 2.0443
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.4108  valid loss: 1.4152  valid_loss_mov_avg: 2.0380
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3748  valid loss: 1.4148  valid_loss_mov_avg: 2.0318
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3552  valid loss: 1.4200  valid_loss_mov_avg: 2.0257
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3631  valid loss: 1.4168  valid_loss_mov_avg: 2.0196
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3472  valid loss: 1.4227  valid_loss_mov_avg: 2.0136
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3558  valid loss: 1.4191  valid_loss_mov_avg: 2.0077
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3421  valid loss: 1.4065  valid_loss_mov_avg: 2.0017
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3265  valid loss: 1.4154  valid_loss_mov_avg: 1.9958
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3322  valid loss: 1.4112  valid_loss_mov_avg: 1.9900
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3135  valid loss: 1.4091  valid_loss_mov_avg: 1.9842
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3050  valid loss: 1.4173  valid_loss_mov_avg: 1.9785
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3656  valid loss: 1.4184  valid_loss_mov_avg: 1.9729
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3360  valid loss: 1.4117  valid_loss_mov_avg: 1.9673
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3410  valid loss: 1.4117  valid_loss_mov_avg: 1.9617
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3257  valid loss: 1.4150  valid_loss_mov_avg: 1.9563
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3316  valid loss: 1.4112  valid_loss_mov_avg: 1.9508
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3242  valid loss: 1.4162  valid_loss_mov_avg: 1.9455
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3370  valid loss: 1.4229  valid_loss_mov_avg: 1.9402
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3511  valid loss: 1.4226  valid_loss_mov_avg: 1.9351
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3359  valid loss: 1.4090  valid_loss_mov_avg: 1.9298

 ---------------- Grad-Step 30 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 31 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 32 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 33 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 34 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 35 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 36 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 37 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 38 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 39 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.473          |
| Data-EnvSampler-Poli... | 2.36           |
| Data-EnvTrajs-Averag... | -143           |
| Data-EnvTrajs-MaxReturn | -107           |
| Data-EnvTrajs-MinReturn | -190           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 37.2           |
| Data-TimeEnvSampleProc  | 0.000618       |
| Data-TimeEnvSampling    | 3.94           |
| Iteration               | 3              |
| ItrTime                 | 83.7           |
| LossAfter               | -0.015812246   |
| LossBefore              | -2.2649764e-09 |
| Model-TimeModelFit      | 15.1           |
| ModelSampler-n_times... | 160000         |
| Policy-AverageAbsPol... | 0.72572935     |
| Policy-AverageDiscou... | -98.9          |
| Policy-AveragePolicyStd | 0.94753176     |
| Policy-AverageReturn    | -236           |
| Policy-MaxReturn        | -191           |
| Policy-MinReturn        | -294           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 28.2           |
| Policy-TimeAlgoOpt      | 1.05           |
| Policy-TimeSampleProc   | 0.135          |
| Policy-TimeSampling     | 63.4           |
| Policy-TimeStep         | 64.6           |
| Time                    | 307            |
| n_timesteps             | 4000           |
--------------------------------------------

 ---------------- Iteration 4 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.6005  valid loss: 1.3949  valid_loss_mov_avg: 2.0854
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.5770  valid loss: 1.3762  valid_loss_mov_avg: 2.0783
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.5460  valid loss: 1.3708  valid_loss_mov_avg: 2.0713
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.5165  valid loss: 1.3850  valid_loss_mov_avg: 2.0644
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.5351  valid loss: 1.3739  valid_loss_mov_avg: 2.0575
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.5130  valid loss: 1.3641  valid_loss_mov_avg: 2.0506
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.5090  valid loss: 1.3615  valid_loss_mov_avg: 2.0437
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4910  valid loss: 1.3514  valid_loss_mov_avg: 2.0368
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4665  valid loss: 1.3484  valid_loss_mov_avg: 2.0299
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4754  valid loss: 1.3523  valid_loss_mov_avg: 2.0231
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.4573  valid loss: 1.3424  valid_loss_mov_avg: 2.0163
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.4385  valid loss: 1.3459  valid_loss_mov_avg: 2.0096
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.4322  valid loss: 1.3543  valid_loss_mov_avg: 2.0030
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.4416  valid loss: 1.3561  valid_loss_mov_avg: 1.9966
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.4423  valid loss: 1.3421  valid_loss_mov_avg: 1.9900
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.4070  valid loss: 1.3342  valid_loss_mov_avg: 1.9835
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3972  valid loss: 1.3385  valid_loss_mov_avg: 1.9770
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.4012  valid loss: 1.3466  valid_loss_mov_avg: 1.9707
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.4011  valid loss: 1.3426  valid_loss_mov_avg: 1.9644
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3878  valid loss: 1.3447  valid_loss_mov_avg: 1.9582
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3940  valid loss: 1.3440  valid_loss_mov_avg: 1.9521
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3767  valid loss: 1.3352  valid_loss_mov_avg: 1.9459
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3664  valid loss: 1.3377  valid_loss_mov_avg: 1.9398
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3534  valid loss: 1.3412  valid_loss_mov_avg: 1.9338
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3748  valid loss: 1.3403  valid_loss_mov_avg: 1.9279
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3880  valid loss: 1.3454  valid_loss_mov_avg: 1.9221
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3613  valid loss: 1.3314  valid_loss_mov_avg: 1.9162
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3518  valid loss: 1.3336  valid_loss_mov_avg: 1.9104
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3555  valid loss: 1.3354  valid_loss_mov_avg: 1.9046
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3453  valid loss: 1.3335  valid_loss_mov_avg: 1.8989
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3430  valid loss: 1.3469  valid_loss_mov_avg: 1.8934
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3407  valid loss: 1.3371  valid_loss_mov_avg: 1.8878
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3393  valid loss: 1.3431  valid_loss_mov_avg: 1.8824
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3321  valid loss: 1.3399  valid_loss_mov_avg: 1.8769
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3451  valid loss: 1.3450  valid_loss_mov_avg: 1.8716
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3524  valid loss: 1.3392  valid_loss_mov_avg: 1.8663
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3547  valid loss: 1.3344  valid_loss_mov_avg: 1.8610
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3515  valid loss: 1.3465  valid_loss_mov_avg: 1.8558
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3319  valid loss: 1.3388  valid_loss_mov_avg: 1.8507
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3470  valid loss: 1.3415  valid_loss_mov_avg: 1.8456
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3267  valid loss: 1.3440  valid_loss_mov_avg: 1.8406
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3193  valid loss: 1.3323  valid_loss_mov_avg: 1.8355
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3150  valid loss: 1.3410  valid_loss_mov_avg: 1.8305
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3256  valid loss: 1.3466  valid_loss_mov_avg: 1.8257
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3325  valid loss: 1.3524  valid_loss_mov_avg: 1.8210
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3327  valid loss: 1.3423  valid_loss_mov_avg: 1.8162
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3195  valid loss: 1.3320  valid_loss_mov_avg: 1.8113
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3116  valid loss: 1.3266  valid_loss_mov_avg: 1.8065
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2743  valid loss: 1.3349  valid_loss_mov_avg: 1.8018
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2818  valid loss: 1.3384  valid_loss_mov_avg: 1.7971

 ---------------- Grad-Step 40 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 41 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 42 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 43 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 44 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 45 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 46 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 47 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 48 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 49 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.46           |
| Data-EnvSampler-Poli... | 2.27           |
| Data-EnvTrajs-Averag... | -69.3          |
| Data-EnvTrajs-MaxReturn | -39.2          |
| Data-EnvTrajs-MinReturn | -112           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 28.5           |
| Data-TimeEnvSampleProc  | 0.000689       |
| Data-TimeEnvSampling    | 3.83           |
| Iteration               | 4              |
| ItrTime                 | 81.2           |
| LossAfter               | -0.01334598    |
| LossBefore              | -1.9073487e-09 |
| Model-TimeModelFit      | 17.4           |
| ModelSampler-n_times... | 200000         |
| Policy-AverageAbsPol... | 0.7384764      |
| Policy-AverageDiscou... | -89            |
| Policy-AveragePolicyStd | 0.9387778      |
| Policy-AverageReturn    | -238           |
| Policy-MaxReturn        | -131           |
| Policy-MinReturn        | -1.56e+03      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 304            |
| Policy-TimeAlgoOpt      | 1.08           |
| Policy-TimeSampleProc   | 0.154          |
| Policy-TimeSampling     | 58.7           |
| Policy-TimeStep         | 60             |
| Time                    | 388            |
| n_timesteps             | 5000           |
--------------------------------------------

 ---------------- Iteration 5 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.5359  valid loss: 1.3474  valid_loss_mov_avg: 2.0144
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.5560  valid loss: 1.3335  valid_loss_mov_avg: 2.0076
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.5255  valid loss: 1.3242  valid_loss_mov_avg: 2.0008
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.5031  valid loss: 1.3210  valid_loss_mov_avg: 1.9940
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4984  valid loss: 1.3202  valid_loss_mov_avg: 1.9872
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4769  valid loss: 1.3177  valid_loss_mov_avg: 1.9805
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4770  valid loss: 1.3108  valid_loss_mov_avg: 1.9738
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4893  valid loss: 1.3040  valid_loss_mov_avg: 1.9671
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4669  valid loss: 1.3127  valid_loss_mov_avg: 1.9606
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4696  valid loss: 1.3018  valid_loss_mov_avg: 1.9540
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.4369  valid loss: 1.3191  valid_loss_mov_avg: 1.9476
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.4423  valid loss: 1.3023  valid_loss_mov_avg: 1.9412
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.4105  valid loss: 1.3053  valid_loss_mov_avg: 1.9348
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.4314  valid loss: 1.3079  valid_loss_mov_avg: 1.9286
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.4173  valid loss: 1.3053  valid_loss_mov_avg: 1.9223
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3998  valid loss: 1.2991  valid_loss_mov_avg: 1.9161
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3966  valid loss: 1.2966  valid_loss_mov_avg: 1.9099
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3931  valid loss: 1.2960  valid_loss_mov_avg: 1.9038
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3860  valid loss: 1.3004  valid_loss_mov_avg: 1.8977
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3875  valid loss: 1.3065  valid_loss_mov_avg: 1.8918
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3944  valid loss: 1.2931  valid_loss_mov_avg: 1.8858
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3713  valid loss: 1.2892  valid_loss_mov_avg: 1.8799
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3611  valid loss: 1.2966  valid_loss_mov_avg: 1.8740
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3666  valid loss: 1.2997  valid_loss_mov_avg: 1.8683
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3712  valid loss: 1.2905  valid_loss_mov_avg: 1.8625
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3852  valid loss: 1.3031  valid_loss_mov_avg: 1.8569
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3570  valid loss: 1.2997  valid_loss_mov_avg: 1.8513
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3515  valid loss: 1.2894  valid_loss_mov_avg: 1.8457
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3317  valid loss: 1.2925  valid_loss_mov_avg: 1.8402
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3342  valid loss: 1.2986  valid_loss_mov_avg: 1.8348
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3297  valid loss: 1.3000  valid_loss_mov_avg: 1.8294
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3477  valid loss: 1.2931  valid_loss_mov_avg: 1.8241
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3324  valid loss: 1.2972  valid_loss_mov_avg: 1.8188
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3323  valid loss: 1.3018  valid_loss_mov_avg: 1.8136
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3401  valid loss: 1.3007  valid_loss_mov_avg: 1.8085
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3274  valid loss: 1.2957  valid_loss_mov_avg: 1.8034
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3169  valid loss: 1.3014  valid_loss_mov_avg: 1.7984
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3250  valid loss: 1.2945  valid_loss_mov_avg: 1.7933
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3217  valid loss: 1.2970  valid_loss_mov_avg: 1.7884
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3233  valid loss: 1.3017  valid_loss_mov_avg: 1.7835
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3323  valid loss: 1.3099  valid_loss_mov_avg: 1.7787
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3284  valid loss: 1.3117  valid_loss_mov_avg: 1.7741
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3309  valid loss: 1.2950  valid_loss_mov_avg: 1.7693
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3101  valid loss: 1.2937  valid_loss_mov_avg: 1.7645
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2961  valid loss: 1.2927  valid_loss_mov_avg: 1.7598
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2978  valid loss: 1.3005  valid_loss_mov_avg: 1.7552
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2906  valid loss: 1.2914  valid_loss_mov_avg: 1.7506
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2977  valid loss: 1.2991  valid_loss_mov_avg: 1.7461
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2841  valid loss: 1.2936  valid_loss_mov_avg: 1.7415
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2851  valid loss: 1.2952  valid_loss_mov_avg: 1.7371

 ---------------- Grad-Step 50 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 51 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 52 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 53 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 54 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 55 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 56 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 57 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 58 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 59 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.542          |
| Data-EnvSampler-Poli... | 2.66           |
| Data-EnvTrajs-Averag... | -74.9          |
| Data-EnvTrajs-MaxReturn | -24.5          |
| Data-EnvTrajs-MinReturn | -142           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 39.5           |
| Data-TimeEnvSampleProc  | 0.00072        |
| Data-TimeEnvSampling    | 4.45           |
| Iteration               | 5              |
| ItrTime                 | 92.3           |
| LossAfter               | -0.015355628   |
| LossBefore              | -2.3841857e-09 |
| Model-TimeModelFit      | 20.3           |
| ModelSampler-n_times... | 240000         |
| Policy-AverageAbsPol... | 0.6929128      |
| Policy-AverageDiscou... | -80.9          |
| Policy-AveragePolicyStd | 0.92304534     |
| Policy-AverageReturn    | -184           |
| Policy-MaxReturn        | -98            |
| Policy-MinReturn        | -252           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 42.7           |
| Policy-TimeAlgoOpt      | 1.21           |
| Policy-TimeSampleProc   | 0.18           |
| Policy-TimeSampling     | 66.2           |
| Policy-TimeStep         | 67.6           |
| Time                    | 481            |
| n_timesteps             | 6000           |
--------------------------------------------

 ---------------- Iteration 6 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.5058  valid loss: 1.3002  valid_loss_mov_avg: 1.9439
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4942  valid loss: 1.3037  valid_loss_mov_avg: 1.9375
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4827  valid loss: 1.2907  valid_loss_mov_avg: 1.9310
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4776  valid loss: 1.2884  valid_loss_mov_avg: 1.9246
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4643  valid loss: 1.2847  valid_loss_mov_avg: 1.9182
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4490  valid loss: 1.2748  valid_loss_mov_avg: 1.9117
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4520  valid loss: 1.2741  valid_loss_mov_avg: 1.9054
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4303  valid loss: 1.2816  valid_loss_mov_avg: 1.8991
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4385  valid loss: 1.2958  valid_loss_mov_avg: 1.8931
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4364  valid loss: 1.2745  valid_loss_mov_avg: 1.8869
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.4285  valid loss: 1.2820  valid_loss_mov_avg: 1.8808
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.4263  valid loss: 1.2790  valid_loss_mov_avg: 1.8748
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.4077  valid loss: 1.2700  valid_loss_mov_avg: 1.8688
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3895  valid loss: 1.2706  valid_loss_mov_avg: 1.8628
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.4157  valid loss: 1.2730  valid_loss_mov_avg: 1.8569
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3981  valid loss: 1.2750  valid_loss_mov_avg: 1.8511
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3899  valid loss: 1.2641  valid_loss_mov_avg: 1.8452
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3697  valid loss: 1.2726  valid_loss_mov_avg: 1.8395
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3701  valid loss: 1.2706  valid_loss_mov_avg: 1.8338
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3706  valid loss: 1.2698  valid_loss_mov_avg: 1.8282
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3643  valid loss: 1.2759  valid_loss_mov_avg: 1.8226
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3792  valid loss: 1.2754  valid_loss_mov_avg: 1.8172
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3599  valid loss: 1.2760  valid_loss_mov_avg: 1.8117
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3658  valid loss: 1.2811  valid_loss_mov_avg: 1.8064
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3695  valid loss: 1.2817  valid_loss_mov_avg: 1.8012
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3627  valid loss: 1.2736  valid_loss_mov_avg: 1.7959
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3589  valid loss: 1.2685  valid_loss_mov_avg: 1.7906
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3458  valid loss: 1.2752  valid_loss_mov_avg: 1.7855
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3486  valid loss: 1.2733  valid_loss_mov_avg: 1.7804
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3314  valid loss: 1.2714  valid_loss_mov_avg: 1.7753
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3315  valid loss: 1.2734  valid_loss_mov_avg: 1.7703
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3188  valid loss: 1.2722  valid_loss_mov_avg: 1.7653
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3225  valid loss: 1.2823  valid_loss_mov_avg: 1.7604
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3196  valid loss: 1.2731  valid_loss_mov_avg: 1.7556
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3109  valid loss: 1.2764  valid_loss_mov_avg: 1.7508
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3301  valid loss: 1.2792  valid_loss_mov_avg: 1.7461
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3322  valid loss: 1.2768  valid_loss_mov_avg: 1.7414
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3229  valid loss: 1.2717  valid_loss_mov_avg: 1.7367
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3331  valid loss: 1.2807  valid_loss_mov_avg: 1.7321
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3200  valid loss: 1.2791  valid_loss_mov_avg: 1.7276
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3231  valid loss: 1.2736  valid_loss_mov_avg: 1.7230
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3069  valid loss: 1.2719  valid_loss_mov_avg: 1.7185
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3079  valid loss: 1.2808  valid_loss_mov_avg: 1.7142
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3098  valid loss: 1.2787  valid_loss_mov_avg: 1.7098
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3239  valid loss: 1.2844  valid_loss_mov_avg: 1.7056
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3185  valid loss: 1.2807  valid_loss_mov_avg: 1.7013
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2982  valid loss: 1.2734  valid_loss_mov_avg: 1.6970
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2886  valid loss: 1.2722  valid_loss_mov_avg: 1.6928
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3018  valid loss: 1.2895  valid_loss_mov_avg: 1.6887
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3147  valid loss: 1.2721  valid_loss_mov_avg: 1.6846

 ---------------- Grad-Step 60 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 61 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 62 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 63 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 64 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 65 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 66 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 67 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 68 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 69 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.453         |
| Data-EnvSampler-Poli... | 2.19          |
| Data-EnvTrajs-Averag... | -62.2         |
| Data-EnvTrajs-MaxReturn | 0.287         |
| Data-EnvTrajs-MinReturn | -133          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 51.7          |
| Data-TimeEnvSampleProc  | 0.000847      |
| Data-TimeEnvSampling    | 3.93          |
| Iteration               | 6             |
| ItrTime                 | 90.8          |
| LossAfter               | -0.013662737  |
| LossBefore              | -6.556511e-10 |
| Model-TimeModelFit      | 20.8          |
| ModelSampler-n_times... | 280000        |
| Policy-AverageAbsPol... | 0.68841815    |
| Policy-AverageDiscou... | -85.4         |
| Policy-AveragePolicyStd | 0.8961222     |
| Policy-AverageReturn    | -204          |
| Policy-MaxReturn        | -162          |
| Policy-MinReturn        | -262          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 27.3          |
| Policy-TimeAlgoOpt      | 1.12          |
| Policy-TimeSampleProc   | 0.16          |
| Policy-TimeSampling     | 64.8          |
| Policy-TimeStep         | 66.1          |
| Time                    | 572           |
| n_timesteps             | 7000          |
-------------------------------------------

 ---------------- Iteration 7 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4610  valid loss: 1.2738  valid_loss_mov_avg: 1.9043
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4620  valid loss: 1.2670  valid_loss_mov_avg: 1.8979
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4639  valid loss: 1.2711  valid_loss_mov_avg: 1.8917
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4535  valid loss: 1.2508  valid_loss_mov_avg: 1.8853
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4407  valid loss: 1.2545  valid_loss_mov_avg: 1.8790
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4297  valid loss: 1.2503  valid_loss_mov_avg: 1.8727
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4088  valid loss: 1.2537  valid_loss_mov_avg: 1.8665
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3997  valid loss: 1.2628  valid_loss_mov_avg: 1.8604
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4246  valid loss: 1.2480  valid_loss_mov_avg: 1.8543
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4040  valid loss: 1.2573  valid_loss_mov_avg: 1.8483
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.4030  valid loss: 1.2613  valid_loss_mov_avg: 1.8425
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3966  valid loss: 1.2509  valid_loss_mov_avg: 1.8366
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3807  valid loss: 1.2441  valid_loss_mov_avg: 1.8306
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3917  valid loss: 1.2661  valid_loss_mov_avg: 1.8250
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3997  valid loss: 1.2455  valid_loss_mov_avg: 1.8192
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3864  valid loss: 1.2455  valid_loss_mov_avg: 1.8135
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3714  valid loss: 1.2556  valid_loss_mov_avg: 1.8079
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3794  valid loss: 1.2506  valid_loss_mov_avg: 1.8023
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3730  valid loss: 1.2391  valid_loss_mov_avg: 1.7967
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3503  valid loss: 1.2442  valid_loss_mov_avg: 1.7911
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3499  valid loss: 1.2467  valid_loss_mov_avg: 1.7857
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3529  valid loss: 1.2438  valid_loss_mov_avg: 1.7803
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3410  valid loss: 1.2523  valid_loss_mov_avg: 1.7750
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3460  valid loss: 1.2445  valid_loss_mov_avg: 1.7697
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3525  valid loss: 1.2480  valid_loss_mov_avg: 1.7645
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3450  valid loss: 1.2489  valid_loss_mov_avg: 1.7593
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3439  valid loss: 1.2429  valid_loss_mov_avg: 1.7542
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3395  valid loss: 1.2418  valid_loss_mov_avg: 1.7490
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3342  valid loss: 1.2450  valid_loss_mov_avg: 1.7440
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3297  valid loss: 1.2456  valid_loss_mov_avg: 1.7390
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3259  valid loss: 1.2553  valid_loss_mov_avg: 1.7342
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3386  valid loss: 1.2432  valid_loss_mov_avg: 1.7293
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3295  valid loss: 1.2537  valid_loss_mov_avg: 1.7245
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3238  valid loss: 1.2497  valid_loss_mov_avg: 1.7198
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3122  valid loss: 1.2414  valid_loss_mov_avg: 1.7150
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3162  valid loss: 1.2524  valid_loss_mov_avg: 1.7104
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3202  valid loss: 1.2500  valid_loss_mov_avg: 1.7058
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3095  valid loss: 1.2522  valid_loss_mov_avg: 1.7012
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3131  valid loss: 1.2488  valid_loss_mov_avg: 1.6967
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3108  valid loss: 1.2488  valid_loss_mov_avg: 1.6922
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3174  valid loss: 1.2648  valid_loss_mov_avg: 1.6879
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3093  valid loss: 1.2501  valid_loss_mov_avg: 1.6836
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3002  valid loss: 1.2492  valid_loss_mov_avg: 1.6792
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3165  valid loss: 1.2626  valid_loss_mov_avg: 1.6750
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3149  valid loss: 1.2505  valid_loss_mov_avg: 1.6708
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3007  valid loss: 1.2541  valid_loss_mov_avg: 1.6666
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2998  valid loss: 1.2493  valid_loss_mov_avg: 1.6625
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2922  valid loss: 1.2517  valid_loss_mov_avg: 1.6584
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2894  valid loss: 1.2496  valid_loss_mov_avg: 1.6543
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2963  valid loss: 1.2458  valid_loss_mov_avg: 1.6502

 ---------------- Grad-Step 70 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 71 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 72 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 73 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 74 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 75 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 76 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 77 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 78 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 79 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.379         |
| Data-EnvSampler-Poli... | 1.79          |
| Data-EnvTrajs-Averag... | -82.7         |
| Data-EnvTrajs-MaxReturn | 31.4          |
| Data-EnvTrajs-MinReturn | -158          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 65.5          |
| Data-TimeEnvSampleProc  | 0.000685      |
| Data-TimeEnvSampling    | 3.22          |
| Iteration               | 7             |
| ItrTime                 | 91.3          |
| LossAfter               | -0.01556236   |
| LossBefore              | 1.0371208e-08 |
| Model-TimeModelFit      | 28            |
| ModelSampler-n_times... | 320000        |
| Policy-AverageAbsPol... | 0.7002278     |
| Policy-AverageDiscou... | -30.8         |
| Policy-AveragePolicyStd | 0.8669676     |
| Policy-AverageReturn    | -71.3         |
| Policy-MaxReturn        | -21.9         |
| Policy-MinReturn        | -157          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 28.7          |
| Policy-TimeAlgoOpt      | 1.01          |
| Policy-TimeSampleProc   | 0.162         |
| Policy-TimeSampling     | 58.8          |
| Policy-TimeStep         | 60            |
| Time                    | 663           |
| n_timesteps             | 8000          |
-------------------------------------------

 ---------------- Iteration 8 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.5151  valid loss: 1.2901  valid_loss_mov_avg: 1.9286
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.5060  valid loss: 1.2951  valid_loss_mov_avg: 1.9223
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.5016  valid loss: 1.2899  valid_loss_mov_avg: 1.9160
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.5005  valid loss: 1.2747  valid_loss_mov_avg: 1.9096
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4822  valid loss: 1.2895  valid_loss_mov_avg: 1.9034
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4804  valid loss: 1.2802  valid_loss_mov_avg: 1.8971
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4631  valid loss: 1.2861  valid_loss_mov_avg: 1.8910
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4909  valid loss: 1.2692  valid_loss_mov_avg: 1.8848
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4627  valid loss: 1.2717  valid_loss_mov_avg: 1.8787
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4496  valid loss: 1.2783  valid_loss_mov_avg: 1.8727
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.4526  valid loss: 1.2722  valid_loss_mov_avg: 1.8667
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.4440  valid loss: 1.2772  valid_loss_mov_avg: 1.8608
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.4764  valid loss: 1.2726  valid_loss_mov_avg: 1.8549
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.4433  valid loss: 1.2776  valid_loss_mov_avg: 1.8491
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.4321  valid loss: 1.2845  valid_loss_mov_avg: 1.8435
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.4475  valid loss: 1.2799  valid_loss_mov_avg: 1.8378
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.4199  valid loss: 1.2800  valid_loss_mov_avg: 1.8323
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.4058  valid loss: 1.2739  valid_loss_mov_avg: 1.8267
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.4025  valid loss: 1.2828  valid_loss_mov_avg: 1.8212
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.4053  valid loss: 1.2835  valid_loss_mov_avg: 1.8159
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.4038  valid loss: 1.2884  valid_loss_mov_avg: 1.8106
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.4140  valid loss: 1.2728  valid_loss_mov_avg: 1.8052
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3958  valid loss: 1.2810  valid_loss_mov_avg: 1.8000
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3896  valid loss: 1.2725  valid_loss_mov_avg: 1.7947
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3812  valid loss: 1.2876  valid_loss_mov_avg: 1.7896
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3783  valid loss: 1.2751  valid_loss_mov_avg: 1.7845
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3830  valid loss: 1.2820  valid_loss_mov_avg: 1.7795
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3891  valid loss: 1.2852  valid_loss_mov_avg: 1.7745
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.4018  valid loss: 1.2923  valid_loss_mov_avg: 1.7697
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3988  valid loss: 1.2892  valid_loss_mov_avg: 1.7649
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3790  valid loss: 1.2818  valid_loss_mov_avg: 1.7601
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3674  valid loss: 1.2844  valid_loss_mov_avg: 1.7553
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3616  valid loss: 1.2900  valid_loss_mov_avg: 1.7506
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3632  valid loss: 1.2881  valid_loss_mov_avg: 1.7460
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3562  valid loss: 1.2866  valid_loss_mov_avg: 1.7414
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3581  valid loss: 1.2905  valid_loss_mov_avg: 1.7369
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3493  valid loss: 1.2838  valid_loss_mov_avg: 1.7324
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3566  valid loss: 1.2879  valid_loss_mov_avg: 1.7279
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3584  valid loss: 1.2815  valid_loss_mov_avg: 1.7235
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3444  valid loss: 1.2927  valid_loss_mov_avg: 1.7192
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3457  valid loss: 1.2843  valid_loss_mov_avg: 1.7148
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3318  valid loss: 1.2856  valid_loss_mov_avg: 1.7105
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3208  valid loss: 1.2859  valid_loss_mov_avg: 1.7063
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3314  valid loss: 1.2859  valid_loss_mov_avg: 1.7021
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3364  valid loss: 1.2923  valid_loss_mov_avg: 1.6980
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3374  valid loss: 1.2924  valid_loss_mov_avg: 1.6939
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3270  valid loss: 1.2956  valid_loss_mov_avg: 1.6899
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3376  valid loss: 1.2978  valid_loss_mov_avg: 1.6860
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3405  valid loss: 1.2983  valid_loss_mov_avg: 1.6821
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3372  valid loss: 1.3005  valid_loss_mov_avg: 1.6783

 ---------------- Grad-Step 80 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 81 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 82 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 83 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 84 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 85 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 86 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 87 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 88 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 89 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.465          |
| Data-EnvSampler-Poli... | 2.29           |
| Data-EnvTrajs-Averag... | -47.3          |
| Data-EnvTrajs-MaxReturn | -22            |
| Data-EnvTrajs-MinReturn | -94.1          |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 26.5           |
| Data-TimeEnvSampleProc  | 0.000546       |
| Data-TimeEnvSampling    | 3.83           |
| Iteration               | 8              |
| ItrTime                 | 94.9           |
| LossAfter               | -0.017249443   |
| LossBefore              | -1.4841556e-08 |
| Model-TimeModelFit      | 30.6           |
| ModelSampler-n_times... | 360000         |
| Policy-AverageAbsPol... | 0.7211675      |
| Policy-AverageDiscou... | -58.5          |
| Policy-AveragePolicyStd | 0.8445963      |
| Policy-AverageReturn    | -142           |
| Policy-MaxReturn        | -88.8          |
| Policy-MinReturn        | -197           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 27.5           |
| Policy-TimeAlgoOpt      | 1.01           |
| Policy-TimeSampleProc   | 0.144          |
| Policy-TimeSampling     | 59.3           |
| Policy-TimeStep         | 60.5           |
| Time                    | 758            |
| n_timesteps             | 9000           |
--------------------------------------------

 ---------------- Iteration 9 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.5250  valid loss: 1.2829  valid_loss_mov_avg: 1.9180
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4921  valid loss: 1.3008  valid_loss_mov_avg: 1.9118
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4928  valid loss: 1.2908  valid_loss_mov_avg: 1.9056
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4861  valid loss: 1.2816  valid_loss_mov_avg: 1.8993
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4843  valid loss: 1.2818  valid_loss_mov_avg: 1.8932
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4852  valid loss: 1.2662  valid_loss_mov_avg: 1.8869
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4624  valid loss: 1.2701  valid_loss_mov_avg: 1.8807
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4511  valid loss: 1.2769  valid_loss_mov_avg: 1.8747
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4527  valid loss: 1.2699  valid_loss_mov_avg: 1.8686
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4451  valid loss: 1.2736  valid_loss_mov_avg: 1.8627
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.4569  valid loss: 1.2695  valid_loss_mov_avg: 1.8568
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.4564  valid loss: 1.2651  valid_loss_mov_avg: 1.8508
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.4348  valid loss: 1.2710  valid_loss_mov_avg: 1.8450
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.4605  valid loss: 1.2716  valid_loss_mov_avg: 1.8393
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.4320  valid loss: 1.2694  valid_loss_mov_avg: 1.8336
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.4305  valid loss: 1.2712  valid_loss_mov_avg: 1.8280
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.4051  valid loss: 1.2765  valid_loss_mov_avg: 1.8225
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.4116  valid loss: 1.2794  valid_loss_mov_avg: 1.8170
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.4394  valid loss: 1.2667  valid_loss_mov_avg: 1.8115
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.4180  valid loss: 1.2728  valid_loss_mov_avg: 1.8062
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.4243  valid loss: 1.2779  valid_loss_mov_avg: 1.8009
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.4215  valid loss: 1.2658  valid_loss_mov_avg: 1.7955
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3920  valid loss: 1.2773  valid_loss_mov_avg: 1.7903
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3845  valid loss: 1.2757  valid_loss_mov_avg: 1.7852
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.4086  valid loss: 1.2873  valid_loss_mov_avg: 1.7802
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.4202  valid loss: 1.2773  valid_loss_mov_avg: 1.7752
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.4211  valid loss: 1.2647  valid_loss_mov_avg: 1.7701
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.4039  valid loss: 1.2659  valid_loss_mov_avg: 1.7650
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3975  valid loss: 1.2768  valid_loss_mov_avg: 1.7602
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3836  valid loss: 1.2760  valid_loss_mov_avg: 1.7553
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3853  valid loss: 1.2784  valid_loss_mov_avg: 1.7505
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.4268  valid loss: 1.2654  valid_loss_mov_avg: 1.7457
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3685  valid loss: 1.3026  valid_loss_mov_avg: 1.7413
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.4113  valid loss: 1.2827  valid_loss_mov_avg: 1.7367
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3961  valid loss: 1.2678  valid_loss_mov_avg: 1.7320
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3708  valid loss: 1.2632  valid_loss_mov_avg: 1.7273
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3812  valid loss: 1.2667  valid_loss_mov_avg: 1.7227
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3539  valid loss: 1.2829  valid_loss_mov_avg: 1.7183
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.4033  valid loss: 1.2725  valid_loss_mov_avg: 1.7138
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3555  valid loss: 1.2763  valid_loss_mov_avg: 1.7095
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3657  valid loss: 1.2752  valid_loss_mov_avg: 1.7051
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3773  valid loss: 1.2679  valid_loss_mov_avg: 1.7007
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3488  valid loss: 1.2754  valid_loss_mov_avg: 1.6965
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3737  valid loss: 1.2679  valid_loss_mov_avg: 1.6922
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3709  valid loss: 1.2733  valid_loss_mov_avg: 1.6880
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3847  valid loss: 1.2728  valid_loss_mov_avg: 1.6839
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3481  valid loss: 1.2848  valid_loss_mov_avg: 1.6799
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3786  valid loss: 1.2673  valid_loss_mov_avg: 1.6758
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3388  valid loss: 1.2850  valid_loss_mov_avg: 1.6718
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3434  valid loss: 1.2881  valid_loss_mov_avg: 1.6680

 ---------------- Grad-Step 90 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 91 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 92 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 93 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 94 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 95 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 96 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 97 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 98 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 99 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
------------------------------------------
| Data-EnvSampler-EnvE... | 0.369        |
| Data-EnvSampler-Poli... | 1.75         |
| Data-EnvTrajs-Averag... | -37.7        |
| Data-EnvTrajs-MaxReturn | 17.6         |
| Data-EnvTrajs-MinReturn | -74.8        |
| Data-EnvTrajs-NumTrajs  | 5            |
| Data-EnvTrajs-StdReturn | 30.2         |
| Data-TimeEnvSampleProc  | 0.000563     |
| Data-TimeEnvSampling    | 3.09         |
| Iteration               | 9            |
| ItrTime                 | 87.8         |
| LossAfter               | -0.01398831  |
| LossBefore              | 7.867813e-09 |
| Model-TimeModelFit      | 26.6         |
| ModelSampler-n_times... | 400000       |
| Policy-AverageAbsPol... | 0.7355177    |
| Policy-AverageDiscou... | -43.2        |
| Policy-AveragePolicyStd | 0.8267127    |
| Policy-AverageReturn    | -103         |
| Policy-MaxReturn        | -24.6        |
| Policy-MinReturn        | -181         |
| Policy-NumTrajs         | 20           |
| Policy-StdReturn        | 45.6         |
| Policy-TimeAlgoOpt      | 0.925        |
| Policy-TimeSampleProc   | 0.15         |
| Policy-TimeSampling     | 57           |
| Policy-TimeStep         | 58.1         |
| Time                    | 846          |
| n_timesteps             | 10000        |
------------------------------------------

 ---------------- Iteration 10 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4858  valid loss: 1.2669  valid_loss_mov_avg: 1.8941
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4512  valid loss: 1.2662  valid_loss_mov_avg: 1.8878
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4502  valid loss: 1.2552  valid_loss_mov_avg: 1.8815
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4315  valid loss: 1.2700  valid_loss_mov_avg: 1.8754
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4360  valid loss: 1.2574  valid_loss_mov_avg: 1.8692
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4162  valid loss: 1.2765  valid_loss_mov_avg: 1.8633
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4204  valid loss: 1.2556  valid_loss_mov_avg: 1.8572
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4043  valid loss: 1.2505  valid_loss_mov_avg: 1.8511
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3980  valid loss: 1.2619  valid_loss_mov_avg: 1.8452
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4164  valid loss: 1.2633  valid_loss_mov_avg: 1.8394
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.4212  valid loss: 1.2532  valid_loss_mov_avg: 1.8335
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.4043  valid loss: 1.2588  valid_loss_mov_avg: 1.8278
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3939  valid loss: 1.2617  valid_loss_mov_avg: 1.8221
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3922  valid loss: 1.2582  valid_loss_mov_avg: 1.8165
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3997  valid loss: 1.2573  valid_loss_mov_avg: 1.8109
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3958  valid loss: 1.2550  valid_loss_mov_avg: 1.8053
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3903  valid loss: 1.2552  valid_loss_mov_avg: 1.7998
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3741  valid loss: 1.2625  valid_loss_mov_avg: 1.7945
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3898  valid loss: 1.2562  valid_loss_mov_avg: 1.7891
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3720  valid loss: 1.2609  valid_loss_mov_avg: 1.7838
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3698  valid loss: 1.2748  valid_loss_mov_avg: 1.7787
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3906  valid loss: 1.2596  valid_loss_mov_avg: 1.7735
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3574  valid loss: 1.2643  valid_loss_mov_avg: 1.7684
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3606  valid loss: 1.2618  valid_loss_mov_avg: 1.7634
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3641  valid loss: 1.2601  valid_loss_mov_avg: 1.7583
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3690  valid loss: 1.2687  valid_loss_mov_avg: 1.7534
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3611  valid loss: 1.2682  valid_loss_mov_avg: 1.7486
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3649  valid loss: 1.2618  valid_loss_mov_avg: 1.7437
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3538  valid loss: 1.2588  valid_loss_mov_avg: 1.7389
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3464  valid loss: 1.2632  valid_loss_mov_avg: 1.7341
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3554  valid loss: 1.2588  valid_loss_mov_avg: 1.7294
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3541  valid loss: 1.2562  valid_loss_mov_avg: 1.7246
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3537  valid loss: 1.2687  valid_loss_mov_avg: 1.7201
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3581  valid loss: 1.2603  valid_loss_mov_avg: 1.7155
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3653  valid loss: 1.2652  valid_loss_mov_avg: 1.7110
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3464  valid loss: 1.2593  valid_loss_mov_avg: 1.7064
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3503  valid loss: 1.2645  valid_loss_mov_avg: 1.7020
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3414  valid loss: 1.2619  valid_loss_mov_avg: 1.6976
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3372  valid loss: 1.2601  valid_loss_mov_avg: 1.6933
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3453  valid loss: 1.2590  valid_loss_mov_avg: 1.6889
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3234  valid loss: 1.2688  valid_loss_mov_avg: 1.6847
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3303  valid loss: 1.2704  valid_loss_mov_avg: 1.6806
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3473  valid loss: 1.2692  valid_loss_mov_avg: 1.6765
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3347  valid loss: 1.2641  valid_loss_mov_avg: 1.6723
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3192  valid loss: 1.2667  valid_loss_mov_avg: 1.6683
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3395  valid loss: 1.2660  valid_loss_mov_avg: 1.6643
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3276  valid loss: 1.2659  valid_loss_mov_avg: 1.6603
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3350  valid loss: 1.2606  valid_loss_mov_avg: 1.6563
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3370  valid loss: 1.2584  valid_loss_mov_avg: 1.6523
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3201  valid loss: 1.2693  valid_loss_mov_avg: 1.6485

 ---------------- Grad-Step 100 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 101 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 102 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 103 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 104 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 105 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 106 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 107 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 108 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 109 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.375         |
| Data-EnvSampler-Poli... | 1.79          |
| Data-EnvTrajs-Averag... | -23.3         |
| Data-EnvTrajs-MaxReturn | 10.2          |
| Data-EnvTrajs-MinReturn | -51.8         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 22.4          |
| Data-TimeEnvSampleProc  | 0.000557      |
| Data-TimeEnvSampling    | 3.17          |
| Iteration               | 10            |
| ItrTime                 | 92.4          |
| LossAfter               | -0.0145241385 |
| LossBefore              | -6.645918e-09 |
| Model-TimeModelFit      | 30.4          |
| ModelSampler-n_times... | 440000        |
| Policy-AverageAbsPol... | 0.74257666    |
| Policy-AverageDiscou... | -39.4         |
| Policy-AveragePolicyStd | 0.8061051     |
| Policy-AverageReturn    | -88.8         |
| Policy-MaxReturn        | -39.3         |
| Policy-MinReturn        | -166          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 30.4          |
| Policy-TimeAlgoOpt      | 1.08          |
| Policy-TimeSampleProc   | 0.149         |
| Policy-TimeSampling     | 57.6          |
| Policy-TimeStep         | 58.8          |
| Time                    | 938           |
| n_timesteps             | 11000         |
-------------------------------------------

 ---------------- Iteration 11 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4438  valid loss: 1.2786  valid_loss_mov_avg: 1.9114
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4403  valid loss: 1.2519  valid_loss_mov_avg: 1.9048
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4239  valid loss: 1.2529  valid_loss_mov_avg: 1.8983
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4314  valid loss: 1.2542  valid_loss_mov_avg: 1.8919
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4281  valid loss: 1.2541  valid_loss_mov_avg: 1.8855
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4141  valid loss: 1.2741  valid_loss_mov_avg: 1.8794
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4204  valid loss: 1.2567  valid_loss_mov_avg: 1.8732
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4049  valid loss: 1.2606  valid_loss_mov_avg: 1.8670
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4001  valid loss: 1.2499  valid_loss_mov_avg: 1.8609
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3904  valid loss: 1.2522  valid_loss_mov_avg: 1.8548
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3847  valid loss: 1.2514  valid_loss_mov_avg: 1.8487
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3886  valid loss: 1.2526  valid_loss_mov_avg: 1.8428
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3888  valid loss: 1.2573  valid_loss_mov_avg: 1.8369
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3876  valid loss: 1.2652  valid_loss_mov_avg: 1.8312
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.4137  valid loss: 1.2513  valid_loss_mov_avg: 1.8254
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3769  valid loss: 1.2533  valid_loss_mov_avg: 1.8197
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3667  valid loss: 1.2582  valid_loss_mov_avg: 1.8141
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3690  valid loss: 1.2585  valid_loss_mov_avg: 1.8085
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3691  valid loss: 1.2528  valid_loss_mov_avg: 1.8030
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3641  valid loss: 1.2556  valid_loss_mov_avg: 1.7975
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3619  valid loss: 1.2622  valid_loss_mov_avg: 1.7921
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3670  valid loss: 1.2572  valid_loss_mov_avg: 1.7868
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3653  valid loss: 1.2595  valid_loss_mov_avg: 1.7815
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3586  valid loss: 1.2549  valid_loss_mov_avg: 1.7763
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3513  valid loss: 1.2600  valid_loss_mov_avg: 1.7711
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3502  valid loss: 1.2589  valid_loss_mov_avg: 1.7660
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3576  valid loss: 1.2552  valid_loss_mov_avg: 1.7609
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3431  valid loss: 1.2683  valid_loss_mov_avg: 1.7559
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3602  valid loss: 1.2607  valid_loss_mov_avg: 1.7510
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3530  valid loss: 1.2627  valid_loss_mov_avg: 1.7461
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3411  valid loss: 1.2531  valid_loss_mov_avg: 1.7412
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3407  valid loss: 1.2591  valid_loss_mov_avg: 1.7363
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3347  valid loss: 1.2645  valid_loss_mov_avg: 1.7316
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3477  valid loss: 1.2636  valid_loss_mov_avg: 1.7269
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3404  valid loss: 1.2632  valid_loss_mov_avg: 1.7223
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3237  valid loss: 1.2607  valid_loss_mov_avg: 1.7177
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3350  valid loss: 1.2585  valid_loss_mov_avg: 1.7131
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3289  valid loss: 1.2629  valid_loss_mov_avg: 1.7086
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3362  valid loss: 1.2627  valid_loss_mov_avg: 1.7041
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3363  valid loss: 1.2599  valid_loss_mov_avg: 1.6997
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3232  valid loss: 1.2634  valid_loss_mov_avg: 1.6953
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3371  valid loss: 1.2653  valid_loss_mov_avg: 1.6910
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3271  valid loss: 1.2657  valid_loss_mov_avg: 1.6868
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3248  valid loss: 1.2560  valid_loss_mov_avg: 1.6825
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3280  valid loss: 1.2560  valid_loss_mov_avg: 1.6782
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3185  valid loss: 1.2634  valid_loss_mov_avg: 1.6741
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3198  valid loss: 1.2612  valid_loss_mov_avg: 1.6699
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3222  valid loss: 1.2621  valid_loss_mov_avg: 1.6659
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3246  valid loss: 1.2602  valid_loss_mov_avg: 1.6618
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3244  valid loss: 1.2624  valid_loss_mov_avg: 1.6578

 ---------------- Grad-Step 110 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 111 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 112 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 113 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 114 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 115 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 116 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 117 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 118 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 119 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
------------------------------------------
| Data-EnvSampler-EnvE... | 0.373        |
| Data-EnvSampler-Poli... | 1.77         |
| Data-EnvTrajs-Averag... | -28.7        |
| Data-EnvTrajs-MaxReturn | 4.29         |
| Data-EnvTrajs-MinReturn | -66.8        |
| Data-EnvTrajs-NumTrajs  | 5            |
| Data-EnvTrajs-StdReturn | 27.5         |
| Data-TimeEnvSampleProc  | 0.000851     |
| Data-TimeEnvSampling    | 3.12         |
| Iteration               | 11           |
| ItrTime                 | 96.6         |
| LossAfter               | -0.016234793 |
| LossBefore              | 4.887581e-09 |
| Model-TimeModelFit      | 34.5         |
| ModelSampler-n_times... | 480000       |
| Policy-AverageAbsPol... | 0.7933969    |
| Policy-AverageDiscou... | -15.2        |
| Policy-AveragePolicyStd | 0.784235     |
| Policy-AverageReturn    | -33.8        |
| Policy-MaxReturn        | 11.5         |
| Policy-MinReturn        | -153         |
| Policy-NumTrajs         | 20           |
| Policy-StdReturn        | 35.2         |
| Policy-TimeAlgoOpt      | 0.995        |
| Policy-TimeSampleProc   | 0.155        |
| Policy-TimeSampling     | 57.8         |
| Policy-TimeStep         | 59           |
| Time                    | 1.03e+03     |
| n_timesteps             | 12000        |
------------------------------------------

 ---------------- Iteration 12 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.6942  valid loss: 1.2692  valid_loss_mov_avg: 1.8974
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.6290  valid loss: 1.2541  valid_loss_mov_avg: 1.8910
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.5956  valid loss: 1.2383  valid_loss_mov_avg: 1.8844
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.6088  valid loss: 1.2113  valid_loss_mov_avg: 1.8777
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.5912  valid loss: 1.2140  valid_loss_mov_avg: 1.8711
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.5533  valid loss: 1.2040  valid_loss_mov_avg: 1.8644
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.5609  valid loss: 1.2045  valid_loss_mov_avg: 1.8578
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.5155  valid loss: 1.2415  valid_loss_mov_avg: 1.8516
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.5643  valid loss: 1.2102  valid_loss_mov_avg: 1.8452
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4978  valid loss: 1.2156  valid_loss_mov_avg: 1.8389
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.5378  valid loss: 1.2122  valid_loss_mov_avg: 1.8327
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.5220  valid loss: 1.2262  valid_loss_mov_avg: 1.8266
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.5171  valid loss: 1.1988  valid_loss_mov_avg: 1.8203
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.4702  valid loss: 1.2120  valid_loss_mov_avg: 1.8142
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.5086  valid loss: 1.1951  valid_loss_mov_avg: 1.8081
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.4871  valid loss: 1.1951  valid_loss_mov_avg: 1.8019
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.4463  valid loss: 1.2064  valid_loss_mov_avg: 1.7960
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.4905  valid loss: 1.1942  valid_loss_mov_avg: 1.7899
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.4598  valid loss: 1.1965  valid_loss_mov_avg: 1.7840
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.4642  valid loss: 1.1957  valid_loss_mov_avg: 1.7781
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.4572  valid loss: 1.2029  valid_loss_mov_avg: 1.7724
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.4560  valid loss: 1.2004  valid_loss_mov_avg: 1.7667
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.4556  valid loss: 1.1957  valid_loss_mov_avg: 1.7609
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.4466  valid loss: 1.1984  valid_loss_mov_avg: 1.7553
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.4466  valid loss: 1.2029  valid_loss_mov_avg: 1.7498
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.4324  valid loss: 1.2002  valid_loss_mov_avg: 1.7443
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.4559  valid loss: 1.1995  valid_loss_mov_avg: 1.7389
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.4215  valid loss: 1.2100  valid_loss_mov_avg: 1.7336
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.4545  valid loss: 1.1975  valid_loss_mov_avg: 1.7282
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.4296  valid loss: 1.2006  valid_loss_mov_avg: 1.7229
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.4040  valid loss: 1.2127  valid_loss_mov_avg: 1.7178
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.4448  valid loss: 1.2028  valid_loss_mov_avg: 1.7127
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.4282  valid loss: 1.2017  valid_loss_mov_avg: 1.7076
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3951  valid loss: 1.2114  valid_loss_mov_avg: 1.7026
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.4390  valid loss: 1.2017  valid_loss_mov_avg: 1.6976
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3943  valid loss: 1.2055  valid_loss_mov_avg: 1.6927
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.4237  valid loss: 1.1984  valid_loss_mov_avg: 1.6877
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3751  valid loss: 1.2110  valid_loss_mov_avg: 1.6830
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.4227  valid loss: 1.2078  valid_loss_mov_avg: 1.6782
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3808  valid loss: 1.2144  valid_loss_mov_avg: 1.6736
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.4174  valid loss: 1.1992  valid_loss_mov_avg: 1.6688
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3816  valid loss: 1.2047  valid_loss_mov_avg: 1.6642
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3690  valid loss: 1.2124  valid_loss_mov_avg: 1.6597
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.4164  valid loss: 1.2105  valid_loss_mov_avg: 1.6552
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3642  valid loss: 1.2146  valid_loss_mov_avg: 1.6508
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.4184  valid loss: 1.2110  valid_loss_mov_avg: 1.6464
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3823  valid loss: 1.2023  valid_loss_mov_avg: 1.6419
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3912  valid loss: 1.2049  valid_loss_mov_avg: 1.6376
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3825  valid loss: 1.2069  valid_loss_mov_avg: 1.6333
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3812  valid loss: 1.2029  valid_loss_mov_avg: 1.6290

 ---------------- Grad-Step 120 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 121 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 122 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 123 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 124 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 125 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 126 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 127 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 128 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 129 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.381         |
| Data-EnvSampler-Poli... | 1.81          |
| Data-EnvTrajs-Averag... | -23.6         |
| Data-EnvTrajs-MaxReturn | 54.7          |
| Data-EnvTrajs-MinReturn | -68.5         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 42.3          |
| Data-TimeEnvSampleProc  | 0.000846      |
| Data-TimeEnvSampling    | 3.25          |
| Iteration               | 12            |
| ItrTime                 | 100           |
| LossAfter               | -0.020462606  |
| LossBefore              | 6.6757204e-09 |
| Model-TimeModelFit      | 36.8          |
| ModelSampler-n_times... | 520000        |
| Policy-AverageAbsPol... | 0.75272274    |
| Policy-AverageDiscou... | -57.7         |
| Policy-AveragePolicyStd | 0.75955206    |
| Policy-AverageReturn    | -146          |
| Policy-MaxReturn        | 43.9          |
| Policy-MinReturn        | -712          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 150           |
| Policy-TimeAlgoOpt      | 1.13          |
| Policy-TimeSampleProc   | 0.184         |
| Policy-TimeSampling     | 59.1          |
| Policy-TimeStep         | 60.4          |
| Time                    | 1.13e+03      |
| n_timesteps             | 13000         |
-------------------------------------------

 ---------------- Iteration 13 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.6011  valid loss: 1.2298  valid_loss_mov_avg: 1.8385
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.5800  valid loss: 1.1948  valid_loss_mov_avg: 1.8321
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.5334  valid loss: 1.1943  valid_loss_mov_avg: 1.8257
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.5164  valid loss: 1.1877  valid_loss_mov_avg: 1.8193
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4966  valid loss: 1.1979  valid_loss_mov_avg: 1.8131
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.5285  valid loss: 1.1808  valid_loss_mov_avg: 1.8068
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4751  valid loss: 1.2060  valid_loss_mov_avg: 1.8008
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4955  valid loss: 1.1811  valid_loss_mov_avg: 1.7946
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4713  valid loss: 1.1839  valid_loss_mov_avg: 1.7885
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4844  valid loss: 1.1724  valid_loss_mov_avg: 1.7823
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.4758  valid loss: 1.1766  valid_loss_mov_avg: 1.7763
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.4713  valid loss: 1.1763  valid_loss_mov_avg: 1.7703
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.4396  valid loss: 1.1859  valid_loss_mov_avg: 1.7644
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.4431  valid loss: 1.1742  valid_loss_mov_avg: 1.7585
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.4427  valid loss: 1.1733  valid_loss_mov_avg: 1.7527
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.4352  valid loss: 1.1726  valid_loss_mov_avg: 1.7469
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.4642  valid loss: 1.1865  valid_loss_mov_avg: 1.7413
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.4190  valid loss: 1.1867  valid_loss_mov_avg: 1.7357
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.4484  valid loss: 1.1893  valid_loss_mov_avg: 1.7302
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.4338  valid loss: 1.1878  valid_loss_mov_avg: 1.7248
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.4446  valid loss: 1.1893  valid_loss_mov_avg: 1.7195
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.4092  valid loss: 1.1794  valid_loss_mov_avg: 1.7141
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.4188  valid loss: 1.1828  valid_loss_mov_avg: 1.7088
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.4309  valid loss: 1.1835  valid_loss_mov_avg: 1.7035
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.4004  valid loss: 1.1833  valid_loss_mov_avg: 1.6983
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.4157  valid loss: 1.1951  valid_loss_mov_avg: 1.6933
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.4441  valid loss: 1.1786  valid_loss_mov_avg: 1.6881
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.4017  valid loss: 1.1742  valid_loss_mov_avg: 1.6830
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.4108  valid loss: 1.1708  valid_loss_mov_avg: 1.6779
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3907  valid loss: 1.1915  valid_loss_mov_avg: 1.6730
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.4067  valid loss: 1.1761  valid_loss_mov_avg: 1.6680
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3744  valid loss: 1.1836  valid_loss_mov_avg: 1.6632
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.4075  valid loss: 1.1753  valid_loss_mov_avg: 1.6583
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3821  valid loss: 1.1888  valid_loss_mov_avg: 1.6536
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.4024  valid loss: 1.1767  valid_loss_mov_avg: 1.6488
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3718  valid loss: 1.1800  valid_loss_mov_avg: 1.6442
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3819  valid loss: 1.1814  valid_loss_mov_avg: 1.6395
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3804  valid loss: 1.1818  valid_loss_mov_avg: 1.6349
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3858  valid loss: 1.1869  valid_loss_mov_avg: 1.6305
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.4013  valid loss: 1.1758  valid_loss_mov_avg: 1.6259
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3713  valid loss: 1.1791  valid_loss_mov_avg: 1.6214
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3872  valid loss: 1.1763  valid_loss_mov_avg: 1.6170
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3490  valid loss: 1.1922  valid_loss_mov_avg: 1.6128
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3727  valid loss: 1.1839  valid_loss_mov_avg: 1.6085
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3895  valid loss: 1.1848  valid_loss_mov_avg: 1.6042
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3714  valid loss: 1.1820  valid_loss_mov_avg: 1.6000
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3610  valid loss: 1.1896  valid_loss_mov_avg: 1.5959
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3803  valid loss: 1.1807  valid_loss_mov_avg: 1.5917
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3487  valid loss: 1.1877  valid_loss_mov_avg: 1.5877
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3839  valid loss: 1.1784  valid_loss_mov_avg: 1.5836

 ---------------- Grad-Step 130 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 131 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 132 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 133 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 134 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 135 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 136 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 137 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 138 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 139 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.377         |
| Data-EnvSampler-Poli... | 1.8           |
| Data-EnvTrajs-Averag... | -20.3         |
| Data-EnvTrajs-MaxReturn | 17.6          |
| Data-EnvTrajs-MinReturn | -76.9         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 32.2          |
| Data-TimeEnvSampleProc  | 0.000609      |
| Data-TimeEnvSampling    | 3.16          |
| Iteration               | 13            |
| ItrTime                 | 103           |
| LossAfter               | -0.01700095   |
| LossBefore              | 2.6226044e-09 |
| Model-TimeModelFit      | 39.7          |
| ModelSampler-n_times... | 560000        |
| Policy-AverageAbsPol... | 0.8022011     |
| Policy-AverageDiscou... | -19.8         |
| Policy-AveragePolicyStd | 0.74464947    |
| Policy-AverageReturn    | -32.8         |
| Policy-MaxReturn        | 61.1          |
| Policy-MinReturn        | -103          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 40.2          |
| Policy-TimeAlgoOpt      | 1.06          |
| Policy-TimeSampleProc   | 0.155         |
| Policy-TimeSampling     | 58.7          |
| Policy-TimeStep         | 59.9          |
| Time                    | 1.24e+03      |
| n_timesteps             | 14000         |
-------------------------------------------

 ---------------- Iteration 14 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.5739  valid loss: 1.1910  valid_loss_mov_avg: 1.7806
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.5323  valid loss: 1.1904  valid_loss_mov_avg: 1.7747
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.5139  valid loss: 1.1904  valid_loss_mov_avg: 1.7688
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.5029  valid loss: 1.1914  valid_loss_mov_avg: 1.7631
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4819  valid loss: 1.2035  valid_loss_mov_avg: 1.7575
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.5211  valid loss: 1.1750  valid_loss_mov_avg: 1.7516
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4863  valid loss: 1.1749  valid_loss_mov_avg: 1.7459
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4386  valid loss: 1.1916  valid_loss_mov_avg: 1.7403
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4805  valid loss: 1.1919  valid_loss_mov_avg: 1.7348
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4895  valid loss: 1.1844  valid_loss_mov_avg: 1.7293
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.4320  valid loss: 1.2016  valid_loss_mov_avg: 1.7241
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.4844  valid loss: 1.1904  valid_loss_mov_avg: 1.7187
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.4570  valid loss: 1.1805  valid_loss_mov_avg: 1.7133
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.4678  valid loss: 1.1751  valid_loss_mov_avg: 1.7080
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.4386  valid loss: 1.1749  valid_loss_mov_avg: 1.7026
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.4174  valid loss: 1.1866  valid_loss_mov_avg: 1.6975
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.4462  valid loss: 1.1948  valid_loss_mov_avg: 1.6924
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.4405  valid loss: 1.1750  valid_loss_mov_avg: 1.6873
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.4115  valid loss: 1.1809  valid_loss_mov_avg: 1.6822
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.4356  valid loss: 1.1755  valid_loss_mov_avg: 1.6771
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.4124  valid loss: 1.1808  valid_loss_mov_avg: 1.6722
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.4058  valid loss: 1.1755  valid_loss_mov_avg: 1.6672
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.4092  valid loss: 1.1755  valid_loss_mov_avg: 1.6623
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.4217  valid loss: 1.1695  valid_loss_mov_avg: 1.6574
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3999  valid loss: 1.1954  valid_loss_mov_avg: 1.6527
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.4075  valid loss: 1.1881  valid_loss_mov_avg: 1.6481
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3782  valid loss: 1.1843  valid_loss_mov_avg: 1.6435
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3887  valid loss: 1.1795  valid_loss_mov_avg: 1.6388
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3710  valid loss: 1.1886  valid_loss_mov_avg: 1.6343
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.4206  valid loss: 1.1851  valid_loss_mov_avg: 1.6298
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3662  valid loss: 1.1948  valid_loss_mov_avg: 1.6255
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.4053  valid loss: 1.1835  valid_loss_mov_avg: 1.6211
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3589  valid loss: 1.1872  valid_loss_mov_avg: 1.6167
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3863  valid loss: 1.1924  valid_loss_mov_avg: 1.6125
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3717  valid loss: 1.1899  valid_loss_mov_avg: 1.6082
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3739  valid loss: 1.1978  valid_loss_mov_avg: 1.6041
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3760  valid loss: 1.2035  valid_loss_mov_avg: 1.6001
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3929  valid loss: 1.1859  valid_loss_mov_avg: 1.5960
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3583  valid loss: 1.1887  valid_loss_mov_avg: 1.5919
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3775  valid loss: 1.1869  valid_loss_mov_avg: 1.5879
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3654  valid loss: 1.1963  valid_loss_mov_avg: 1.5840
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3857  valid loss: 1.1887  valid_loss_mov_avg: 1.5800
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3569  valid loss: 1.1899  valid_loss_mov_avg: 1.5761
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3839  valid loss: 1.1827  valid_loss_mov_avg: 1.5722
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3648  valid loss: 1.1925  valid_loss_mov_avg: 1.5684
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3458  valid loss: 1.1884  valid_loss_mov_avg: 1.5646
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3613  valid loss: 1.1893  valid_loss_mov_avg: 1.5608
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3437  valid loss: 1.1959  valid_loss_mov_avg: 1.5572
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3536  valid loss: 1.1971  valid_loss_mov_avg: 1.5536
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3661  valid loss: 1.2087  valid_loss_mov_avg: 1.5501

 ---------------- Grad-Step 140 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 141 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 142 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 143 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 144 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 145 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 146 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 147 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 148 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 149 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.437         |
| Data-EnvSampler-Poli... | 2.13          |
| Data-EnvTrajs-Averag... | 2.16          |
| Data-EnvTrajs-MaxReturn | 42.1          |
| Data-EnvTrajs-MinReturn | -50.3         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 30            |
| Data-TimeEnvSampleProc  | 0.000631      |
| Data-TimeEnvSampling    | 3.6           |
| Iteration               | 14            |
| ItrTime                 | 102           |
| LossAfter               | -0.016915066  |
| LossBefore              | 1.1265278e-08 |
| Model-TimeModelFit      | 35.5          |
| ModelSampler-n_times... | 600000        |
| Policy-AverageAbsPol... | 0.82235414    |
| Policy-AverageDiscou... | 6.35          |
| Policy-AveragePolicyStd | 0.7232562     |
| Policy-AverageReturn    | 23.3          |
| Policy-MaxReturn        | 169           |
| Policy-MinReturn        | -56.8         |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 48.7          |
| Policy-TimeAlgoOpt      | 1.16          |
| Policy-TimeSampleProc   | 0.171         |
| Policy-TimeSampling     | 61.7          |
| Policy-TimeStep         | 63.1          |
| Time                    | 1.34e+03      |
| n_timesteps             | 15000         |
-------------------------------------------

 ---------------- Iteration 15 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.5691  valid loss: 1.2057  valid_loss_mov_avg: 1.8026
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.5144  valid loss: 1.1901  valid_loss_mov_avg: 1.7965
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.5074  valid loss: 1.2017  valid_loss_mov_avg: 1.7905
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.5078  valid loss: 1.1889  valid_loss_mov_avg: 1.7845
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.5044  valid loss: 1.1816  valid_loss_mov_avg: 1.7785
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4895  valid loss: 1.1872  valid_loss_mov_avg: 1.7726
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4899  valid loss: 1.1840  valid_loss_mov_avg: 1.7667
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4817  valid loss: 1.1832  valid_loss_mov_avg: 1.7608
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4623  valid loss: 1.1910  valid_loss_mov_avg: 1.7551
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4841  valid loss: 1.1781  valid_loss_mov_avg: 1.7494
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.4395  valid loss: 1.1919  valid_loss_mov_avg: 1.7438
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.4587  valid loss: 1.1911  valid_loss_mov_avg: 1.7383
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.4432  valid loss: 1.1818  valid_loss_mov_avg: 1.7327
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.4344  valid loss: 1.1805  valid_loss_mov_avg: 1.7272
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.4332  valid loss: 1.2052  valid_loss_mov_avg: 1.7220
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.4521  valid loss: 1.1824  valid_loss_mov_avg: 1.7166
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.4237  valid loss: 1.1951  valid_loss_mov_avg: 1.7114
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.4426  valid loss: 1.1859  valid_loss_mov_avg: 1.7061
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.4239  valid loss: 1.1831  valid_loss_mov_avg: 1.7009
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.4077  valid loss: 1.1840  valid_loss_mov_avg: 1.6957
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.4230  valid loss: 1.1791  valid_loss_mov_avg: 1.6905
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.4006  valid loss: 1.1808  valid_loss_mov_avg: 1.6854
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.4093  valid loss: 1.1967  valid_loss_mov_avg: 1.6806
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3993  valid loss: 1.1881  valid_loss_mov_avg: 1.6756
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.4168  valid loss: 1.1765  valid_loss_mov_avg: 1.6706
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.4095  valid loss: 1.1825  valid_loss_mov_avg: 1.6658
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3933  valid loss: 1.1815  valid_loss_mov_avg: 1.6609
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3710  valid loss: 1.1938  valid_loss_mov_avg: 1.6562
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.4094  valid loss: 1.1809  valid_loss_mov_avg: 1.6515
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.4040  valid loss: 1.1853  valid_loss_mov_avg: 1.6468
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3741  valid loss: 1.1865  valid_loss_mov_avg: 1.6422
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3956  valid loss: 1.1781  valid_loss_mov_avg: 1.6376
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3792  valid loss: 1.1805  valid_loss_mov_avg: 1.6330
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3738  valid loss: 1.1949  valid_loss_mov_avg: 1.6286
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3786  valid loss: 1.1832  valid_loss_mov_avg: 1.6242
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3591  valid loss: 1.1998  valid_loss_mov_avg: 1.6199
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3792  valid loss: 1.1923  valid_loss_mov_avg: 1.6157
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3877  valid loss: 1.1825  valid_loss_mov_avg: 1.6113
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3445  valid loss: 1.1950  valid_loss_mov_avg: 1.6072
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3721  valid loss: 1.1922  valid_loss_mov_avg: 1.6030
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3878  valid loss: 1.1841  valid_loss_mov_avg: 1.5988
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3748  valid loss: 1.1835  valid_loss_mov_avg: 1.5947
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3329  valid loss: 1.1986  valid_loss_mov_avg: 1.5907
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3756  valid loss: 1.1903  valid_loss_mov_avg: 1.5867
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3316  valid loss: 1.1904  valid_loss_mov_avg: 1.5827
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3671  valid loss: 1.1894  valid_loss_mov_avg: 1.5788
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3565  valid loss: 1.1834  valid_loss_mov_avg: 1.5749
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3404  valid loss: 1.1945  valid_loss_mov_avg: 1.5710
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3697  valid loss: 1.1901  valid_loss_mov_avg: 1.5672
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3480  valid loss: 1.1897  valid_loss_mov_avg: 1.5635

 ---------------- Grad-Step 150 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 151 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 152 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 153 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 154 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 155 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 156 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 157 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 158 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 159 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.392         |
| Data-EnvSampler-Poli... | 1.9           |
| Data-EnvTrajs-Averag... | 32.2          |
| Data-EnvTrajs-MaxReturn | 63.8          |
| Data-EnvTrajs-MinReturn | -13.8         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 25.6          |
| Data-TimeEnvSampleProc  | 0.00081       |
| Data-TimeEnvSampling    | 3.29          |
| Iteration               | 15            |
| ItrTime                 | 102           |
| LossAfter               | -0.017324004  |
| LossBefore              | 1.1175871e-08 |
| Model-TimeModelFit      | 40.6          |
| ModelSampler-n_times... | 640000        |
| Policy-AverageAbsPol... | 0.8401592     |
| Policy-AverageDiscou... | 44.1          |
| Policy-AveragePolicyStd | 0.6993573     |
| Policy-AverageReturn    | 113           |
| Policy-MaxReturn        | 170           |
| Policy-MinReturn        | 57.6          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 27.1          |
| Policy-TimeAlgoOpt      | 0.974         |
| Policy-TimeSampleProc   | 0.148         |
| Policy-TimeSampling     | 57.4          |
| Policy-TimeStep         | 58.5          |
| Time                    | 1.44e+03      |
| n_timesteps             | 16000         |
-------------------------------------------

 ---------------- Iteration 16 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.5679  valid loss: 1.1948  valid_loss_mov_avg: 1.7862
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.5176  valid loss: 1.2088  valid_loss_mov_avg: 1.7804
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.5073  valid loss: 1.1830  valid_loss_mov_avg: 1.7745
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4764  valid loss: 1.1878  valid_loss_mov_avg: 1.7686
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4741  valid loss: 1.1864  valid_loss_mov_avg: 1.7628
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4854  valid loss: 1.1922  valid_loss_mov_avg: 1.7571
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4818  valid loss: 1.1818  valid_loss_mov_avg: 1.7513
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4793  valid loss: 1.1728  valid_loss_mov_avg: 1.7455
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4540  valid loss: 1.1875  valid_loss_mov_avg: 1.7400
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4515  valid loss: 1.1714  valid_loss_mov_avg: 1.7343
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.4389  valid loss: 1.1911  valid_loss_mov_avg: 1.7288
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.4368  valid loss: 1.1830  valid_loss_mov_avg: 1.7234
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.4341  valid loss: 1.1820  valid_loss_mov_avg: 1.7180
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.4238  valid loss: 1.1786  valid_loss_mov_avg: 1.7126
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.4151  valid loss: 1.1824  valid_loss_mov_avg: 1.7073
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.4248  valid loss: 1.1825  valid_loss_mov_avg: 1.7020
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.4027  valid loss: 1.1877  valid_loss_mov_avg: 1.6969
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.4312  valid loss: 1.1845  valid_loss_mov_avg: 1.6918
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.4160  valid loss: 1.1829  valid_loss_mov_avg: 1.6867
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3868  valid loss: 1.1860  valid_loss_mov_avg: 1.6817
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.4595  valid loss: 1.1692  valid_loss_mov_avg: 1.6765
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.4098  valid loss: 1.1873  valid_loss_mov_avg: 1.6716
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3871  valid loss: 1.1962  valid_loss_mov_avg: 1.6669
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.4018  valid loss: 1.1950  valid_loss_mov_avg: 1.6622
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.4077  valid loss: 1.1853  valid_loss_mov_avg: 1.6574
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3940  valid loss: 1.1806  valid_loss_mov_avg: 1.6526
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3911  valid loss: 1.1903  valid_loss_mov_avg: 1.6480
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3943  valid loss: 1.1796  valid_loss_mov_avg: 1.6433
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3638  valid loss: 1.1810  valid_loss_mov_avg: 1.6387
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3653  valid loss: 1.1932  valid_loss_mov_avg: 1.6342
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3922  valid loss: 1.1855  valid_loss_mov_avg: 1.6298
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3585  valid loss: 1.1864  valid_loss_mov_avg: 1.6253
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3837  valid loss: 1.1908  valid_loss_mov_avg: 1.6210
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3768  valid loss: 1.1893  valid_loss_mov_avg: 1.6167
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3499  valid loss: 1.1947  valid_loss_mov_avg: 1.6124
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3678  valid loss: 1.1938  valid_loss_mov_avg: 1.6083
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3534  valid loss: 1.2022  valid_loss_mov_avg: 1.6042
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3567  valid loss: 1.1929  valid_loss_mov_avg: 1.6001
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3758  valid loss: 1.1922  valid_loss_mov_avg: 1.5960
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3467  valid loss: 1.1991  valid_loss_mov_avg: 1.5920
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3780  valid loss: 1.1914  valid_loss_mov_avg: 1.5880
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3483  valid loss: 1.1860  valid_loss_mov_avg: 1.5840
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3251  valid loss: 1.1988  valid_loss_mov_avg: 1.5802
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3597  valid loss: 1.1942  valid_loss_mov_avg: 1.5763
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3502  valid loss: 1.1936  valid_loss_mov_avg: 1.5725
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3595  valid loss: 1.1877  valid_loss_mov_avg: 1.5686
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3379  valid loss: 1.1943  valid_loss_mov_avg: 1.5649
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3301  valid loss: 1.2013  valid_loss_mov_avg: 1.5612
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3369  valid loss: 1.2044  valid_loss_mov_avg: 1.5577
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3325  valid loss: 1.2095  valid_loss_mov_avg: 1.5542

 ---------------- Grad-Step 160 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 161 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 162 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 163 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 164 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 165 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 166 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 167 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 168 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 169 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.369          |
| Data-EnvSampler-Poli... | 1.76           |
| Data-EnvTrajs-Averag... | 15.2           |
| Data-EnvTrajs-MaxReturn | 50.2           |
| Data-EnvTrajs-MinReturn | -15.9          |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 25.3           |
| Data-TimeEnvSampleProc  | 0.000565       |
| Data-TimeEnvSampling    | 3.12           |
| Iteration               | 16             |
| ItrTime                 | 102            |
| LossAfter               | -0.019017799   |
| LossBefore              | -6.6161157e-09 |
| Model-TimeModelFit      | 39             |
| ModelSampler-n_times... | 680000         |
| Policy-AverageAbsPol... | 0.8309569      |
| Policy-AverageDiscou... | 29.1           |
| Policy-AveragePolicyStd | 0.6763316      |
| Policy-AverageReturn    | 82.5           |
| Policy-MaxReturn        | 159            |
| Policy-MinReturn        | -4.03          |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 45.4           |
| Policy-TimeAlgoOpt      | 1.06           |
| Policy-TimeSampleProc   | 0.154          |
| Policy-TimeSampling     | 58.9           |
| Policy-TimeStep         | 60.1           |
| Time                    | 1.54e+03       |
| n_timesteps             | 17000          |
--------------------------------------------

 ---------------- Iteration 17 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.5771  valid loss: 1.2064  valid_loss_mov_avg: 1.8036
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.5325  valid loss: 1.1966  valid_loss_mov_avg: 1.7976
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.5059  valid loss: 1.1938  valid_loss_mov_avg: 1.7915
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.5019  valid loss: 1.1868  valid_loss_mov_avg: 1.7855
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4700  valid loss: 1.1937  valid_loss_mov_avg: 1.7796
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4955  valid loss: 1.1783  valid_loss_mov_avg: 1.7735
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4784  valid loss: 1.1878  valid_loss_mov_avg: 1.7677
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4717  valid loss: 1.1879  valid_loss_mov_avg: 1.7619
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4517  valid loss: 1.1871  valid_loss_mov_avg: 1.7561
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4596  valid loss: 1.1784  valid_loss_mov_avg: 1.7504
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.4536  valid loss: 1.1890  valid_loss_mov_avg: 1.7447
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.4615  valid loss: 1.1809  valid_loss_mov_avg: 1.7391
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.4214  valid loss: 1.1850  valid_loss_mov_avg: 1.7336
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.4386  valid loss: 1.1883  valid_loss_mov_avg: 1.7281
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.4172  valid loss: 1.1904  valid_loss_mov_avg: 1.7227
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.4374  valid loss: 1.1797  valid_loss_mov_avg: 1.7173
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3985  valid loss: 1.1883  valid_loss_mov_avg: 1.7120
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.4186  valid loss: 1.1822  valid_loss_mov_avg: 1.7067
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.4240  valid loss: 1.1749  valid_loss_mov_avg: 1.7014
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.4031  valid loss: 1.1808  valid_loss_mov_avg: 1.6962
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.4061  valid loss: 1.1835  valid_loss_mov_avg: 1.6911
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3747  valid loss: 1.1828  valid_loss_mov_avg: 1.6860
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.4045  valid loss: 1.1865  valid_loss_mov_avg: 1.6810
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.4088  valid loss: 1.1849  valid_loss_mov_avg: 1.6760
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.4097  valid loss: 1.1897  valid_loss_mov_avg: 1.6712
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3900  valid loss: 1.1933  valid_loss_mov_avg: 1.6664
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3845  valid loss: 1.1874  valid_loss_mov_avg: 1.6616
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.4031  valid loss: 1.1822  valid_loss_mov_avg: 1.6568
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3791  valid loss: 1.1820  valid_loss_mov_avg: 1.6521
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3810  valid loss: 1.1786  valid_loss_mov_avg: 1.6473
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3659  valid loss: 1.1906  valid_loss_mov_avg: 1.6428
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3854  valid loss: 1.1879  valid_loss_mov_avg: 1.6382
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3705  valid loss: 1.1877  valid_loss_mov_avg: 1.6337
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3576  valid loss: 1.1873  valid_loss_mov_avg: 1.6292
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3687  valid loss: 1.1899  valid_loss_mov_avg: 1.6248
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3510  valid loss: 1.2028  valid_loss_mov_avg: 1.6206
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3846  valid loss: 1.1861  valid_loss_mov_avg: 1.6163
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3464  valid loss: 1.1892  valid_loss_mov_avg: 1.6120
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3809  valid loss: 1.1818  valid_loss_mov_avg: 1.6077
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3498  valid loss: 1.1841  valid_loss_mov_avg: 1.6035
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3378  valid loss: 1.1993  valid_loss_mov_avg: 1.5994
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3631  valid loss: 1.1933  valid_loss_mov_avg: 1.5954
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3711  valid loss: 1.1848  valid_loss_mov_avg: 1.5913
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3551  valid loss: 1.1917  valid_loss_mov_avg: 1.5873
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3398  valid loss: 1.1972  valid_loss_mov_avg: 1.5834
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3592  valid loss: 1.1992  valid_loss_mov_avg: 1.5795
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3563  valid loss: 1.1898  valid_loss_mov_avg: 1.5756
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3315  valid loss: 1.2000  valid_loss_mov_avg: 1.5719
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3460  valid loss: 1.1911  valid_loss_mov_avg: 1.5681
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3521  valid loss: 1.1860  valid_loss_mov_avg: 1.5642

 ---------------- Grad-Step 170 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 171 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 172 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 173 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 174 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 175 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 176 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 177 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 178 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 179 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.385         |
| Data-EnvSampler-Poli... | 1.88          |
| Data-EnvTrajs-Averag... | -28.7         |
| Data-EnvTrajs-MaxReturn | 31.4          |
| Data-EnvTrajs-MinReturn | -79.7         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 36.8          |
| Data-TimeEnvSampleProc  | 0.000559      |
| Data-TimeEnvSampling    | 3.24          |
| Iteration               | 17            |
| ItrTime                 | 98.9          |
| LossAfter               | -0.018237084  |
| LossBefore              | 1.1324882e-09 |
| Model-TimeModelFit      | 36.3          |
| ModelSampler-n_times... | 720000        |
| Policy-AverageAbsPol... | 0.8231065     |
| Policy-AverageDiscou... | 35.7          |
| Policy-AveragePolicyStd | 0.65618914    |
| Policy-AverageReturn    | 98.5          |
| Policy-MaxReturn        | 183           |
| Policy-MinReturn        | 19.8          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 39.8          |
| Policy-TimeAlgoOpt      | 0.965         |
| Policy-TimeSampleProc   | 0.156         |
| Policy-TimeSampling     | 58.3          |
| Policy-TimeStep         | 59.4          |
| Time                    | 1.64e+03      |
| n_timesteps             | 18000         |
-------------------------------------------

 ---------------- Iteration 18 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.5728  valid loss: 1.2014  valid_loss_mov_avg: 1.7961
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.5078  valid loss: 1.2135  valid_loss_mov_avg: 1.7903
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4971  valid loss: 1.1965  valid_loss_mov_avg: 1.7843
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4899  valid loss: 1.1922  valid_loss_mov_avg: 1.7784
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4738  valid loss: 1.1928  valid_loss_mov_avg: 1.7726
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4596  valid loss: 1.2074  valid_loss_mov_avg: 1.7669
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4904  valid loss: 1.1869  valid_loss_mov_avg: 1.7611
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4339  valid loss: 1.1979  valid_loss_mov_avg: 1.7555
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4514  valid loss: 1.1954  valid_loss_mov_avg: 1.7499
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4703  valid loss: 1.1830  valid_loss_mov_avg: 1.7442
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.4396  valid loss: 1.1894  valid_loss_mov_avg: 1.7387
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.4174  valid loss: 1.1988  valid_loss_mov_avg: 1.7333
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.4259  valid loss: 1.2091  valid_loss_mov_avg: 1.7280
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.4450  valid loss: 1.1811  valid_loss_mov_avg: 1.7225
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.4245  valid loss: 1.1836  valid_loss_mov_avg: 1.7172
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.4145  valid loss: 1.1953  valid_loss_mov_avg: 1.7119
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.4240  valid loss: 1.1881  valid_loss_mov_avg: 1.7067
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3903  valid loss: 1.1885  valid_loss_mov_avg: 1.7015
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.4163  valid loss: 1.1851  valid_loss_mov_avg: 1.6964
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3791  valid loss: 1.1889  valid_loss_mov_avg: 1.6913
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.4017  valid loss: 1.1936  valid_loss_mov_avg: 1.6863
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3918  valid loss: 1.1942  valid_loss_mov_avg: 1.6814
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.4053  valid loss: 1.1885  valid_loss_mov_avg: 1.6765
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3857  valid loss: 1.1863  valid_loss_mov_avg: 1.6716
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3692  valid loss: 1.1998  valid_loss_mov_avg: 1.6668
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3835  valid loss: 1.2051  valid_loss_mov_avg: 1.6622
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.4093  valid loss: 1.1950  valid_loss_mov_avg: 1.6575
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3690  valid loss: 1.1965  valid_loss_mov_avg: 1.6529
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3792  valid loss: 1.1935  valid_loss_mov_avg: 1.6483
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3530  valid loss: 1.1992  valid_loss_mov_avg: 1.6438
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3935  valid loss: 1.1925  valid_loss_mov_avg: 1.6393
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3522  valid loss: 1.2008  valid_loss_mov_avg: 1.6349
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3901  valid loss: 1.1952  valid_loss_mov_avg: 1.6306
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3611  valid loss: 1.1873  valid_loss_mov_avg: 1.6261
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3697  valid loss: 1.1964  valid_loss_mov_avg: 1.6218
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3570  valid loss: 1.2021  valid_loss_mov_avg: 1.6176
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3589  valid loss: 1.1989  valid_loss_mov_avg: 1.6134
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3668  valid loss: 1.1930  valid_loss_mov_avg: 1.6092
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3600  valid loss: 1.1941  valid_loss_mov_avg: 1.6051
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3291  valid loss: 1.2085  valid_loss_mov_avg: 1.6011
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3475  valid loss: 1.2037  valid_loss_mov_avg: 1.5971
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3778  valid loss: 1.1973  valid_loss_mov_avg: 1.5931
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3483  valid loss: 1.2171  valid_loss_mov_avg: 1.5894
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3595  valid loss: 1.2013  valid_loss_mov_avg: 1.5855
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3564  valid loss: 1.1973  valid_loss_mov_avg: 1.5816
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3469  valid loss: 1.1921  valid_loss_mov_avg: 1.5777
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3490  valid loss: 1.2015  valid_loss_mov_avg: 1.5740
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3364  valid loss: 1.1976  valid_loss_mov_avg: 1.5702
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3549  valid loss: 1.1997  valid_loss_mov_avg: 1.5665
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3330  valid loss: 1.2028  valid_loss_mov_avg: 1.5629

 ---------------- Grad-Step 180 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 181 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 182 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 183 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 184 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 185 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 186 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 187 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 188 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 189 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.413         |
| Data-EnvSampler-Poli... | 2             |
| Data-EnvTrajs-Averag... | 5.79          |
| Data-EnvTrajs-MaxReturn | 29.9          |
| Data-EnvTrajs-MinReturn | -16.1         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 16            |
| Data-TimeEnvSampleProc  | 0.000512      |
| Data-TimeEnvSampling    | 3.46          |
| Iteration               | 18            |
| ItrTime                 | 113           |
| LossAfter               | -0.019919438  |
| LossBefore              | 4.7683715e-09 |
| Model-TimeModelFit      | 45            |
| ModelSampler-n_times... | 760000        |
| Policy-AverageAbsPol... | 0.82517594    |
| Policy-AverageDiscou... | 4.18          |
| Policy-AveragePolicyStd | 0.6365052     |
| Policy-AverageReturn    | 36.3          |
| Policy-MaxReturn        | 149           |
| Policy-MinReturn        | -92.2         |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 52.3          |
| Policy-TimeAlgoOpt      | 1.07          |
| Policy-TimeSampleProc   | 0.168         |
| Policy-TimeSampling     | 63.7          |
| Policy-TimeStep         | 65            |
| Time                    | 1.76e+03      |
| n_timesteps             | 19000         |
-------------------------------------------

 ---------------- Iteration 19 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.5264  valid loss: 1.2277  valid_loss_mov_avg: 1.8354
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.5005  valid loss: 1.2041  valid_loss_mov_avg: 1.8291
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4867  valid loss: 1.1996  valid_loss_mov_avg: 1.8228
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4720  valid loss: 1.2041  valid_loss_mov_avg: 1.8166
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4628  valid loss: 1.2038  valid_loss_mov_avg: 1.8105
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4445  valid loss: 1.2051  valid_loss_mov_avg: 1.8044
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4496  valid loss: 1.2086  valid_loss_mov_avg: 1.7985
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4252  valid loss: 1.2494  valid_loss_mov_avg: 1.7930
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4693  valid loss: 1.2116  valid_loss_mov_avg: 1.7872
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4465  valid loss: 1.2193  valid_loss_mov_avg: 1.7815
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.4286  valid loss: 1.1995  valid_loss_mov_avg: 1.7757
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.4214  valid loss: 1.2052  valid_loss_mov_avg: 1.7700
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.4335  valid loss: 1.2091  valid_loss_mov_avg: 1.7644
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.4157  valid loss: 1.2020  valid_loss_mov_avg: 1.7587
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.4059  valid loss: 1.2147  valid_loss_mov_avg: 1.7533
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.4180  valid loss: 1.2065  valid_loss_mov_avg: 1.7478
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.4266  valid loss: 1.1985  valid_loss_mov_avg: 1.7423
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3999  valid loss: 1.2173  valid_loss_mov_avg: 1.7371
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3936  valid loss: 1.2051  valid_loss_mov_avg: 1.7318
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.4053  valid loss: 1.2001  valid_loss_mov_avg: 1.7265
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3803  valid loss: 1.2122  valid_loss_mov_avg: 1.7213
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.4049  valid loss: 1.2009  valid_loss_mov_avg: 1.7161
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3895  valid loss: 1.2049  valid_loss_mov_avg: 1.7110
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3721  valid loss: 1.2142  valid_loss_mov_avg: 1.7060
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3721  valid loss: 1.2124  valid_loss_mov_avg: 1.7011
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3907  valid loss: 1.2013  valid_loss_mov_avg: 1.6961
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3557  valid loss: 1.2159  valid_loss_mov_avg: 1.6913
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3780  valid loss: 1.2058  valid_loss_mov_avg: 1.6864
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3784  valid loss: 1.2193  valid_loss_mov_avg: 1.6818
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3737  valid loss: 1.2055  valid_loss_mov_avg: 1.6770
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3442  valid loss: 1.2193  valid_loss_mov_avg: 1.6724
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3798  valid loss: 1.2105  valid_loss_mov_avg: 1.6678
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3762  valid loss: 1.2174  valid_loss_mov_avg: 1.6633
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3530  valid loss: 1.2208  valid_loss_mov_avg: 1.6589
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3638  valid loss: 1.2150  valid_loss_mov_avg: 1.6544
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3445  valid loss: 1.2204  valid_loss_mov_avg: 1.6501
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3605  valid loss: 1.2185  valid_loss_mov_avg: 1.6458
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3383  valid loss: 1.2153  valid_loss_mov_avg: 1.6415
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3543  valid loss: 1.2085  valid_loss_mov_avg: 1.6372
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3349  valid loss: 1.2152  valid_loss_mov_avg: 1.6329
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3399  valid loss: 1.2142  valid_loss_mov_avg: 1.6287
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3532  valid loss: 1.2132  valid_loss_mov_avg: 1.6246
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3220  valid loss: 1.2152  valid_loss_mov_avg: 1.6205
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3505  valid loss: 1.2147  valid_loss_mov_avg: 1.6164
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3346  valid loss: 1.2135  valid_loss_mov_avg: 1.6124
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3338  valid loss: 1.2176  valid_loss_mov_avg: 1.6085
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3446  valid loss: 1.2193  valid_loss_mov_avg: 1.6046
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3154  valid loss: 1.2181  valid_loss_mov_avg: 1.6007
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3388  valid loss: 1.2205  valid_loss_mov_avg: 1.5969
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3233  valid loss: 1.2137  valid_loss_mov_avg: 1.5931

 ---------------- Grad-Step 190 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 191 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 192 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 193 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 194 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 195 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 196 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 197 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 198 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 199 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
------------------------------------------
| Data-EnvSampler-EnvE... | 0.407        |
| Data-EnvSampler-Poli... | 1.99         |
| Data-EnvTrajs-Averag... | 54.5         |
| Data-EnvTrajs-MaxReturn | 82.3         |
| Data-EnvTrajs-MinReturn | 21.9         |
| Data-EnvTrajs-NumTrajs  | 5            |
| Data-EnvTrajs-StdReturn | 26.4         |
| Data-TimeEnvSampleProc  | 0.000816     |
| Data-TimeEnvSampling    | 3.48         |
| Iteration               | 19           |
| ItrTime                 | 112          |
| LossAfter               | -0.018461158 |
| LossBefore              | 7.390976e-09 |
| Model-TimeModelFit      | 44.8         |
| ModelSampler-n_times... | 800000       |
| Policy-AverageAbsPol... | 0.76421356   |
| Policy-AverageDiscou... | 15.5         |
| Policy-AveragePolicyStd | 0.61647195   |
| Policy-AverageReturn    | 55           |
| Policy-MaxReturn        | 127          |
| Policy-MinReturn        | -53.5        |
| Policy-NumTrajs         | 20           |
| Policy-StdReturn        | 42.9         |
| Policy-TimeAlgoOpt      | 1.18         |
| Policy-TimeSampleProc   | 0.19         |
| Policy-TimeSampling     | 62.7         |
| Policy-TimeStep         | 64.1         |
| Time                    | 1.87e+03     |
| n_timesteps             | 20000        |
------------------------------------------

 ---------------- Iteration 20 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.5279  valid loss: 1.2162  valid_loss_mov_avg: 1.8182
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4944  valid loss: 1.2147  valid_loss_mov_avg: 1.8122
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4716  valid loss: 1.2087  valid_loss_mov_avg: 1.8061
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4589  valid loss: 1.2236  valid_loss_mov_avg: 1.8003
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4441  valid loss: 1.2200  valid_loss_mov_avg: 1.7945
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4469  valid loss: 1.2160  valid_loss_mov_avg: 1.7887
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4681  valid loss: 1.2032  valid_loss_mov_avg: 1.7829
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4240  valid loss: 1.2262  valid_loss_mov_avg: 1.7773
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4498  valid loss: 1.2161  valid_loss_mov_avg: 1.7717
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4155  valid loss: 1.2201  valid_loss_mov_avg: 1.7662
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.4428  valid loss: 1.2011  valid_loss_mov_avg: 1.7605
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.4107  valid loss: 1.2089  valid_loss_mov_avg: 1.7550
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.4160  valid loss: 1.1973  valid_loss_mov_avg: 1.7494
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3881  valid loss: 1.2075  valid_loss_mov_avg: 1.7440
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.4114  valid loss: 1.2141  valid_loss_mov_avg: 1.7387
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3837  valid loss: 1.2034  valid_loss_mov_avg: 1.7333
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3867  valid loss: 1.2041  valid_loss_mov_avg: 1.7281
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3793  valid loss: 1.2011  valid_loss_mov_avg: 1.7228
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3794  valid loss: 1.2077  valid_loss_mov_avg: 1.7176
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3859  valid loss: 1.2054  valid_loss_mov_avg: 1.7125
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3735  valid loss: 1.2150  valid_loss_mov_avg: 1.7075
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3772  valid loss: 1.2031  valid_loss_mov_avg: 1.7025
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3686  valid loss: 1.2117  valid_loss_mov_avg: 1.6976
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3695  valid loss: 1.2096  valid_loss_mov_avg: 1.6927
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3612  valid loss: 1.2185  valid_loss_mov_avg: 1.6880
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3897  valid loss: 1.2102  valid_loss_mov_avg: 1.6832
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3646  valid loss: 1.2084  valid_loss_mov_avg: 1.6784
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3411  valid loss: 1.2169  valid_loss_mov_avg: 1.6738
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3760  valid loss: 1.2148  valid_loss_mov_avg: 1.6692
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3435  valid loss: 1.2163  valid_loss_mov_avg: 1.6647
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3658  valid loss: 1.2100  valid_loss_mov_avg: 1.6602
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3396  valid loss: 1.2090  valid_loss_mov_avg: 1.6556
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3707  valid loss: 1.2102  valid_loss_mov_avg: 1.6512
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3457  valid loss: 1.2155  valid_loss_mov_avg: 1.6468
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3488  valid loss: 1.2138  valid_loss_mov_avg: 1.6425
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3403  valid loss: 1.2158  valid_loss_mov_avg: 1.6382
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3429  valid loss: 1.2172  valid_loss_mov_avg: 1.6340
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3344  valid loss: 1.2236  valid_loss_mov_avg: 1.6299
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3672  valid loss: 1.2122  valid_loss_mov_avg: 1.6257
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3254  valid loss: 1.2231  valid_loss_mov_avg: 1.6217
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3536  valid loss: 1.2210  valid_loss_mov_avg: 1.6177
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3258  valid loss: 1.2232  valid_loss_mov_avg: 1.6138
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3240  valid loss: 1.2175  valid_loss_mov_avg: 1.6098
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3484  valid loss: 1.2159  valid_loss_mov_avg: 1.6059
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3341  valid loss: 1.2163  valid_loss_mov_avg: 1.6020
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3242  valid loss: 1.2142  valid_loss_mov_avg: 1.5981
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3030  valid loss: 1.2247  valid_loss_mov_avg: 1.5944
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3281  valid loss: 1.2164  valid_loss_mov_avg: 1.5906
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3330  valid loss: 1.2125  valid_loss_mov_avg: 1.5868
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3145  valid loss: 1.2183  valid_loss_mov_avg: 1.5831

 ---------------- Grad-Step 200 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 201 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 202 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 203 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 204 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 205 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 206 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 207 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 208 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 209 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.407         |
| Data-EnvSampler-Poli... | 2.01          |
| Data-EnvTrajs-Averag... | 11.9          |
| Data-EnvTrajs-MaxReturn | 60.8          |
| Data-EnvTrajs-MinReturn | -20.8         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 31.5          |
| Data-TimeEnvSampleProc  | 0.000633      |
| Data-TimeEnvSampling    | 3.46          |
| Iteration               | 20            |
| ItrTime                 | 115           |
| LossAfter               | -0.016953072  |
| LossBefore              | 1.4901161e-09 |
| Model-TimeModelFit      | 42.6          |
| ModelSampler-n_times... | 840000        |
| Policy-AverageAbsPol... | 0.7531618     |
| Policy-AverageDiscou... | 39.3          |
| Policy-AveragePolicyStd | 0.6015304     |
| Policy-AverageReturn    | 116           |
| Policy-MaxReturn        | 193           |
| Policy-MinReturn        | 15.5          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 44.5          |
| Policy-TimeAlgoOpt      | 1.25          |
| Policy-TimeSampleProc   | 0.165         |
| Policy-TimeSampling     | 67.2          |
| Policy-TimeStep         | 68.6          |
| Time                    | 1.98e+03      |
| n_timesteps             | 21000         |
-------------------------------------------

 ---------------- Iteration 21 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.5170  valid loss: 1.2452  valid_loss_mov_avg: 1.8615
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4724  valid loss: 1.2288  valid_loss_mov_avg: 1.8552
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4531  valid loss: 1.2199  valid_loss_mov_avg: 1.8488
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4584  valid loss: 1.2185  valid_loss_mov_avg: 1.8425
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4601  valid loss: 1.2314  valid_loss_mov_avg: 1.8364
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4200  valid loss: 1.2104  valid_loss_mov_avg: 1.8302
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4175  valid loss: 1.2142  valid_loss_mov_avg: 1.8240
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4154  valid loss: 1.2091  valid_loss_mov_avg: 1.8179
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3967  valid loss: 1.2209  valid_loss_mov_avg: 1.8119
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4323  valid loss: 1.2115  valid_loss_mov_avg: 1.8059
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3862  valid loss: 1.2221  valid_loss_mov_avg: 1.8000
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.4105  valid loss: 1.2191  valid_loss_mov_avg: 1.7942
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3906  valid loss: 1.2259  valid_loss_mov_avg: 1.7885
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.4137  valid loss: 1.2079  valid_loss_mov_avg: 1.7827
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.4042  valid loss: 1.2060  valid_loss_mov_avg: 1.7770
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.4003  valid loss: 1.2045  valid_loss_mov_avg: 1.7713
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3891  valid loss: 1.2160  valid_loss_mov_avg: 1.7657
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3594  valid loss: 1.2155  valid_loss_mov_avg: 1.7602
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3830  valid loss: 1.2167  valid_loss_mov_avg: 1.7548
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3668  valid loss: 1.2179  valid_loss_mov_avg: 1.7494
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3900  valid loss: 1.2075  valid_loss_mov_avg: 1.7440
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3756  valid loss: 1.2086  valid_loss_mov_avg: 1.7386
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3594  valid loss: 1.2109  valid_loss_mov_avg: 1.7333
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3560  valid loss: 1.2123  valid_loss_mov_avg: 1.7281
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3694  valid loss: 1.2155  valid_loss_mov_avg: 1.7230
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3419  valid loss: 1.2115  valid_loss_mov_avg: 1.7179
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3588  valid loss: 1.2149  valid_loss_mov_avg: 1.7129
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3636  valid loss: 1.2253  valid_loss_mov_avg: 1.7080
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3513  valid loss: 1.2165  valid_loss_mov_avg: 1.7031
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3457  valid loss: 1.2195  valid_loss_mov_avg: 1.6982
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3560  valid loss: 1.2135  valid_loss_mov_avg: 1.6934
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3321  valid loss: 1.2192  valid_loss_mov_avg: 1.6886
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3309  valid loss: 1.2238  valid_loss_mov_avg: 1.6840
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3406  valid loss: 1.2168  valid_loss_mov_avg: 1.6793
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3508  valid loss: 1.2195  valid_loss_mov_avg: 1.6747
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3204  valid loss: 1.2219  valid_loss_mov_avg: 1.6702
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3356  valid loss: 1.2249  valid_loss_mov_avg: 1.6657
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3426  valid loss: 1.2134  valid_loss_mov_avg: 1.6612
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3264  valid loss: 1.2182  valid_loss_mov_avg: 1.6568
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3232  valid loss: 1.2117  valid_loss_mov_avg: 1.6523
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3252  valid loss: 1.2187  valid_loss_mov_avg: 1.6480
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3230  valid loss: 1.2263  valid_loss_mov_avg: 1.6438
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3313  valid loss: 1.2186  valid_loss_mov_avg: 1.6395
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3190  valid loss: 1.2164  valid_loss_mov_avg: 1.6353
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3097  valid loss: 1.2213  valid_loss_mov_avg: 1.6312
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3150  valid loss: 1.2209  valid_loss_mov_avg: 1.6271
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3223  valid loss: 1.2244  valid_loss_mov_avg: 1.6230
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3113  valid loss: 1.2194  valid_loss_mov_avg: 1.6190
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3109  valid loss: 1.2247  valid_loss_mov_avg: 1.6151
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3200  valid loss: 1.2202  valid_loss_mov_avg: 1.6111

 ---------------- Grad-Step 210 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 211 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 212 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 213 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 214 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 215 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 216 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 217 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 218 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 219 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.415         |
| Data-EnvSampler-Poli... | 2.03          |
| Data-EnvTrajs-Averag... | 35            |
| Data-EnvTrajs-MaxReturn | 91.2          |
| Data-EnvTrajs-MinReturn | -33.5         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 47.9          |
| Data-TimeEnvSampleProc  | 0.000511      |
| Data-TimeEnvSampling    | 3.5           |
| Iteration               | 21            |
| ItrTime                 | 110           |
| LossAfter               | -0.013190497  |
| LossBefore              | 1.5735626e-08 |
| Model-TimeModelFit      | 42.3          |
| ModelSampler-n_times... | 880000        |
| Policy-AverageAbsPol... | 0.75114495    |
| Policy-AverageDiscou... | 10.5          |
| Policy-AveragePolicyStd | 0.58714086    |
| Policy-AverageReturn    | 31.7          |
| Policy-MaxReturn        | 99.7          |
| Policy-MinReturn        | -64.4         |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 41.5          |
| Policy-TimeAlgoOpt      | 1.17          |
| Policy-TimeSampleProc   | 0.178         |
| Policy-TimeSampling     | 63.2          |
| Policy-TimeStep         | 64.5          |
| Time                    | 2.09e+03      |
| n_timesteps             | 22000         |
-------------------------------------------

 ---------------- Iteration 22 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4996  valid loss: 1.2508  valid_loss_mov_avg: 1.8700
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4735  valid loss: 1.2200  valid_loss_mov_avg: 1.8635
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4540  valid loss: 1.2322  valid_loss_mov_avg: 1.8572
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4478  valid loss: 1.2336  valid_loss_mov_avg: 1.8510
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4300  valid loss: 1.2285  valid_loss_mov_avg: 1.8447
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4296  valid loss: 1.2300  valid_loss_mov_avg: 1.8386
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4388  valid loss: 1.2121  valid_loss_mov_avg: 1.8323
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4202  valid loss: 1.2096  valid_loss_mov_avg: 1.8261
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4195  valid loss: 1.2088  valid_loss_mov_avg: 1.8199
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3879  valid loss: 1.2079  valid_loss_mov_avg: 1.8138
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.4090  valid loss: 1.2072  valid_loss_mov_avg: 1.8077
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3993  valid loss: 1.2112  valid_loss_mov_avg: 1.8018
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3862  valid loss: 1.2270  valid_loss_mov_avg: 1.7960
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3885  valid loss: 1.2190  valid_loss_mov_avg: 1.7903
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3918  valid loss: 1.2130  valid_loss_mov_avg: 1.7845
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3852  valid loss: 1.2103  valid_loss_mov_avg: 1.7787
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3672  valid loss: 1.2112  valid_loss_mov_avg: 1.7731
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3551  valid loss: 1.2153  valid_loss_mov_avg: 1.7675
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3771  valid loss: 1.2337  valid_loss_mov_avg: 1.7621
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3838  valid loss: 1.2149  valid_loss_mov_avg: 1.7567
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3795  valid loss: 1.2083  valid_loss_mov_avg: 1.7512
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3663  valid loss: 1.2092  valid_loss_mov_avg: 1.7458
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3628  valid loss: 1.2140  valid_loss_mov_avg: 1.7405
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3433  valid loss: 1.2193  valid_loss_mov_avg: 1.7352
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3712  valid loss: 1.2146  valid_loss_mov_avg: 1.7300
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3327  valid loss: 1.2185  valid_loss_mov_avg: 1.7249
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3489  valid loss: 1.2192  valid_loss_mov_avg: 1.7199
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3355  valid loss: 1.2242  valid_loss_mov_avg: 1.7149
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3404  valid loss: 1.2275  valid_loss_mov_avg: 1.7100
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3462  valid loss: 1.2256  valid_loss_mov_avg: 1.7052
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3527  valid loss: 1.2165  valid_loss_mov_avg: 1.7003
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3295  valid loss: 1.2164  valid_loss_mov_avg: 1.6955
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3138  valid loss: 1.2234  valid_loss_mov_avg: 1.6907
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3422  valid loss: 1.2169  valid_loss_mov_avg: 1.6860
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3268  valid loss: 1.2207  valid_loss_mov_avg: 1.6814
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3394  valid loss: 1.2134  valid_loss_mov_avg: 1.6767
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3219  valid loss: 1.2152  valid_loss_mov_avg: 1.6721
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3023  valid loss: 1.2211  valid_loss_mov_avg: 1.6675
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3265  valid loss: 1.2179  valid_loss_mov_avg: 1.6631
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3166  valid loss: 1.2213  valid_loss_mov_avg: 1.6586
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3280  valid loss: 1.2198  valid_loss_mov_avg: 1.6542
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.2988  valid loss: 1.2249  valid_loss_mov_avg: 1.6500
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3306  valid loss: 1.2196  valid_loss_mov_avg: 1.6456
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3212  valid loss: 1.2221  valid_loss_mov_avg: 1.6414
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3101  valid loss: 1.2233  valid_loss_mov_avg: 1.6372
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3091  valid loss: 1.2234  valid_loss_mov_avg: 1.6331
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3198  valid loss: 1.2205  valid_loss_mov_avg: 1.6290
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2893  valid loss: 1.2319  valid_loss_mov_avg: 1.6250
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3324  valid loss: 1.2232  valid_loss_mov_avg: 1.6210
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3091  valid loss: 1.2218  valid_loss_mov_avg: 1.6170

 ---------------- Grad-Step 220 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 221 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 222 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 223 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 224 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 225 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 226 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 227 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 228 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 229 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.512         |
| Data-EnvSampler-Poli... | 2.57          |
| Data-EnvTrajs-Averag... | 85.8          |
| Data-EnvTrajs-MaxReturn | 130           |
| Data-EnvTrajs-MinReturn | 19.3          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 36.6          |
| Data-TimeEnvSampleProc  | 0.000624      |
| Data-TimeEnvSampling    | 4.36          |
| Iteration               | 22            |
| ItrTime                 | 115           |
| LossAfter               | -0.015240261  |
| LossBefore              | 2.7418137e-09 |
| Model-TimeModelFit      | 46.9          |
| ModelSampler-n_times... | 920000        |
| Policy-AverageAbsPol... | 0.7664811     |
| Policy-AverageDiscou... | 47.6          |
| Policy-AveragePolicyStd | 0.57151794    |
| Policy-AverageReturn    | 132           |
| Policy-MaxReturn        | 210           |
| Policy-MinReturn        | 56.4          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 47.4          |
| Policy-TimeAlgoOpt      | 1.12          |
| Policy-TimeSampleProc   | 0.165         |
| Policy-TimeSampling     | 62.7          |
| Policy-TimeStep         | 64            |
| Time                    | 2.21e+03      |
| n_timesteps             | 23000         |
-------------------------------------------

 ---------------- Iteration 23 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4969  valid loss: 1.2401  valid_loss_mov_avg: 1.8539
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4636  valid loss: 1.2289  valid_loss_mov_avg: 1.8476
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4433  valid loss: 1.2198  valid_loss_mov_avg: 1.8414
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4465  valid loss: 1.2116  valid_loss_mov_avg: 1.8351
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4460  valid loss: 1.2209  valid_loss_mov_avg: 1.8289
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4237  valid loss: 1.2166  valid_loss_mov_avg: 1.8228
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4193  valid loss: 1.2224  valid_loss_mov_avg: 1.8168
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4331  valid loss: 1.2178  valid_loss_mov_avg: 1.8108
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3990  valid loss: 1.2201  valid_loss_mov_avg: 1.8049
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4243  valid loss: 1.2121  valid_loss_mov_avg: 1.7990
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3972  valid loss: 1.2172  valid_loss_mov_avg: 1.7931
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3988  valid loss: 1.2130  valid_loss_mov_avg: 1.7873
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3983  valid loss: 1.2046  valid_loss_mov_avg: 1.7815
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3741  valid loss: 1.2115  valid_loss_mov_avg: 1.7758
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3782  valid loss: 1.2083  valid_loss_mov_avg: 1.7701
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3788  valid loss: 1.2074  valid_loss_mov_avg: 1.7645
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3722  valid loss: 1.2091  valid_loss_mov_avg: 1.7590
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3567  valid loss: 1.2170  valid_loss_mov_avg: 1.7535
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3658  valid loss: 1.2160  valid_loss_mov_avg: 1.7482
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3566  valid loss: 1.2129  valid_loss_mov_avg: 1.7428
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3685  valid loss: 1.2106  valid_loss_mov_avg: 1.7375
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3352  valid loss: 1.2145  valid_loss_mov_avg: 1.7323
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3561  valid loss: 1.2145  valid_loss_mov_avg: 1.7271
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3329  valid loss: 1.2298  valid_loss_mov_avg: 1.7221
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3541  valid loss: 1.2266  valid_loss_mov_avg: 1.7172
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3386  valid loss: 1.2276  valid_loss_mov_avg: 1.7123
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3577  valid loss: 1.2232  valid_loss_mov_avg: 1.7074
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3367  valid loss: 1.2190  valid_loss_mov_avg: 1.7025
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3413  valid loss: 1.2251  valid_loss_mov_avg: 1.6977
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3469  valid loss: 1.2205  valid_loss_mov_avg: 1.6929
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3248  valid loss: 1.2237  valid_loss_mov_avg: 1.6882
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3156  valid loss: 1.2298  valid_loss_mov_avg: 1.6837
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3474  valid loss: 1.2284  valid_loss_mov_avg: 1.6791
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3237  valid loss: 1.2274  valid_loss_mov_avg: 1.6746
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3383  valid loss: 1.2198  valid_loss_mov_avg: 1.6700
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3218  valid loss: 1.2237  valid_loss_mov_avg: 1.6656
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3200  valid loss: 1.2290  valid_loss_mov_avg: 1.6612
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3195  valid loss: 1.2281  valid_loss_mov_avg: 1.6569
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3086  valid loss: 1.2281  valid_loss_mov_avg: 1.6526
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3220  valid loss: 1.2307  valid_loss_mov_avg: 1.6484
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3085  valid loss: 1.2342  valid_loss_mov_avg: 1.6442
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3248  valid loss: 1.2249  valid_loss_mov_avg: 1.6400
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3120  valid loss: 1.2276  valid_loss_mov_avg: 1.6359
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3056  valid loss: 1.2279  valid_loss_mov_avg: 1.6318
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3105  valid loss: 1.2358  valid_loss_mov_avg: 1.6279
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3120  valid loss: 1.2259  valid_loss_mov_avg: 1.6239
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2957  valid loss: 1.2312  valid_loss_mov_avg: 1.6199
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3105  valid loss: 1.2258  valid_loss_mov_avg: 1.6160
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2965  valid loss: 1.2273  valid_loss_mov_avg: 1.6121
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3119  valid loss: 1.2314  valid_loss_mov_avg: 1.6083

 ---------------- Grad-Step 230 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 231 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 232 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 233 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 234 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 235 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 236 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 237 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 238 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 239 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.407         |
| Data-EnvSampler-Poli... | 1.98          |
| Data-EnvTrajs-Averag... | 91.9          |
| Data-EnvTrajs-MaxReturn | 133           |
| Data-EnvTrajs-MinReturn | 49            |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 30.1          |
| Data-TimeEnvSampleProc  | 0.000558      |
| Data-TimeEnvSampling    | 3.49          |
| Iteration               | 23            |
| ItrTime                 | 119           |
| LossAfter               | -0.017345166  |
| LossBefore              | 2.1457671e-09 |
| Model-TimeModelFit      | 49.1          |
| ModelSampler-n_times... | 960000        |
| Policy-AverageAbsPol... | 0.78892434    |
| Policy-AverageDiscou... | 9.4           |
| Policy-AveragePolicyStd | 0.5545005     |
| Policy-AverageReturn    | 39.5          |
| Policy-MaxReturn        | 79.5          |
| Policy-MinReturn        | -25           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 29.3          |
| Policy-TimeAlgoOpt      | 1.19          |
| Policy-TimeSampleProc   | 0.156         |
| Policy-TimeSampling     | 65            |
| Policy-TimeStep         | 66.3          |
| Time                    | 2.33e+03      |
| n_timesteps             | 24000         |
-------------------------------------------

 ---------------- Iteration 24 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.5061  valid loss: 1.2466  valid_loss_mov_avg: 1.8636
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4646  valid loss: 1.2476  valid_loss_mov_avg: 1.8575
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4504  valid loss: 1.2463  valid_loss_mov_avg: 1.8513
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4291  valid loss: 1.2381  valid_loss_mov_avg: 1.8452
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4515  valid loss: 1.2219  valid_loss_mov_avg: 1.8390
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4133  valid loss: 1.2338  valid_loss_mov_avg: 1.8329
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4181  valid loss: 1.2224  valid_loss_mov_avg: 1.8268
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4126  valid loss: 1.2362  valid_loss_mov_avg: 1.8209
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.4498  valid loss: 1.2243  valid_loss_mov_avg: 1.8150
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4235  valid loss: 1.2186  valid_loss_mov_avg: 1.8090
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3934  valid loss: 1.2244  valid_loss_mov_avg: 1.8031
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3956  valid loss: 1.2121  valid_loss_mov_avg: 1.7972
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3699  valid loss: 1.2210  valid_loss_mov_avg: 1.7915
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3922  valid loss: 1.2155  valid_loss_mov_avg: 1.7857
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3631  valid loss: 1.2311  valid_loss_mov_avg: 1.7802
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3696  valid loss: 1.2406  valid_loss_mov_avg: 1.7748
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3956  valid loss: 1.2155  valid_loss_mov_avg: 1.7692
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3565  valid loss: 1.2172  valid_loss_mov_avg: 1.7637
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3639  valid loss: 1.2133  valid_loss_mov_avg: 1.7581
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3566  valid loss: 1.2312  valid_loss_mov_avg: 1.7529
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3625  valid loss: 1.2204  valid_loss_mov_avg: 1.7476
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3486  valid loss: 1.2218  valid_loss_mov_avg: 1.7423
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3605  valid loss: 1.2285  valid_loss_mov_avg: 1.7372
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3525  valid loss: 1.2155  valid_loss_mov_avg: 1.7319
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3362  valid loss: 1.2282  valid_loss_mov_avg: 1.7269
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3453  valid loss: 1.2237  valid_loss_mov_avg: 1.7219
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3454  valid loss: 1.2177  valid_loss_mov_avg: 1.7168
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3368  valid loss: 1.2193  valid_loss_mov_avg: 1.7119
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3359  valid loss: 1.2303  valid_loss_mov_avg: 1.7070
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3213  valid loss: 1.2285  valid_loss_mov_avg: 1.7023
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3198  valid loss: 1.2244  valid_loss_mov_avg: 1.6975
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3175  valid loss: 1.2256  valid_loss_mov_avg: 1.6928
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3348  valid loss: 1.2266  valid_loss_mov_avg: 1.6881
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3291  valid loss: 1.2295  valid_loss_mov_avg: 1.6835
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3057  valid loss: 1.2354  valid_loss_mov_avg: 1.6790
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3208  valid loss: 1.2335  valid_loss_mov_avg: 1.6746
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3291  valid loss: 1.2293  valid_loss_mov_avg: 1.6701
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3204  valid loss: 1.2261  valid_loss_mov_avg: 1.6657
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.2996  valid loss: 1.2374  valid_loss_mov_avg: 1.6614
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3255  valid loss: 1.2241  valid_loss_mov_avg: 1.6570
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3155  valid loss: 1.2295  valid_loss_mov_avg: 1.6528
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3160  valid loss: 1.2296  valid_loss_mov_avg: 1.6485
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2885  valid loss: 1.2365  valid_loss_mov_avg: 1.6444
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3166  valid loss: 1.2310  valid_loss_mov_avg: 1.6403
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2957  valid loss: 1.2340  valid_loss_mov_avg: 1.6362
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2941  valid loss: 1.2402  valid_loss_mov_avg: 1.6322
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3258  valid loss: 1.2329  valid_loss_mov_avg: 1.6283
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3089  valid loss: 1.2330  valid_loss_mov_avg: 1.6243
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2982  valid loss: 1.2322  valid_loss_mov_avg: 1.6204
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3110  valid loss: 1.2330  valid_loss_mov_avg: 1.6165

 ---------------- Grad-Step 240 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 241 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 242 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 243 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 244 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 245 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 246 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 247 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 248 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 249 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.554          |
| Data-EnvSampler-Poli... | 2.71           |
| Data-EnvTrajs-Averag... | 150            |
| Data-EnvTrajs-MaxReturn | 189            |
| Data-EnvTrajs-MinReturn | 122            |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 23.9           |
| Data-TimeEnvSampleProc  | 0.00051        |
| Data-TimeEnvSampling    | 4.51           |
| Iteration               | 24             |
| ItrTime                 | 112            |
| LossAfter               | -0.01749781    |
| LossBefore              | -4.3809414e-09 |
| Model-TimeModelFit      | 43.8           |
| ModelSampler-n_times... | 1000000        |
| Policy-AverageAbsPol... | 0.7825973      |
| Policy-AverageDiscou... | 25.9           |
| Policy-AveragePolicyStd | 0.5339812      |
| Policy-AverageReturn    | 80.8           |
| Policy-MaxReturn        | 161            |
| Policy-MinReturn        | -1.41          |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 44.9           |
| Policy-TimeAlgoOpt      | 1.22           |
| Policy-TimeSampleProc   | 0.174          |
| Policy-TimeSampling     | 61.9           |
| Policy-TimeStep         | 63.3           |
| Time                    | 2.44e+03       |
| n_timesteps             | 25000          |
--------------------------------------------

 ---------------- Iteration 25 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4887  valid loss: 1.2297  valid_loss_mov_avg: 1.8384
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4453  valid loss: 1.2370  valid_loss_mov_avg: 1.8324
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4153  valid loss: 1.2368  valid_loss_mov_avg: 1.8265
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4354  valid loss: 1.2258  valid_loss_mov_avg: 1.8205
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4129  valid loss: 1.2237  valid_loss_mov_avg: 1.8145
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3934  valid loss: 1.2231  valid_loss_mov_avg: 1.8086
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3926  valid loss: 1.2214  valid_loss_mov_avg: 1.8027
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3786  valid loss: 1.2294  valid_loss_mov_avg: 1.7970
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3912  valid loss: 1.2225  valid_loss_mov_avg: 1.7912
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3940  valid loss: 1.2226  valid_loss_mov_avg: 1.7855
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3733  valid loss: 1.2198  valid_loss_mov_avg: 1.7799
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3736  valid loss: 1.2142  valid_loss_mov_avg: 1.7742
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3621  valid loss: 1.2352  valid_loss_mov_avg: 1.7688
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3699  valid loss: 1.2184  valid_loss_mov_avg: 1.7633
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3535  valid loss: 1.2234  valid_loss_mov_avg: 1.7579
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3608  valid loss: 1.2154  valid_loss_mov_avg: 1.7525
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3462  valid loss: 1.2231  valid_loss_mov_avg: 1.7472
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3530  valid loss: 1.2255  valid_loss_mov_avg: 1.7420
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3387  valid loss: 1.2255  valid_loss_mov_avg: 1.7368
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3493  valid loss: 1.2238  valid_loss_mov_avg: 1.7317
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3499  valid loss: 1.2224  valid_loss_mov_avg: 1.7266
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3340  valid loss: 1.2233  valid_loss_mov_avg: 1.7216
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3206  valid loss: 1.2225  valid_loss_mov_avg: 1.7166
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3337  valid loss: 1.2275  valid_loss_mov_avg: 1.7117
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3396  valid loss: 1.2171  valid_loss_mov_avg: 1.7068
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3197  valid loss: 1.2275  valid_loss_mov_avg: 1.7020
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3263  valid loss: 1.2238  valid_loss_mov_avg: 1.6972
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3087  valid loss: 1.2330  valid_loss_mov_avg: 1.6925
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3157  valid loss: 1.2344  valid_loss_mov_avg: 1.6880
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3267  valid loss: 1.2259  valid_loss_mov_avg: 1.6833
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3109  valid loss: 1.2310  valid_loss_mov_avg: 1.6788
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.2989  valid loss: 1.2305  valid_loss_mov_avg: 1.6743
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3262  valid loss: 1.2298  valid_loss_mov_avg: 1.6699
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3042  valid loss: 1.2353  valid_loss_mov_avg: 1.6655
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3219  valid loss: 1.2348  valid_loss_mov_avg: 1.6612
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3001  valid loss: 1.2300  valid_loss_mov_avg: 1.6569
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3052  valid loss: 1.2308  valid_loss_mov_avg: 1.6527
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3058  valid loss: 1.2358  valid_loss_mov_avg: 1.6485
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3023  valid loss: 1.2291  valid_loss_mov_avg: 1.6443
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3040  valid loss: 1.2308  valid_loss_mov_avg: 1.6402
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.2778  valid loss: 1.2333  valid_loss_mov_avg: 1.6361
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3048  valid loss: 1.2329  valid_loss_mov_avg: 1.6321
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2917  valid loss: 1.2341  valid_loss_mov_avg: 1.6281
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3006  valid loss: 1.2310  valid_loss_mov_avg: 1.6241
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2810  valid loss: 1.2351  valid_loss_mov_avg: 1.6202
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2997  valid loss: 1.2358  valid_loss_mov_avg: 1.6164
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2756  valid loss: 1.2410  valid_loss_mov_avg: 1.6126
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3014  valid loss: 1.2351  valid_loss_mov_avg: 1.6088
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2901  valid loss: 1.2384  valid_loss_mov_avg: 1.6051
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2713  valid loss: 1.2402  valid_loss_mov_avg: 1.6015

 ---------------- Grad-Step 250 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 251 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 252 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 253 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 254 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 255 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 256 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 257 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 258 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 259 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.414         |
| Data-EnvSampler-Poli... | 2.02          |
| Data-EnvTrajs-Averag... | 176           |
| Data-EnvTrajs-MaxReturn | 236           |
| Data-EnvTrajs-MinReturn | 139           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 36.9          |
| Data-TimeEnvSampleProc  | 0.000512      |
| Data-TimeEnvSampling    | 3.55          |
| Iteration               | 25            |
| ItrTime                 | 113           |
| LossAfter               | -0.017797096  |
| LossBefore              | -0.0028050628 |
| Model-TimeModelFit      | 42.5          |
| ModelSampler-n_times... | 1040000       |
| Policy-AverageAbsPol... | 0.7504774     |
| Policy-AverageDiscou... | 71.9          |
| Policy-AveragePolicyStd | 0.51585704    |
| Policy-AverageReturn    | 199           |
| Policy-MaxReturn        | 251           |
| Policy-MinReturn        | 155           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 26.4          |
| Policy-TimeAlgoOpt      | 1.23          |
| Policy-TimeSampleProc   | 0.18          |
| Policy-TimeSampling     | 65.9          |
| Policy-TimeStep         | 67.3          |
| Time                    | 2.55e+03      |
| n_timesteps             | 26000         |
-------------------------------------------

 ---------------- Iteration 26 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4969  valid loss: 1.2527  valid_loss_mov_avg: 1.8728
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4623  valid loss: 1.2386  valid_loss_mov_avg: 1.8664
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4465  valid loss: 1.2484  valid_loss_mov_avg: 1.8603
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4514  valid loss: 1.2382  valid_loss_mov_avg: 1.8540
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4136  valid loss: 1.2320  valid_loss_mov_avg: 1.8478
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4421  valid loss: 1.2353  valid_loss_mov_avg: 1.8417
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4137  valid loss: 1.2268  valid_loss_mov_avg: 1.8356
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4097  valid loss: 1.2237  valid_loss_mov_avg: 1.8294
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3883  valid loss: 1.2235  valid_loss_mov_avg: 1.8234
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.4056  valid loss: 1.2167  valid_loss_mov_avg: 1.8173
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3960  valid loss: 1.2317  valid_loss_mov_avg: 1.8115
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3782  valid loss: 1.2264  valid_loss_mov_avg: 1.8056
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3892  valid loss: 1.2198  valid_loss_mov_avg: 1.7997
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3747  valid loss: 1.2313  valid_loss_mov_avg: 1.7941
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3916  valid loss: 1.2176  valid_loss_mov_avg: 1.7883
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3661  valid loss: 1.2226  valid_loss_mov_avg: 1.7826
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3484  valid loss: 1.2275  valid_loss_mov_avg: 1.7771
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3475  valid loss: 1.2303  valid_loss_mov_avg: 1.7716
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3463  valid loss: 1.2250  valid_loss_mov_avg: 1.7662
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3533  valid loss: 1.2278  valid_loss_mov_avg: 1.7608
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3426  valid loss: 1.2251  valid_loss_mov_avg: 1.7554
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3400  valid loss: 1.2412  valid_loss_mov_avg: 1.7503
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3552  valid loss: 1.2313  valid_loss_mov_avg: 1.7451
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3472  valid loss: 1.2234  valid_loss_mov_avg: 1.7399
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3283  valid loss: 1.2234  valid_loss_mov_avg: 1.7347
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3357  valid loss: 1.2271  valid_loss_mov_avg: 1.7296
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3381  valid loss: 1.2241  valid_loss_mov_avg: 1.7246
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3348  valid loss: 1.2266  valid_loss_mov_avg: 1.7196
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3196  valid loss: 1.2304  valid_loss_mov_avg: 1.7147
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3254  valid loss: 1.2286  valid_loss_mov_avg: 1.7098
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3214  valid loss: 1.2297  valid_loss_mov_avg: 1.7050
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3190  valid loss: 1.2293  valid_loss_mov_avg: 1.7003
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.2999  valid loss: 1.2314  valid_loss_mov_avg: 1.6956
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3141  valid loss: 1.2315  valid_loss_mov_avg: 1.6909
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3249  valid loss: 1.2296  valid_loss_mov_avg: 1.6863
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3036  valid loss: 1.2286  valid_loss_mov_avg: 1.6818
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3199  valid loss: 1.2296  valid_loss_mov_avg: 1.6772
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.2948  valid loss: 1.2313  valid_loss_mov_avg: 1.6728
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3002  valid loss: 1.2343  valid_loss_mov_avg: 1.6684
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3022  valid loss: 1.2390  valid_loss_mov_avg: 1.6641
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3060  valid loss: 1.2349  valid_loss_mov_avg: 1.6598
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3067  valid loss: 1.2381  valid_loss_mov_avg: 1.6556
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3182  valid loss: 1.2307  valid_loss_mov_avg: 1.6513
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2922  valid loss: 1.2398  valid_loss_mov_avg: 1.6472
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2993  valid loss: 1.2383  valid_loss_mov_avg: 1.6431
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3115  valid loss: 1.2299  valid_loss_mov_avg: 1.6390
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2951  valid loss: 1.2345  valid_loss_mov_avg: 1.6350
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3020  valid loss: 1.2326  valid_loss_mov_avg: 1.6309
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2859  valid loss: 1.2334  valid_loss_mov_avg: 1.6270
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2951  valid loss: 1.2333  valid_loss_mov_avg: 1.6230

 ---------------- Grad-Step 260 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 261 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 262 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 263 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 264 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 265 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 266 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 267 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 268 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 269 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.376         |
| Data-EnvSampler-Poli... | 1.8           |
| Data-EnvTrajs-Averag... | 152           |
| Data-EnvTrajs-MaxReturn | 210           |
| Data-EnvTrajs-MinReturn | 57.9          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 58.2          |
| Data-TimeEnvSampleProc  | 0.000663      |
| Data-TimeEnvSampling    | 3.16          |
| Iteration               | 26            |
| ItrTime                 | 99.1          |
| LossAfter               | -0.01824716   |
| LossBefore              | 6.9737434e-09 |
| Model-TimeModelFit      | 37.7          |
| ModelSampler-n_times... | 1080000       |
| Policy-AverageAbsPol... | 0.74517596    |
| Policy-AverageDiscou... | 82.5          |
| Policy-AveragePolicyStd | 0.49631348    |
| Policy-AverageReturn    | 226           |
| Policy-MaxReturn        | 263           |
| Policy-MinReturn        | 157           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 22.7          |
| Policy-TimeAlgoOpt      | 0.963         |
| Policy-TimeSampleProc   | 0.137         |
| Policy-TimeSampling     | 57.1          |
| Policy-TimeStep         | 58.2          |
| Time                    | 2.65e+03      |
| n_timesteps             | 27000         |
-------------------------------------------

 ---------------- Iteration 27 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4902  valid loss: 1.2339  valid_loss_mov_avg: 1.8446
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4544  valid loss: 1.2352  valid_loss_mov_avg: 1.8385
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4393  valid loss: 1.2408  valid_loss_mov_avg: 1.8326
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4462  valid loss: 1.2283  valid_loss_mov_avg: 1.8265
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4282  valid loss: 1.2293  valid_loss_mov_avg: 1.8205
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4179  valid loss: 1.2219  valid_loss_mov_avg: 1.8146
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4200  valid loss: 1.2234  valid_loss_mov_avg: 1.8086
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4050  valid loss: 1.2157  valid_loss_mov_avg: 1.8027
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3991  valid loss: 1.2208  valid_loss_mov_avg: 1.7969
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3806  valid loss: 1.2118  valid_loss_mov_avg: 1.7910
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3883  valid loss: 1.2237  valid_loss_mov_avg: 1.7854
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3630  valid loss: 1.2228  valid_loss_mov_avg: 1.7797
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3603  valid loss: 1.2382  valid_loss_mov_avg: 1.7743
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3624  valid loss: 1.2267  valid_loss_mov_avg: 1.7689
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3652  valid loss: 1.2269  valid_loss_mov_avg: 1.7634
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3489  valid loss: 1.2307  valid_loss_mov_avg: 1.7581
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3684  valid loss: 1.2285  valid_loss_mov_avg: 1.7528
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3498  valid loss: 1.2314  valid_loss_mov_avg: 1.7476
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3380  valid loss: 1.2236  valid_loss_mov_avg: 1.7424
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3269  valid loss: 1.2378  valid_loss_mov_avg: 1.7373
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3684  valid loss: 1.2328  valid_loss_mov_avg: 1.7323
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3429  valid loss: 1.2302  valid_loss_mov_avg: 1.7272
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3531  valid loss: 1.2376  valid_loss_mov_avg: 1.7223
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3432  valid loss: 1.2256  valid_loss_mov_avg: 1.7174
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3351  valid loss: 1.2272  valid_loss_mov_avg: 1.7125
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3307  valid loss: 1.2305  valid_loss_mov_avg: 1.7077
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3308  valid loss: 1.2353  valid_loss_mov_avg: 1.7029
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3418  valid loss: 1.2248  valid_loss_mov_avg: 1.6982
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3241  valid loss: 1.2284  valid_loss_mov_avg: 1.6935
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3239  valid loss: 1.2374  valid_loss_mov_avg: 1.6889
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3415  valid loss: 1.2316  valid_loss_mov_avg: 1.6843
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3223  valid loss: 1.2279  valid_loss_mov_avg: 1.6798
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3272  valid loss: 1.2298  valid_loss_mov_avg: 1.6753
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3118  valid loss: 1.2340  valid_loss_mov_avg: 1.6708
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3169  valid loss: 1.2419  valid_loss_mov_avg: 1.6666
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3293  valid loss: 1.2373  valid_loss_mov_avg: 1.6623
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3070  valid loss: 1.2357  valid_loss_mov_avg: 1.6580
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3208  valid loss: 1.2311  valid_loss_mov_avg: 1.6537
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.2982  valid loss: 1.2370  valid_loss_mov_avg: 1.6496
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3072  valid loss: 1.2427  valid_loss_mov_avg: 1.6455
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3218  valid loss: 1.2302  valid_loss_mov_avg: 1.6413
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3023  valid loss: 1.2305  valid_loss_mov_avg: 1.6372
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2992  valid loss: 1.2396  valid_loss_mov_avg: 1.6333
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3074  valid loss: 1.2377  valid_loss_mov_avg: 1.6293
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2954  valid loss: 1.2445  valid_loss_mov_avg: 1.6255
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3176  valid loss: 1.2385  valid_loss_mov_avg: 1.6216
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2954  valid loss: 1.2330  valid_loss_mov_avg: 1.6177
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2793  valid loss: 1.2395  valid_loss_mov_avg: 1.6139
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3099  valid loss: 1.2353  valid_loss_mov_avg: 1.6101
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2896  valid loss: 1.2398  valid_loss_mov_avg: 1.6064

 ---------------- Grad-Step 270 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 271 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 272 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 273 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 274 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 275 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 276 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 277 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 278 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 279 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.484         |
| Data-EnvSampler-Poli... | 2.36          |
| Data-EnvTrajs-Averag... | 215           |
| Data-EnvTrajs-MaxReturn | 244           |
| Data-EnvTrajs-MinReturn | 179           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 23.1          |
| Data-TimeEnvSampleProc  | 0.000684      |
| Data-TimeEnvSampling    | 3.96          |
| Iteration               | 27            |
| ItrTime                 | 99.3          |
| LossAfter               | -0.017757645  |
| LossBefore              | 1.4305115e-09 |
| Model-TimeModelFit      | 36.2          |
| ModelSampler-n_times... | 1120000       |
| Policy-AverageAbsPol... | 0.769014      |
| Policy-AverageDiscou... | 89.5          |
| Policy-AveragePolicyStd | 0.47594523    |
| Policy-AverageReturn    | 242           |
| Policy-MaxReturn        | 273           |
| Policy-MinReturn        | 207           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 18.8          |
| Policy-TimeAlgoOpt      | 0.951         |
| Policy-TimeSampleProc   | 0.133         |
| Policy-TimeSampling     | 58            |
| Policy-TimeStep         | 59.1          |
| Time                    | 2.75e+03      |
| n_timesteps             | 28000         |
-------------------------------------------

 ---------------- Iteration 28 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4922  valid loss: 1.2654  valid_loss_mov_avg: 1.8918
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4535  valid loss: 1.2268  valid_loss_mov_avg: 1.8852
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4338  valid loss: 1.2297  valid_loss_mov_avg: 1.8786
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4219  valid loss: 1.2263  valid_loss_mov_avg: 1.8721
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4088  valid loss: 1.2329  valid_loss_mov_avg: 1.8657
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3975  valid loss: 1.2271  valid_loss_mov_avg: 1.8593
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3941  valid loss: 1.2333  valid_loss_mov_avg: 1.8530
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3915  valid loss: 1.2222  valid_loss_mov_avg: 1.8467
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3929  valid loss: 1.2203  valid_loss_mov_avg: 1.8405
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3800  valid loss: 1.2198  valid_loss_mov_avg: 1.8343
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3638  valid loss: 1.2231  valid_loss_mov_avg: 1.8282
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3581  valid loss: 1.2275  valid_loss_mov_avg: 1.8221
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3536  valid loss: 1.2242  valid_loss_mov_avg: 1.8162
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3612  valid loss: 1.2253  valid_loss_mov_avg: 1.8103
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3662  valid loss: 1.2239  valid_loss_mov_avg: 1.8044
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3522  valid loss: 1.2247  valid_loss_mov_avg: 1.7986
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3531  valid loss: 1.2321  valid_loss_mov_avg: 1.7929
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3524  valid loss: 1.2260  valid_loss_mov_avg: 1.7873
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3380  valid loss: 1.2230  valid_loss_mov_avg: 1.7816
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3288  valid loss: 1.2289  valid_loss_mov_avg: 1.7761
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3548  valid loss: 1.2339  valid_loss_mov_avg: 1.7707
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3283  valid loss: 1.2263  valid_loss_mov_avg: 1.7652
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3292  valid loss: 1.2323  valid_loss_mov_avg: 1.7599
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3500  valid loss: 1.2340  valid_loss_mov_avg: 1.7546
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3135  valid loss: 1.2356  valid_loss_mov_avg: 1.7494
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3222  valid loss: 1.2295  valid_loss_mov_avg: 1.7442
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3257  valid loss: 1.2304  valid_loss_mov_avg: 1.7391
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3138  valid loss: 1.2446  valid_loss_mov_avg: 1.7342
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3291  valid loss: 1.2325  valid_loss_mov_avg: 1.7291
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3180  valid loss: 1.2281  valid_loss_mov_avg: 1.7241
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3154  valid loss: 1.2352  valid_loss_mov_avg: 1.7192
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3144  valid loss: 1.2343  valid_loss_mov_avg: 1.7144
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3066  valid loss: 1.2314  valid_loss_mov_avg: 1.7096
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.2960  valid loss: 1.2399  valid_loss_mov_avg: 1.7049
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3077  valid loss: 1.2389  valid_loss_mov_avg: 1.7002
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.2986  valid loss: 1.2377  valid_loss_mov_avg: 1.6956
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3048  valid loss: 1.2376  valid_loss_mov_avg: 1.6910
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3154  valid loss: 1.2416  valid_loss_mov_avg: 1.6865
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3035  valid loss: 1.2344  valid_loss_mov_avg: 1.6820
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.2995  valid loss: 1.2362  valid_loss_mov_avg: 1.6775
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.2834  valid loss: 1.2386  valid_loss_mov_avg: 1.6731
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.2983  valid loss: 1.2427  valid_loss_mov_avg: 1.6688
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2986  valid loss: 1.2448  valid_loss_mov_avg: 1.6646
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3153  valid loss: 1.2402  valid_loss_mov_avg: 1.6604
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2942  valid loss: 1.2389  valid_loss_mov_avg: 1.6561
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2953  valid loss: 1.2400  valid_loss_mov_avg: 1.6520
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2778  valid loss: 1.2429  valid_loss_mov_avg: 1.6479
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2937  valid loss: 1.2406  valid_loss_mov_avg: 1.6438
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2817  valid loss: 1.2420  valid_loss_mov_avg: 1.6398
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2782  valid loss: 1.2434  valid_loss_mov_avg: 1.6358

 ---------------- Grad-Step 280 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 281 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 282 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 283 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 284 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 285 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 286 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 287 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 288 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 289 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.377         |
| Data-EnvSampler-Poli... | 1.86          |
| Data-EnvTrajs-Averag... | 244           |
| Data-EnvTrajs-MaxReturn | 365           |
| Data-EnvTrajs-MinReturn | 124           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 88.6          |
| Data-TimeEnvSampleProc  | 0.000557      |
| Data-TimeEnvSampling    | 3.24          |
| Iteration               | 28            |
| ItrTime                 | 102           |
| LossAfter               | -0.018154498  |
| LossBefore              | 3.8146974e-09 |
| Model-TimeModelFit      | 39.6          |
| ModelSampler-n_times... | 1160000       |
| Policy-AverageAbsPol... | 0.7892238     |
| Policy-AverageDiscou... | 126           |
| Policy-AveragePolicyStd | 0.45797938    |
| Policy-AverageReturn    | 322           |
| Policy-MaxReturn        | 398           |
| Policy-MinReturn        | 183           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 43.6          |
| Policy-TimeAlgoOpt      | 0.952         |
| Policy-TimeSampleProc   | 0.195         |
| Policy-TimeSampling     | 58            |
| Policy-TimeStep         | 59.2          |
| Time                    | 2.85e+03      |
| n_timesteps             | 29000         |
-------------------------------------------

 ---------------- Iteration 29 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4643  valid loss: 1.2498  valid_loss_mov_avg: 1.8684
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4668  valid loss: 1.2436  valid_loss_mov_avg: 1.8622
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4221  valid loss: 1.2481  valid_loss_mov_avg: 1.8561
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4114  valid loss: 1.2507  valid_loss_mov_avg: 1.8500
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4200  valid loss: 1.2216  valid_loss_mov_avg: 1.8437
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4038  valid loss: 1.2262  valid_loss_mov_avg: 1.8375
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3969  valid loss: 1.2257  valid_loss_mov_avg: 1.8314
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3902  valid loss: 1.2186  valid_loss_mov_avg: 1.8253
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3720  valid loss: 1.2304  valid_loss_mov_avg: 1.8193
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3840  valid loss: 1.2197  valid_loss_mov_avg: 1.8134
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3670  valid loss: 1.2191  valid_loss_mov_avg: 1.8074
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3806  valid loss: 1.2275  valid_loss_mov_avg: 1.8016
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3707  valid loss: 1.2230  valid_loss_mov_avg: 1.7958
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3481  valid loss: 1.2256  valid_loss_mov_avg: 1.7901
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3615  valid loss: 1.2227  valid_loss_mov_avg: 1.7844
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3389  valid loss: 1.2295  valid_loss_mov_avg: 1.7789
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3657  valid loss: 1.2339  valid_loss_mov_avg: 1.7734
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3446  valid loss: 1.2308  valid_loss_mov_avg: 1.7680
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3559  valid loss: 1.2304  valid_loss_mov_avg: 1.7626
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3352  valid loss: 1.2223  valid_loss_mov_avg: 1.7572
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3273  valid loss: 1.2243  valid_loss_mov_avg: 1.7519
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3275  valid loss: 1.2276  valid_loss_mov_avg: 1.7467
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3261  valid loss: 1.2349  valid_loss_mov_avg: 1.7416
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3371  valid loss: 1.2308  valid_loss_mov_avg: 1.7364
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3270  valid loss: 1.2340  valid_loss_mov_avg: 1.7314
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3281  valid loss: 1.2328  valid_loss_mov_avg: 1.7264
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3341  valid loss: 1.2373  valid_loss_mov_avg: 1.7215
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3097  valid loss: 1.2286  valid_loss_mov_avg: 1.7166
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3051  valid loss: 1.2307  valid_loss_mov_avg: 1.7118
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3264  valid loss: 1.2327  valid_loss_mov_avg: 1.7070
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3277  valid loss: 1.2295  valid_loss_mov_avg: 1.7022
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3157  valid loss: 1.2296  valid_loss_mov_avg: 1.6975
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3069  valid loss: 1.2340  valid_loss_mov_avg: 1.6928
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3170  valid loss: 1.2303  valid_loss_mov_avg: 1.6882
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3203  valid loss: 1.2273  valid_loss_mov_avg: 1.6836
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3111  valid loss: 1.2270  valid_loss_mov_avg: 1.6790
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.2968  valid loss: 1.2323  valid_loss_mov_avg: 1.6746
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.2977  valid loss: 1.2305  valid_loss_mov_avg: 1.6701
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3117  valid loss: 1.2329  valid_loss_mov_avg: 1.6657
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3053  valid loss: 1.2354  valid_loss_mov_avg: 1.6614
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.2838  valid loss: 1.2390  valid_loss_mov_avg: 1.6572
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3133  valid loss: 1.2317  valid_loss_mov_avg: 1.6530
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2823  valid loss: 1.2418  valid_loss_mov_avg: 1.6489
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2915  valid loss: 1.2403  valid_loss_mov_avg: 1.6448
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3069  valid loss: 1.2365  valid_loss_mov_avg: 1.6407
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2907  valid loss: 1.2338  valid_loss_mov_avg: 1.6366
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2909  valid loss: 1.2386  valid_loss_mov_avg: 1.6326
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2845  valid loss: 1.2275  valid_loss_mov_avg: 1.6286
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2866  valid loss: 1.2368  valid_loss_mov_avg: 1.6247
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2753  valid loss: 1.2329  valid_loss_mov_avg: 1.6207

 ---------------- Grad-Step 290 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 291 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 292 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 293 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 294 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 295 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 296 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 297 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 298 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 299 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.377         |
| Data-EnvSampler-Poli... | 1.8           |
| Data-EnvTrajs-Averag... | 248           |
| Data-EnvTrajs-MaxReturn | 322           |
| Data-EnvTrajs-MinReturn | 121           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 67.3          |
| Data-TimeEnvSampleProc  | 0.000579      |
| Data-TimeEnvSampling    | 3.15          |
| Iteration               | 29            |
| ItrTime                 | 101           |
| LossAfter               | -0.016630348  |
| LossBefore              | -5.662441e-09 |
| Model-TimeModelFit      | 36.7          |
| ModelSampler-n_times... | 1200000       |
| Policy-AverageAbsPol... | 0.8132299     |
| Policy-AverageDiscou... | 154           |
| Policy-AveragePolicyStd | 0.44378796    |
| Policy-AverageReturn    | 400           |
| Policy-MaxReturn        | 438           |
| Policy-MinReturn        | 368           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 19            |
| Policy-TimeAlgoOpt      | 1.02          |
| Policy-TimeSampleProc   | 0.217         |
| Policy-TimeSampling     | 60.4          |
| Policy-TimeStep         | 61.6          |
| Time                    | 2.96e+03      |
| n_timesteps             | 30000         |
-------------------------------------------

 ---------------- Iteration 30 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4580  valid loss: 1.2413  valid_loss_mov_avg: 1.8557
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4452  valid loss: 1.2284  valid_loss_mov_avg: 1.8494
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4315  valid loss: 1.2204  valid_loss_mov_avg: 1.8431
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4094  valid loss: 1.2309  valid_loss_mov_avg: 1.8370
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4022  valid loss: 1.2246  valid_loss_mov_avg: 1.8309
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3830  valid loss: 1.2211  valid_loss_mov_avg: 1.8248
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3881  valid loss: 1.2335  valid_loss_mov_avg: 1.8189
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3994  valid loss: 1.2224  valid_loss_mov_avg: 1.8129
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3623  valid loss: 1.2261  valid_loss_mov_avg: 1.8070
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3686  valid loss: 1.2270  valid_loss_mov_avg: 1.8012
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3711  valid loss: 1.2346  valid_loss_mov_avg: 1.7956
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3756  valid loss: 1.2339  valid_loss_mov_avg: 1.7900
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3736  valid loss: 1.2272  valid_loss_mov_avg: 1.7843
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3427  valid loss: 1.2286  valid_loss_mov_avg: 1.7788
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3498  valid loss: 1.2310  valid_loss_mov_avg: 1.7733
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3622  valid loss: 1.2233  valid_loss_mov_avg: 1.7678
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3410  valid loss: 1.2380  valid_loss_mov_avg: 1.7625
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3486  valid loss: 1.2288  valid_loss_mov_avg: 1.7572
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3350  valid loss: 1.2345  valid_loss_mov_avg: 1.7519
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3303  valid loss: 1.2341  valid_loss_mov_avg: 1.7468
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3436  valid loss: 1.2328  valid_loss_mov_avg: 1.7416
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3263  valid loss: 1.2325  valid_loss_mov_avg: 1.7365
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3294  valid loss: 1.2346  valid_loss_mov_avg: 1.7315
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3128  valid loss: 1.2382  valid_loss_mov_avg: 1.7266
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3202  valid loss: 1.2504  valid_loss_mov_avg: 1.7218
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3336  valid loss: 1.2297  valid_loss_mov_avg: 1.7169
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3089  valid loss: 1.2382  valid_loss_mov_avg: 1.7121
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3187  valid loss: 1.2274  valid_loss_mov_avg: 1.7073
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3003  valid loss: 1.2368  valid_loss_mov_avg: 1.7026
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3190  valid loss: 1.2318  valid_loss_mov_avg: 1.6979
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3246  valid loss: 1.2317  valid_loss_mov_avg: 1.6932
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3115  valid loss: 1.2380  valid_loss_mov_avg: 1.6886
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3165  valid loss: 1.2306  valid_loss_mov_avg: 1.6841
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3022  valid loss: 1.2321  valid_loss_mov_avg: 1.6795
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.2997  valid loss: 1.2329  valid_loss_mov_avg: 1.6751
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.2990  valid loss: 1.2398  valid_loss_mov_avg: 1.6707
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.2858  valid loss: 1.2381  valid_loss_mov_avg: 1.6664
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3129  valid loss: 1.2378  valid_loss_mov_avg: 1.6621
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.2989  valid loss: 1.2422  valid_loss_mov_avg: 1.6579
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.2948  valid loss: 1.2402  valid_loss_mov_avg: 1.6537
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.2948  valid loss: 1.2363  valid_loss_mov_avg: 1.6496
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.2812  valid loss: 1.2432  valid_loss_mov_avg: 1.6455
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2952  valid loss: 1.2385  valid_loss_mov_avg: 1.6414
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2920  valid loss: 1.2350  valid_loss_mov_avg: 1.6374
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2702  valid loss: 1.2367  valid_loss_mov_avg: 1.6334
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2752  valid loss: 1.2440  valid_loss_mov_avg: 1.6295
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2857  valid loss: 1.2526  valid_loss_mov_avg: 1.6257
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2753  valid loss: 1.2451  valid_loss_mov_avg: 1.6219
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3089  valid loss: 1.2447  valid_loss_mov_avg: 1.6181
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2963  valid loss: 1.2333  valid_loss_mov_avg: 1.6143

 ---------------- Grad-Step 300 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 301 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 302 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 303 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 304 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 305 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 306 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 307 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 308 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 309 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
------------------------------------------
| Data-EnvSampler-EnvE... | 0.455        |
| Data-EnvSampler-Poli... | 2.27         |
| Data-EnvTrajs-Averag... | 360          |
| Data-EnvTrajs-MaxReturn | 426          |
| Data-EnvTrajs-MinReturn | 193          |
| Data-EnvTrajs-NumTrajs  | 5            |
| Data-EnvTrajs-StdReturn | 88           |
| Data-TimeEnvSampleProc  | 0.000525     |
| Data-TimeEnvSampling    | 3.83         |
| Iteration               | 30           |
| ItrTime                 | 101          |
| LossAfter               | -0.01400899  |
| LossBefore              | 4.172325e-09 |
| Model-TimeModelFit      | 37.2         |
| ModelSampler-n_times... | 1240000      |
| Policy-AverageAbsPol... | 0.843961     |
| Policy-AverageDiscou... | 155          |
| Policy-AveragePolicyStd | 0.42732805   |
| Policy-AverageReturn    | 411          |
| Policy-MaxReturn        | 459          |
| Policy-MinReturn        | 353          |
| Policy-NumTrajs         | 20           |
| Policy-StdReturn        | 33           |
| Policy-TimeAlgoOpt      | 0.938        |
| Policy-TimeSampleProc   | 0.138        |
| Policy-TimeSampling     | 58.8         |
| Policy-TimeStep         | 59.9         |
| Time                    | 3.06e+03     |
| n_timesteps             | 31000        |
------------------------------------------

 ---------------- Iteration 31 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4536  valid loss: 1.2422  valid_loss_mov_avg: 1.8571
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4147  valid loss: 1.2506  valid_loss_mov_avg: 1.8510
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4151  valid loss: 1.2415  valid_loss_mov_avg: 1.8449
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4061  valid loss: 1.2366  valid_loss_mov_avg: 1.8388
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.3884  valid loss: 1.2246  valid_loss_mov_avg: 1.8327
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3823  valid loss: 1.2361  valid_loss_mov_avg: 1.8267
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3733  valid loss: 1.2289  valid_loss_mov_avg: 1.8207
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3719  valid loss: 1.2335  valid_loss_mov_avg: 1.8149
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3767  valid loss: 1.2220  valid_loss_mov_avg: 1.8089
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3533  valid loss: 1.2276  valid_loss_mov_avg: 1.8031
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3645  valid loss: 1.2267  valid_loss_mov_avg: 1.7974
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3437  valid loss: 1.2432  valid_loss_mov_avg: 1.7918
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3592  valid loss: 1.2330  valid_loss_mov_avg: 1.7862
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3614  valid loss: 1.2319  valid_loss_mov_avg: 1.7807
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3370  valid loss: 1.2323  valid_loss_mov_avg: 1.7752
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3388  valid loss: 1.2323  valid_loss_mov_avg: 1.7698
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3419  valid loss: 1.2266  valid_loss_mov_avg: 1.7644
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3224  valid loss: 1.2340  valid_loss_mov_avg: 1.7590
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3360  valid loss: 1.2293  valid_loss_mov_avg: 1.7538
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3321  valid loss: 1.2276  valid_loss_mov_avg: 1.7485
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3118  valid loss: 1.2329  valid_loss_mov_avg: 1.7433
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3229  valid loss: 1.2379  valid_loss_mov_avg: 1.7383
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3336  valid loss: 1.2270  valid_loss_mov_avg: 1.7332
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3091  valid loss: 1.2486  valid_loss_mov_avg: 1.7283
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3305  valid loss: 1.2325  valid_loss_mov_avg: 1.7234
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3100  valid loss: 1.2360  valid_loss_mov_avg: 1.7185
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3236  valid loss: 1.2323  valid_loss_mov_avg: 1.7136
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3158  valid loss: 1.2276  valid_loss_mov_avg: 1.7088
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.2978  valid loss: 1.2349  valid_loss_mov_avg: 1.7040
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3076  valid loss: 1.2269  valid_loss_mov_avg: 1.6993
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3013  valid loss: 1.2314  valid_loss_mov_avg: 1.6946
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.2992  valid loss: 1.2289  valid_loss_mov_avg: 1.6899
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.2861  valid loss: 1.2341  valid_loss_mov_avg: 1.6854
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.2943  valid loss: 1.2433  valid_loss_mov_avg: 1.6809
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.2995  valid loss: 1.2337  valid_loss_mov_avg: 1.6765
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3020  valid loss: 1.2345  valid_loss_mov_avg: 1.6721
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.2898  valid loss: 1.2365  valid_loss_mov_avg: 1.6677
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3104  valid loss: 1.2349  valid_loss_mov_avg: 1.6634
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.2911  valid loss: 1.2356  valid_loss_mov_avg: 1.6591
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.2839  valid loss: 1.2375  valid_loss_mov_avg: 1.6549
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.2845  valid loss: 1.2345  valid_loss_mov_avg: 1.6507
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.2824  valid loss: 1.2390  valid_loss_mov_avg: 1.6466
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2858  valid loss: 1.2349  valid_loss_mov_avg: 1.6424
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2858  valid loss: 1.2357  valid_loss_mov_avg: 1.6384
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2758  valid loss: 1.2399  valid_loss_mov_avg: 1.6344
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2763  valid loss: 1.2358  valid_loss_mov_avg: 1.6304
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2808  valid loss: 1.2385  valid_loss_mov_avg: 1.6265
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2726  valid loss: 1.2445  valid_loss_mov_avg: 1.6227
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2793  valid loss: 1.2374  valid_loss_mov_avg: 1.6188
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2858  valid loss: 1.2465  valid_loss_mov_avg: 1.6151

 ---------------- Grad-Step 310 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 311 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 312 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 313 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 314 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 315 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 316 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 317 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 318 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 319 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.497          |
| Data-EnvSampler-Poli... | 2.44           |
| Data-EnvTrajs-Averag... | 322            |
| Data-EnvTrajs-MaxReturn | 427            |
| Data-EnvTrajs-MinReturn | -41.3          |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 182            |
| Data-TimeEnvSampleProc  | 0.000665       |
| Data-TimeEnvSampling    | 4.15           |
| Iteration               | 31             |
| ItrTime                 | 105            |
| LossAfter               | -0.012835604   |
| LossBefore              | -2.6226044e-09 |
| Model-TimeModelFit      | 41.2           |
| ModelSampler-n_times... | 1280000        |
| Policy-AverageAbsPol... | 0.8465908      |
| Policy-AverageDiscou... | 107            |
| Policy-AveragePolicyStd | 0.41196334     |
| Policy-AverageReturn    | 303            |
| Policy-MaxReturn        | 375            |
| Policy-MinReturn        | 257            |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 27.5           |
| Policy-TimeAlgoOpt      | 0.982          |
| Policy-TimeSampleProc   | 0.159          |
| Policy-TimeSampling     | 58.7           |
| Policy-TimeStep         | 59.8           |
| Time                    | 3.16e+03       |
| n_timesteps             | 32000          |
--------------------------------------------

 ---------------- Iteration 32 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4822  valid loss: 1.2499  valid_loss_mov_avg: 1.8686
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4391  valid loss: 1.2395  valid_loss_mov_avg: 1.8623
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4195  valid loss: 1.2360  valid_loss_mov_avg: 1.8560
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4032  valid loss: 1.2411  valid_loss_mov_avg: 1.8499
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.3978  valid loss: 1.2395  valid_loss_mov_avg: 1.8438
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3774  valid loss: 1.2329  valid_loss_mov_avg: 1.8377
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3886  valid loss: 1.2390  valid_loss_mov_avg: 1.8317
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3900  valid loss: 1.2356  valid_loss_mov_avg: 1.8257
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3709  valid loss: 1.2421  valid_loss_mov_avg: 1.8199
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3741  valid loss: 1.2246  valid_loss_mov_avg: 1.8139
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3594  valid loss: 1.2413  valid_loss_mov_avg: 1.8082
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3650  valid loss: 1.2314  valid_loss_mov_avg: 1.8024
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3571  valid loss: 1.2315  valid_loss_mov_avg: 1.7967
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3536  valid loss: 1.2305  valid_loss_mov_avg: 1.7911
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3533  valid loss: 1.2311  valid_loss_mov_avg: 1.7855
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3477  valid loss: 1.2247  valid_loss_mov_avg: 1.7799
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3475  valid loss: 1.2271  valid_loss_mov_avg: 1.7743
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3316  valid loss: 1.2293  valid_loss_mov_avg: 1.7689
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3417  valid loss: 1.2332  valid_loss_mov_avg: 1.7635
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3410  valid loss: 1.2287  valid_loss_mov_avg: 1.7582
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3276  valid loss: 1.2299  valid_loss_mov_avg: 1.7529
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3264  valid loss: 1.2319  valid_loss_mov_avg: 1.7477
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3355  valid loss: 1.2257  valid_loss_mov_avg: 1.7425
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3142  valid loss: 1.2331  valid_loss_mov_avg: 1.7374
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3287  valid loss: 1.2281  valid_loss_mov_avg: 1.7323
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3254  valid loss: 1.2280  valid_loss_mov_avg: 1.7272
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3170  valid loss: 1.2337  valid_loss_mov_avg: 1.7223
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3092  valid loss: 1.2344  valid_loss_mov_avg: 1.7174
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3127  valid loss: 1.2338  valid_loss_mov_avg: 1.7126
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.2928  valid loss: 1.2351  valid_loss_mov_avg: 1.7078
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3158  valid loss: 1.2434  valid_loss_mov_avg: 1.7032
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3180  valid loss: 1.2506  valid_loss_mov_avg: 1.6986
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3037  valid loss: 1.2466  valid_loss_mov_avg: 1.6941
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3146  valid loss: 1.2408  valid_loss_mov_avg: 1.6896
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.2914  valid loss: 1.2411  valid_loss_mov_avg: 1.6851
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3010  valid loss: 1.2357  valid_loss_mov_avg: 1.6806
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.2878  valid loss: 1.2454  valid_loss_mov_avg: 1.6763
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3133  valid loss: 1.2429  valid_loss_mov_avg: 1.6719
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.2959  valid loss: 1.2389  valid_loss_mov_avg: 1.6676
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.2839  valid loss: 1.2468  valid_loss_mov_avg: 1.6634
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3102  valid loss: 1.2386  valid_loss_mov_avg: 1.6591
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.2838  valid loss: 1.2438  valid_loss_mov_avg: 1.6550
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3008  valid loss: 1.2379  valid_loss_mov_avg: 1.6508
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2897  valid loss: 1.2453  valid_loss_mov_avg: 1.6468
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2830  valid loss: 1.2411  valid_loss_mov_avg: 1.6427
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2872  valid loss: 1.2394  valid_loss_mov_avg: 1.6387
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2787  valid loss: 1.2412  valid_loss_mov_avg: 1.6347
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2916  valid loss: 1.2413  valid_loss_mov_avg: 1.6308
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2795  valid loss: 1.2367  valid_loss_mov_avg: 1.6268
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2915  valid loss: 1.2410  valid_loss_mov_avg: 1.6230

 ---------------- Grad-Step 320 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 321 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 322 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 323 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 324 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 325 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 326 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 327 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 328 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 329 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
------------------------------------------
| Data-EnvSampler-EnvE... | 0.376        |
| Data-EnvSampler-Poli... | 1.79         |
| Data-EnvTrajs-Averag... | 368          |
| Data-EnvTrajs-MaxReturn | 493          |
| Data-EnvTrajs-MinReturn | 14.6         |
| Data-EnvTrajs-NumTrajs  | 5            |
| Data-EnvTrajs-StdReturn | 178          |
| Data-TimeEnvSampleProc  | 0.000563     |
| Data-TimeEnvSampling    | 3.16         |
| Iteration               | 32           |
| ItrTime                 | 99.3         |
| LossAfter               | -0.016494744 |
| LossBefore              | 7.152557e-09 |
| Model-TimeModelFit      | 36.3         |
| ModelSampler-n_times... | 1320000      |
| Policy-AverageAbsPol... | 0.90564007   |
| Policy-AverageDiscou... | 169          |
| Policy-AveragePolicyStd | 0.3968021    |
| Policy-AverageReturn    | 472          |
| Policy-MaxReturn        | 541          |
| Policy-MinReturn        | 415          |
| Policy-NumTrajs         | 20           |
| Policy-StdReturn        | 36.4         |
| Policy-TimeAlgoOpt      | 1.01         |
| Policy-TimeSampleProc   | 0.141        |
| Policy-TimeSampling     | 58.7         |
| Policy-TimeStep         | 59.9         |
| Time                    | 3.26e+03     |
| n_timesteps             | 33000        |
------------------------------------------

 ---------------- Iteration 33 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4336  valid loss: 1.2561  valid_loss_mov_avg: 1.8779
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4366  valid loss: 1.2535  valid_loss_mov_avg: 1.8717
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4246  valid loss: 1.2463  valid_loss_mov_avg: 1.8654
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4090  valid loss: 1.2384  valid_loss_mov_avg: 1.8592
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.3913  valid loss: 1.2383  valid_loss_mov_avg: 1.8530
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3895  valid loss: 1.2331  valid_loss_mov_avg: 1.8468
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3828  valid loss: 1.2333  valid_loss_mov_avg: 1.8406
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3743  valid loss: 1.2427  valid_loss_mov_avg: 1.8346
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3639  valid loss: 1.2406  valid_loss_mov_avg: 1.8287
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3594  valid loss: 1.2362  valid_loss_mov_avg: 1.8228
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3691  valid loss: 1.2396  valid_loss_mov_avg: 1.8169
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3380  valid loss: 1.2401  valid_loss_mov_avg: 1.8112
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3702  valid loss: 1.2346  valid_loss_mov_avg: 1.8054
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3536  valid loss: 1.2488  valid_loss_mov_avg: 1.7998
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3502  valid loss: 1.2334  valid_loss_mov_avg: 1.7942
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3403  valid loss: 1.2391  valid_loss_mov_avg: 1.7886
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3305  valid loss: 1.2417  valid_loss_mov_avg: 1.7832
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3455  valid loss: 1.2402  valid_loss_mov_avg: 1.7777
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3365  valid loss: 1.2429  valid_loss_mov_avg: 1.7724
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3313  valid loss: 1.2435  valid_loss_mov_avg: 1.7671
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3313  valid loss: 1.2446  valid_loss_mov_avg: 1.7619
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3180  valid loss: 1.2383  valid_loss_mov_avg: 1.7566
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3345  valid loss: 1.2408  valid_loss_mov_avg: 1.7515
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3156  valid loss: 1.2367  valid_loss_mov_avg: 1.7463
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3043  valid loss: 1.2410  valid_loss_mov_avg: 1.7413
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3267  valid loss: 1.2384  valid_loss_mov_avg: 1.7362
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3130  valid loss: 1.2400  valid_loss_mov_avg: 1.7313
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3254  valid loss: 1.2363  valid_loss_mov_avg: 1.7263
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3123  valid loss: 1.2415  valid_loss_mov_avg: 1.7215
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.2981  valid loss: 1.2489  valid_loss_mov_avg: 1.7168
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3194  valid loss: 1.2486  valid_loss_mov_avg: 1.7121
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3205  valid loss: 1.2415  valid_loss_mov_avg: 1.7074
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3098  valid loss: 1.2427  valid_loss_mov_avg: 1.7027
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3091  valid loss: 1.2497  valid_loss_mov_avg: 1.6982
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3184  valid loss: 1.2465  valid_loss_mov_avg: 1.6937
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3031  valid loss: 1.2419  valid_loss_mov_avg: 1.6892
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.2986  valid loss: 1.2492  valid_loss_mov_avg: 1.6848
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.2880  valid loss: 1.2476  valid_loss_mov_avg: 1.6804
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3029  valid loss: 1.2481  valid_loss_mov_avg: 1.6761
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.2951  valid loss: 1.2477  valid_loss_mov_avg: 1.6718
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.2937  valid loss: 1.2500  valid_loss_mov_avg: 1.6676
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.2812  valid loss: 1.2542  valid_loss_mov_avg: 1.6634
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3038  valid loss: 1.2463  valid_loss_mov_avg: 1.6593
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2841  valid loss: 1.2545  valid_loss_mov_avg: 1.6552
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3167  valid loss: 1.2450  valid_loss_mov_avg: 1.6511
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2887  valid loss: 1.2455  valid_loss_mov_avg: 1.6471
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2937  valid loss: 1.2474  valid_loss_mov_avg: 1.6431
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2741  valid loss: 1.2494  valid_loss_mov_avg: 1.6391
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2938  valid loss: 1.2514  valid_loss_mov_avg: 1.6352
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2858  valid loss: 1.2520  valid_loss_mov_avg: 1.6314

 ---------------- Grad-Step 330 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 331 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 332 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 333 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 334 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 335 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 336 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 337 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 338 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 339 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.41          |
| Data-EnvSampler-Poli... | 2.02          |
| Data-EnvTrajs-Averag... | 357           |
| Data-EnvTrajs-MaxReturn | 521           |
| Data-EnvTrajs-MinReturn | 234           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 109           |
| Data-TimeEnvSampleProc  | 0.000554      |
| Data-TimeEnvSampling    | 3.43          |
| Iteration               | 33            |
| ItrTime                 | 102           |
| LossAfter               | -0.016437694  |
| LossBefore              | 6.3180923e-09 |
| Model-TimeModelFit      | 36.5          |
| ModelSampler-n_times... | 1360000       |
| Policy-AverageAbsPol... | 0.911841      |
| Policy-AverageDiscou... | 203           |
| Policy-AveragePolicyStd | 0.38248047    |
| Policy-AverageReturn    | 559           |
| Policy-MaxReturn        | 625           |
| Policy-MinReturn        | 489           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 35.8          |
| Policy-TimeAlgoOpt      | 1.1           |
| Policy-TimeSampleProc   | 0.174         |
| Policy-TimeSampling     | 60.4          |
| Policy-TimeStep         | 61.7          |
| Time                    | 3.36e+03      |
| n_timesteps             | 34000         |
-------------------------------------------

 ---------------- Iteration 34 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4640  valid loss: 1.2627  valid_loss_mov_avg: 1.8878
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4401  valid loss: 1.2554  valid_loss_mov_avg: 1.8814
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4232  valid loss: 1.2681  valid_loss_mov_avg: 1.8753
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4160  valid loss: 1.2569  valid_loss_mov_avg: 1.8691
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4113  valid loss: 1.2548  valid_loss_mov_avg: 1.8630
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4050  valid loss: 1.2645  valid_loss_mov_avg: 1.8570
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3916  valid loss: 1.2647  valid_loss_mov_avg: 1.8511
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3697  valid loss: 1.2661  valid_loss_mov_avg: 1.8452
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3749  valid loss: 1.2561  valid_loss_mov_avg: 1.8393
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3703  valid loss: 1.2595  valid_loss_mov_avg: 1.8335
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3583  valid loss: 1.2575  valid_loss_mov_avg: 1.8278
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3600  valid loss: 1.2570  valid_loss_mov_avg: 1.8221
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3559  valid loss: 1.2601  valid_loss_mov_avg: 1.8164
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3570  valid loss: 1.2685  valid_loss_mov_avg: 1.8110
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3480  valid loss: 1.2647  valid_loss_mov_avg: 1.8055
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3539  valid loss: 1.2623  valid_loss_mov_avg: 1.8001
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3495  valid loss: 1.2665  valid_loss_mov_avg: 1.7947
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3469  valid loss: 1.2604  valid_loss_mov_avg: 1.7894
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3376  valid loss: 1.2543  valid_loss_mov_avg: 1.7840
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3290  valid loss: 1.2641  valid_loss_mov_avg: 1.7788
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3495  valid loss: 1.2630  valid_loss_mov_avg: 1.7737
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3242  valid loss: 1.2593  valid_loss_mov_avg: 1.7685
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3184  valid loss: 1.2626  valid_loss_mov_avg: 1.7635
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3309  valid loss: 1.2568  valid_loss_mov_avg: 1.7584
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3277  valid loss: 1.2673  valid_loss_mov_avg: 1.7535
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3178  valid loss: 1.2628  valid_loss_mov_avg: 1.7486
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3124  valid loss: 1.2622  valid_loss_mov_avg: 1.7437
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3123  valid loss: 1.2699  valid_loss_mov_avg: 1.7390
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3059  valid loss: 1.2672  valid_loss_mov_avg: 1.7343
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3171  valid loss: 1.2708  valid_loss_mov_avg: 1.7296
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.2990  valid loss: 1.2690  valid_loss_mov_avg: 1.7250
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3116  valid loss: 1.2651  valid_loss_mov_avg: 1.7204
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3079  valid loss: 1.2675  valid_loss_mov_avg: 1.7159
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.2992  valid loss: 1.2643  valid_loss_mov_avg: 1.7114
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3082  valid loss: 1.2671  valid_loss_mov_avg: 1.7069
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.2910  valid loss: 1.2798  valid_loss_mov_avg: 1.7027
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3009  valid loss: 1.2745  valid_loss_mov_avg: 1.6984
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3046  valid loss: 1.2764  valid_loss_mov_avg: 1.6942
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.2838  valid loss: 1.2701  valid_loss_mov_avg: 1.6899
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3059  valid loss: 1.2785  valid_loss_mov_avg: 1.6858
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.2869  valid loss: 1.2700  valid_loss_mov_avg: 1.6817
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.2924  valid loss: 1.2750  valid_loss_mov_avg: 1.6776
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2870  valid loss: 1.2727  valid_loss_mov_avg: 1.6735
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2821  valid loss: 1.2698  valid_loss_mov_avg: 1.6695
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2736  valid loss: 1.2789  valid_loss_mov_avg: 1.6656
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2894  valid loss: 1.2746  valid_loss_mov_avg: 1.6617
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2828  valid loss: 1.2762  valid_loss_mov_avg: 1.6578
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2778  valid loss: 1.2726  valid_loss_mov_avg: 1.6540
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2788  valid loss: 1.2793  valid_loss_mov_avg: 1.6502
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2722  valid loss: 1.2738  valid_loss_mov_avg: 1.6465

 ---------------- Grad-Step 340 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 341 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 342 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 343 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 344 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 345 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 346 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 347 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 348 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 349 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
------------------------------------------
| Data-EnvSampler-EnvE... | 0.432        |
| Data-EnvSampler-Poli... | 2.07         |
| Data-EnvTrajs-Averag... | 493          |
| Data-EnvTrajs-MaxReturn | 556          |
| Data-EnvTrajs-MinReturn | 431          |
| Data-EnvTrajs-NumTrajs  | 5            |
| Data-EnvTrajs-StdReturn | 39.9         |
| Data-TimeEnvSampleProc  | 0.000696     |
| Data-TimeEnvSampling    | 3.5          |
| Iteration               | 34           |
| ItrTime                 | 104          |
| LossAfter               | -0.017436296 |
| LossBefore              | 9.059906e-09 |
| Model-TimeModelFit      | 39.2         |
| ModelSampler-n_times... | 1400000      |
| Policy-AverageAbsPol... | 0.91840005   |
| Policy-AverageDiscou... | 201          |
| Policy-AveragePolicyStd | 0.3668569    |
| Policy-AverageReturn    | 567          |
| Policy-MaxReturn        | 631          |
| Policy-MinReturn        | 492          |
| Policy-NumTrajs         | 20           |
| Policy-StdReturn        | 29.5         |
| Policy-TimeAlgoOpt      | 1.11         |
| Policy-TimeSampleProc   | 0.186        |
| Policy-TimeSampling     | 60.1         |
| Policy-TimeStep         | 61.4         |
| Time                    | 3.47e+03     |
| n_timesteps             | 35000        |
------------------------------------------

 ---------------- Iteration 35 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4697  valid loss: 1.2766  valid_loss_mov_avg: 1.9085
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4437  valid loss: 1.2821  valid_loss_mov_avg: 1.9022
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4278  valid loss: 1.2880  valid_loss_mov_avg: 1.8961
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4090  valid loss: 1.2843  valid_loss_mov_avg: 1.8900
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.3993  valid loss: 1.2901  valid_loss_mov_avg: 1.8840
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3894  valid loss: 1.2803  valid_loss_mov_avg: 1.8779
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3895  valid loss: 1.2867  valid_loss_mov_avg: 1.8720
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3786  valid loss: 1.3040  valid_loss_mov_avg: 1.8663
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3804  valid loss: 1.2833  valid_loss_mov_avg: 1.8605
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3687  valid loss: 1.2850  valid_loss_mov_avg: 1.8548
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3588  valid loss: 1.2818  valid_loss_mov_avg: 1.8490
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3521  valid loss: 1.2762  valid_loss_mov_avg: 1.8433
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3622  valid loss: 1.2912  valid_loss_mov_avg: 1.8378
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3539  valid loss: 1.2941  valid_loss_mov_avg: 1.8323
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3432  valid loss: 1.2907  valid_loss_mov_avg: 1.8269
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3405  valid loss: 1.2920  valid_loss_mov_avg: 1.8216
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3356  valid loss: 1.2926  valid_loss_mov_avg: 1.8163
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3378  valid loss: 1.2958  valid_loss_mov_avg: 1.8111
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3384  valid loss: 1.2876  valid_loss_mov_avg: 1.8058
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3339  valid loss: 1.2922  valid_loss_mov_avg: 1.8007
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3319  valid loss: 1.2868  valid_loss_mov_avg: 1.7956
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3193  valid loss: 1.2889  valid_loss_mov_avg: 1.7905
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3296  valid loss: 1.2966  valid_loss_mov_avg: 1.7856
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3100  valid loss: 1.2887  valid_loss_mov_avg: 1.7806
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3063  valid loss: 1.2944  valid_loss_mov_avg: 1.7757
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3264  valid loss: 1.2950  valid_loss_mov_avg: 1.7709
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3173  valid loss: 1.2893  valid_loss_mov_avg: 1.7661
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3068  valid loss: 1.2983  valid_loss_mov_avg: 1.7614
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3099  valid loss: 1.2976  valid_loss_mov_avg: 1.7568
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3098  valid loss: 1.2943  valid_loss_mov_avg: 1.7522
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3115  valid loss: 1.2941  valid_loss_mov_avg: 1.7476
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3089  valid loss: 1.2989  valid_loss_mov_avg: 1.7431
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.2958  valid loss: 1.2930  valid_loss_mov_avg: 1.7386
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3052  valid loss: 1.2918  valid_loss_mov_avg: 1.7341
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.2965  valid loss: 1.3004  valid_loss_mov_avg: 1.7298
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.2897  valid loss: 1.2968  valid_loss_mov_avg: 1.7255
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.2864  valid loss: 1.2988  valid_loss_mov_avg: 1.7212
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.2877  valid loss: 1.3057  valid_loss_mov_avg: 1.7170
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3037  valid loss: 1.2973  valid_loss_mov_avg: 1.7128
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.2928  valid loss: 1.2981  valid_loss_mov_avg: 1.7087
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.2783  valid loss: 1.2978  valid_loss_mov_avg: 1.7046
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3015  valid loss: 1.3026  valid_loss_mov_avg: 1.7006
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2949  valid loss: 1.3064  valid_loss_mov_avg: 1.6966
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2816  valid loss: 1.3070  valid_loss_mov_avg: 1.6927
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2922  valid loss: 1.3026  valid_loss_mov_avg: 1.6888
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2722  valid loss: 1.3056  valid_loss_mov_avg: 1.6850
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2802  valid loss: 1.3011  valid_loss_mov_avg: 1.6812
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2775  valid loss: 1.3105  valid_loss_mov_avg: 1.6774
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2907  valid loss: 1.3145  valid_loss_mov_avg: 1.6738
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2824  valid loss: 1.3092  valid_loss_mov_avg: 1.6702

 ---------------- Grad-Step 350 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 351 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 352 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 353 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 354 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 355 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 356 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 357 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 358 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 359 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.372         |
| Data-EnvSampler-Poli... | 1.78          |
| Data-EnvTrajs-Averag... | 504           |
| Data-EnvTrajs-MaxReturn | 569           |
| Data-EnvTrajs-MinReturn | 437           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 49.6          |
| Data-TimeEnvSampleProc  | 0.000778      |
| Data-TimeEnvSampling    | 3.15          |
| Iteration               | 35            |
| ItrTime                 | 107           |
| LossAfter               | -0.017422916  |
| LossBefore              | 4.2915342e-09 |
| Model-TimeModelFit      | 37.5          |
| ModelSampler-n_times... | 1440000       |
| Policy-AverageAbsPol... | 0.9104968     |
| Policy-AverageDiscou... | 193           |
| Policy-AveragePolicyStd | 0.35453796    |
| Policy-AverageReturn    | 544           |
| Policy-MaxReturn        | 601           |
| Policy-MinReturn        | 473           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 33.8          |
| Policy-TimeAlgoOpt      | 1.13          |
| Policy-TimeSampleProc   | 0.172         |
| Policy-TimeSampling     | 65.2          |
| Policy-TimeStep         | 66.5          |
| Time                    | 3.57e+03      |
| n_timesteps             | 36000         |
-------------------------------------------

 ---------------- Iteration 36 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4549  valid loss: 1.2916  valid_loss_mov_avg: 1.9309
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4248  valid loss: 1.3026  valid_loss_mov_avg: 1.9247
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4289  valid loss: 1.2863  valid_loss_mov_avg: 1.9183
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.3948  valid loss: 1.2840  valid_loss_mov_avg: 1.9119
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4020  valid loss: 1.2799  valid_loss_mov_avg: 1.9056
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3916  valid loss: 1.2785  valid_loss_mov_avg: 1.8993
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3923  valid loss: 1.2773  valid_loss_mov_avg: 1.8931
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3756  valid loss: 1.2840  valid_loss_mov_avg: 1.8870
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3782  valid loss: 1.2790  valid_loss_mov_avg: 1.8809
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3707  valid loss: 1.2833  valid_loss_mov_avg: 1.8750
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3607  valid loss: 1.2784  valid_loss_mov_avg: 1.8690
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3627  valid loss: 1.2786  valid_loss_mov_avg: 1.8631
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3453  valid loss: 1.2835  valid_loss_mov_avg: 1.8573
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3585  valid loss: 1.2772  valid_loss_mov_avg: 1.8515
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3498  valid loss: 1.2757  valid_loss_mov_avg: 1.8457
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3480  valid loss: 1.2805  valid_loss_mov_avg: 1.8401
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3221  valid loss: 1.2795  valid_loss_mov_avg: 1.8345
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3474  valid loss: 1.2768  valid_loss_mov_avg: 1.8289
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3401  valid loss: 1.2757  valid_loss_mov_avg: 1.8234
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3279  valid loss: 1.2829  valid_loss_mov_avg: 1.8180
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3290  valid loss: 1.2825  valid_loss_mov_avg: 1.8126
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3327  valid loss: 1.2752  valid_loss_mov_avg: 1.8072
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3228  valid loss: 1.2901  valid_loss_mov_avg: 1.8021
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3167  valid loss: 1.2890  valid_loss_mov_avg: 1.7969
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3350  valid loss: 1.2822  valid_loss_mov_avg: 1.7918
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3092  valid loss: 1.2879  valid_loss_mov_avg: 1.7868
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3287  valid loss: 1.2802  valid_loss_mov_avg: 1.7817
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3171  valid loss: 1.2843  valid_loss_mov_avg: 1.7767
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3071  valid loss: 1.2833  valid_loss_mov_avg: 1.7718
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3231  valid loss: 1.2766  valid_loss_mov_avg: 1.7668
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3138  valid loss: 1.2887  valid_loss_mov_avg: 1.7620
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3079  valid loss: 1.2803  valid_loss_mov_avg: 1.7572
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3052  valid loss: 1.2917  valid_loss_mov_avg: 1.7526
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3009  valid loss: 1.2874  valid_loss_mov_avg: 1.7479
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.2974  valid loss: 1.2916  valid_loss_mov_avg: 1.7434
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.2990  valid loss: 1.2896  valid_loss_mov_avg: 1.7388
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3042  valid loss: 1.2904  valid_loss_mov_avg: 1.7343
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3009  valid loss: 1.2813  valid_loss_mov_avg: 1.7298
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.2895  valid loss: 1.2901  valid_loss_mov_avg: 1.7254
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.2965  valid loss: 1.2907  valid_loss_mov_avg: 1.7211
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.2987  valid loss: 1.2987  valid_loss_mov_avg: 1.7168
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3002  valid loss: 1.2842  valid_loss_mov_avg: 1.7125
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2958  valid loss: 1.2904  valid_loss_mov_avg: 1.7083
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2980  valid loss: 1.2867  valid_loss_mov_avg: 1.7041
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2777  valid loss: 1.2909  valid_loss_mov_avg: 1.6999
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2805  valid loss: 1.2926  valid_loss_mov_avg: 1.6959
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2906  valid loss: 1.2870  valid_loss_mov_avg: 1.6918
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2975  valid loss: 1.2887  valid_loss_mov_avg: 1.6877
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2786  valid loss: 1.2936  valid_loss_mov_avg: 1.6838
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2818  valid loss: 1.2895  valid_loss_mov_avg: 1.6799

 ---------------- Grad-Step 360 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 361 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 362 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 363 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 364 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 365 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 366 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 367 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 368 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 369 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.398         |
| Data-EnvSampler-Poli... | 1.92          |
| Data-EnvTrajs-Averag... | 510           |
| Data-EnvTrajs-MaxReturn | 611           |
| Data-EnvTrajs-MinReturn | 406           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 71.1          |
| Data-TimeEnvSampleProc  | 0.0006        |
| Data-TimeEnvSampling    | 3.33          |
| Iteration               | 36            |
| ItrTime                 | 104           |
| LossAfter               | -0.01337905   |
| LossBefore              | -4.798174e-09 |
| Model-TimeModelFit      | 40.6          |
| ModelSampler-n_times... | 1480000       |
| Policy-AverageAbsPol... | 0.9363285     |
| Policy-AverageDiscou... | 232           |
| Policy-AveragePolicyStd | 0.34351024    |
| Policy-AverageReturn    | 637           |
| Policy-MaxReturn        | 726           |
| Policy-MinReturn        | 551           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 39.8          |
| Policy-TimeAlgoOpt      | 1.03          |
| Policy-TimeSampleProc   | 0.135         |
| Policy-TimeSampling     | 58.5          |
| Policy-TimeStep         | 59.7          |
| Time                    | 3.68e+03      |
| n_timesteps             | 37000         |
-------------------------------------------

 ---------------- Iteration 37 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.5228  valid loss: 1.3059  valid_loss_mov_avg: 1.9524
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4759  valid loss: 1.2932  valid_loss_mov_avg: 1.9458
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4455  valid loss: 1.2964  valid_loss_mov_avg: 1.9393
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4470  valid loss: 1.2967  valid_loss_mov_avg: 1.9329
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4285  valid loss: 1.2976  valid_loss_mov_avg: 1.9265
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4146  valid loss: 1.3085  valid_loss_mov_avg: 1.9203
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4153  valid loss: 1.2973  valid_loss_mov_avg: 1.9141
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.4002  valid loss: 1.3067  valid_loss_mov_avg: 1.9080
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3997  valid loss: 1.2976  valid_loss_mov_avg: 1.9019
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3942  valid loss: 1.2875  valid_loss_mov_avg: 1.8958
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3817  valid loss: 1.2907  valid_loss_mov_avg: 1.8897
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3793  valid loss: 1.3062  valid_loss_mov_avg: 1.8839
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3709  valid loss: 1.2989  valid_loss_mov_avg: 1.8780
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3707  valid loss: 1.2962  valid_loss_mov_avg: 1.8722
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3668  valid loss: 1.3078  valid_loss_mov_avg: 1.8666
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3710  valid loss: 1.3147  valid_loss_mov_avg: 1.8611
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3647  valid loss: 1.2926  valid_loss_mov_avg: 1.8554
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3729  valid loss: 1.2995  valid_loss_mov_avg: 1.8498
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3484  valid loss: 1.3003  valid_loss_mov_avg: 1.8443
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3474  valid loss: 1.3012  valid_loss_mov_avg: 1.8389
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3329  valid loss: 1.3050  valid_loss_mov_avg: 1.8336
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3519  valid loss: 1.3029  valid_loss_mov_avg: 1.8282
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3507  valid loss: 1.3026  valid_loss_mov_avg: 1.8230
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3392  valid loss: 1.2975  valid_loss_mov_avg: 1.8177
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3378  valid loss: 1.3025  valid_loss_mov_avg: 1.8126
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3241  valid loss: 1.3070  valid_loss_mov_avg: 1.8075
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3471  valid loss: 1.3044  valid_loss_mov_avg: 1.8025
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3232  valid loss: 1.3062  valid_loss_mov_avg: 1.7975
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3445  valid loss: 1.2966  valid_loss_mov_avg: 1.7925
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3339  valid loss: 1.3113  valid_loss_mov_avg: 1.7877
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3182  valid loss: 1.3161  valid_loss_mov_avg: 1.7830
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3332  valid loss: 1.3133  valid_loss_mov_avg: 1.7783
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3073  valid loss: 1.3083  valid_loss_mov_avg: 1.7736
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3085  valid loss: 1.3072  valid_loss_mov_avg: 1.7689
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3152  valid loss: 1.3102  valid_loss_mov_avg: 1.7643
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3121  valid loss: 1.3081  valid_loss_mov_avg: 1.7598
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3081  valid loss: 1.3115  valid_loss_mov_avg: 1.7553
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3199  valid loss: 1.3121  valid_loss_mov_avg: 1.7509
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3123  valid loss: 1.3157  valid_loss_mov_avg: 1.7465
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3110  valid loss: 1.3102  valid_loss_mov_avg: 1.7422
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3189  valid loss: 1.3171  valid_loss_mov_avg: 1.7379
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3026  valid loss: 1.3115  valid_loss_mov_avg: 1.7336
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2917  valid loss: 1.3123  valid_loss_mov_avg: 1.7294
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3163  valid loss: 1.3180  valid_loss_mov_avg: 1.7253
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3146  valid loss: 1.3183  valid_loss_mov_avg: 1.7212
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3114  valid loss: 1.3173  valid_loss_mov_avg: 1.7172
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3034  valid loss: 1.3179  valid_loss_mov_avg: 1.7132
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2954  valid loss: 1.3251  valid_loss_mov_avg: 1.7093
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3187  valid loss: 1.3161  valid_loss_mov_avg: 1.7054
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2939  valid loss: 1.3200  valid_loss_mov_avg: 1.7015

 ---------------- Grad-Step 370 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 371 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 372 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 373 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 374 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 375 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 376 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 377 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 378 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 379 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.396         |
| Data-EnvSampler-Poli... | 1.9           |
| Data-EnvTrajs-Averag... | 214           |
| Data-EnvTrajs-MaxReturn | 523           |
| Data-EnvTrajs-MinReturn | -33.6         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 207           |
| Data-TimeEnvSampleProc  | 0.000652      |
| Data-TimeEnvSampling    | 3.28          |
| Iteration               | 37            |
| ItrTime                 | 107           |
| LossAfter               | -0.016302783  |
| LossBefore              | 4.8279762e-09 |
| Model-TimeModelFit      | 43.4          |
| ModelSampler-n_times... | 1520000       |
| Policy-AverageAbsPol... | 0.9400791     |
| Policy-AverageDiscou... | 252           |
| Policy-AveragePolicyStd | 0.3318558     |
| Policy-AverageReturn    | 702           |
| Policy-MaxReturn        | 841           |
| Policy-MinReturn        | 637           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 44.3          |
| Policy-TimeAlgoOpt      | 1.03          |
| Policy-TimeSampleProc   | 0.158         |
| Policy-TimeSampling     | 59.5          |
| Policy-TimeStep         | 60.7          |
| Time                    | 3.78e+03      |
| n_timesteps             | 38000         |
-------------------------------------------

 ---------------- Iteration 38 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4557  valid loss: 1.3116  valid_loss_mov_avg: 1.9608
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4343  valid loss: 1.3245  valid_loss_mov_avg: 1.9544
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4202  valid loss: 1.3126  valid_loss_mov_avg: 1.9480
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4125  valid loss: 1.3198  valid_loss_mov_avg: 1.9417
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.3987  valid loss: 1.3071  valid_loss_mov_avg: 1.9354
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4164  valid loss: 1.3140  valid_loss_mov_avg: 1.9292
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3862  valid loss: 1.3080  valid_loss_mov_avg: 1.9230
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3930  valid loss: 1.3164  valid_loss_mov_avg: 1.9169
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3903  valid loss: 1.3084  valid_loss_mov_avg: 1.9108
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3987  valid loss: 1.3051  valid_loss_mov_avg: 1.9048
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3848  valid loss: 1.3125  valid_loss_mov_avg: 1.8988
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3615  valid loss: 1.3072  valid_loss_mov_avg: 1.8929
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3755  valid loss: 1.3059  valid_loss_mov_avg: 1.8871
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3779  valid loss: 1.2975  valid_loss_mov_avg: 1.8812
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3587  valid loss: 1.3212  valid_loss_mov_avg: 1.8756
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3771  valid loss: 1.3042  valid_loss_mov_avg: 1.8698
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3606  valid loss: 1.3075  valid_loss_mov_avg: 1.8642
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3452  valid loss: 1.3063  valid_loss_mov_avg: 1.8586
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3496  valid loss: 1.3101  valid_loss_mov_avg: 1.8532
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3471  valid loss: 1.3095  valid_loss_mov_avg: 1.8477
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3434  valid loss: 1.3106  valid_loss_mov_avg: 1.8423
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3338  valid loss: 1.3060  valid_loss_mov_avg: 1.8370
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3433  valid loss: 1.3071  valid_loss_mov_avg: 1.8317
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3514  valid loss: 1.3049  valid_loss_mov_avg: 1.8264
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3206  valid loss: 1.3142  valid_loss_mov_avg: 1.8213
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3384  valid loss: 1.3140  valid_loss_mov_avg: 1.8162
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3328  valid loss: 1.3138  valid_loss_mov_avg: 1.8112
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3131  valid loss: 1.3094  valid_loss_mov_avg: 1.8062
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3338  valid loss: 1.3117  valid_loss_mov_avg: 1.8012
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3289  valid loss: 1.3146  valid_loss_mov_avg: 1.7964
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3239  valid loss: 1.3177  valid_loss_mov_avg: 1.7916
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3260  valid loss: 1.3096  valid_loss_mov_avg: 1.7868
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3238  valid loss: 1.3107  valid_loss_mov_avg: 1.7820
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3094  valid loss: 1.3191  valid_loss_mov_avg: 1.7774
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3152  valid loss: 1.3123  valid_loss_mov_avg: 1.7727
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3146  valid loss: 1.3159  valid_loss_mov_avg: 1.7682
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3073  valid loss: 1.3154  valid_loss_mov_avg: 1.7636
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3078  valid loss: 1.3208  valid_loss_mov_avg: 1.7592
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3162  valid loss: 1.3269  valid_loss_mov_avg: 1.7549
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3099  valid loss: 1.3132  valid_loss_mov_avg: 1.7505
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3114  valid loss: 1.3140  valid_loss_mov_avg: 1.7461
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.2936  valid loss: 1.3132  valid_loss_mov_avg: 1.7418
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3159  valid loss: 1.3197  valid_loss_mov_avg: 1.7375
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2911  valid loss: 1.3224  valid_loss_mov_avg: 1.7334
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3048  valid loss: 1.3208  valid_loss_mov_avg: 1.7293
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3049  valid loss: 1.3211  valid_loss_mov_avg: 1.7252
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2928  valid loss: 1.3268  valid_loss_mov_avg: 1.7212
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3121  valid loss: 1.3240  valid_loss_mov_avg: 1.7172
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2855  valid loss: 1.3252  valid_loss_mov_avg: 1.7133
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3091  valid loss: 1.3206  valid_loss_mov_avg: 1.7094

 ---------------- Grad-Step 380 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 381 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 382 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 383 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 384 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 385 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 386 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 387 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 388 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 389 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.444         |
| Data-EnvSampler-Poli... | 2.22          |
| Data-EnvTrajs-Averag... | 206           |
| Data-EnvTrajs-MaxReturn | 522           |
| Data-EnvTrajs-MinReturn | -87.1         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 220           |
| Data-TimeEnvSampleProc  | 0.000832      |
| Data-TimeEnvSampling    | 3.78          |
| Iteration               | 38            |
| ItrTime                 | 102           |
| LossAfter               | -0.013801451  |
| LossBefore              | 2.3841857e-09 |
| Model-TimeModelFit      | 38.5          |
| ModelSampler-n_times... | 1560000       |
| Policy-AverageAbsPol... | 0.95901096    |
| Policy-AverageDiscou... | 206           |
| Policy-AveragePolicyStd | 0.3222637     |
| Policy-AverageReturn    | 579           |
| Policy-MaxReturn        | 636           |
| Policy-MinReturn        | 510           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 35.6          |
| Policy-TimeAlgoOpt      | 1.09          |
| Policy-TimeSampleProc   | 0.152         |
| Policy-TimeSampling     | 58            |
| Policy-TimeStep         | 59.2          |
| Time                    | 3.89e+03      |
| n_timesteps             | 39000         |
-------------------------------------------

 ---------------- Iteration 39 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4655  valid loss: 1.3260  valid_loss_mov_avg: 1.9824
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4478  valid loss: 1.3085  valid_loss_mov_avg: 1.9757
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4190  valid loss: 1.3142  valid_loss_mov_avg: 1.9691
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4230  valid loss: 1.3080  valid_loss_mov_avg: 1.9625
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4016  valid loss: 1.3115  valid_loss_mov_avg: 1.9560
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4084  valid loss: 1.2995  valid_loss_mov_avg: 1.9494
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4075  valid loss: 1.3103  valid_loss_mov_avg: 1.9430
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3974  valid loss: 1.3069  valid_loss_mov_avg: 1.9367
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3926  valid loss: 1.2952  valid_loss_mov_avg: 1.9302
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3811  valid loss: 1.3029  valid_loss_mov_avg: 1.9240
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3686  valid loss: 1.3077  valid_loss_mov_avg: 1.9178
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3628  valid loss: 1.2995  valid_loss_mov_avg: 1.9116
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3822  valid loss: 1.2993  valid_loss_mov_avg: 1.9055
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3661  valid loss: 1.2942  valid_loss_mov_avg: 1.8994
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3633  valid loss: 1.3139  valid_loss_mov_avg: 1.8935
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3638  valid loss: 1.3123  valid_loss_mov_avg: 1.8877
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3570  valid loss: 1.3082  valid_loss_mov_avg: 1.8819
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3665  valid loss: 1.3102  valid_loss_mov_avg: 1.8762
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3673  valid loss: 1.3001  valid_loss_mov_avg: 1.8704
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3500  valid loss: 1.3031  valid_loss_mov_avg: 1.8648
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3485  valid loss: 1.3100  valid_loss_mov_avg: 1.8592
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3498  valid loss: 1.3013  valid_loss_mov_avg: 1.8536
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3351  valid loss: 1.3113  valid_loss_mov_avg: 1.8482
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3465  valid loss: 1.3051  valid_loss_mov_avg: 1.8428
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3314  valid loss: 1.3057  valid_loss_mov_avg: 1.8374
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3328  valid loss: 1.3082  valid_loss_mov_avg: 1.8321
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3290  valid loss: 1.3122  valid_loss_mov_avg: 1.8269
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3274  valid loss: 1.3033  valid_loss_mov_avg: 1.8217
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3231  valid loss: 1.3100  valid_loss_mov_avg: 1.8166
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3288  valid loss: 1.3134  valid_loss_mov_avg: 1.8115
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3272  valid loss: 1.3147  valid_loss_mov_avg: 1.8066
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3231  valid loss: 1.3157  valid_loss_mov_avg: 1.8017
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3207  valid loss: 1.3128  valid_loss_mov_avg: 1.7968
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3148  valid loss: 1.3173  valid_loss_mov_avg: 1.7920
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3222  valid loss: 1.3190  valid_loss_mov_avg: 1.7872
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3243  valid loss: 1.3126  valid_loss_mov_avg: 1.7825
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3184  valid loss: 1.3092  valid_loss_mov_avg: 1.7778
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3159  valid loss: 1.3167  valid_loss_mov_avg: 1.7732
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3086  valid loss: 1.3142  valid_loss_mov_avg: 1.7686
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3080  valid loss: 1.3218  valid_loss_mov_avg: 1.7641
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3149  valid loss: 1.3203  valid_loss_mov_avg: 1.7597
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3119  valid loss: 1.3175  valid_loss_mov_avg: 1.7552
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3103  valid loss: 1.3250  valid_loss_mov_avg: 1.7509
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2936  valid loss: 1.3126  valid_loss_mov_avg: 1.7466
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3181  valid loss: 1.3147  valid_loss_mov_avg: 1.7422
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3014  valid loss: 1.3185  valid_loss_mov_avg: 1.7380
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2968  valid loss: 1.3204  valid_loss_mov_avg: 1.7338
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3130  valid loss: 1.3164  valid_loss_mov_avg: 1.7297
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2907  valid loss: 1.3188  valid_loss_mov_avg: 1.7255
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3070  valid loss: 1.3143  valid_loss_mov_avg: 1.7214

 ---------------- Grad-Step 390 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 391 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 392 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 393 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 394 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 395 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 396 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 397 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 398 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 399 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
------------------------------------------
| Data-EnvSampler-EnvE... | 0.44         |
| Data-EnvSampler-Poli... | 2.14         |
| Data-EnvTrajs-Averag... | 162          |
| Data-EnvTrajs-MaxReturn | 426          |
| Data-EnvTrajs-MinReturn | -69.8        |
| Data-EnvTrajs-NumTrajs  | 5            |
| Data-EnvTrajs-StdReturn | 183          |
| Data-TimeEnvSampleProc  | 0.000718     |
| Data-TimeEnvSampling    | 3.63         |
| Iteration               | 39           |
| ItrTime                 | 111          |
| LossAfter               | -0.013116021 |
| LossBefore              | 7.510185e-09 |
| Model-TimeModelFit      | 38.2         |
| ModelSampler-n_times... | 1600000      |
| Policy-AverageAbsPol... | 0.95202345   |
| Policy-AverageDiscou... | 231          |
| Policy-AveragePolicyStd | 0.3110698    |
| Policy-AverageReturn    | 627          |
| Policy-MaxReturn        | 699          |
| Policy-MinReturn        | 593          |
| Policy-NumTrajs         | 20           |
| Policy-StdReturn        | 27.7         |
| Policy-TimeAlgoOpt      | 1.17         |
| Policy-TimeSampleProc   | 0.171        |
| Policy-TimeSampling     | 67.6         |
| Policy-TimeStep         | 69           |
| Time                    | 4e+03        |
| n_timesteps             | 40000        |
------------------------------------------

 ---------------- Iteration 40 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4787  valid loss: 1.3085  valid_loss_mov_avg: 1.9561
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4378  valid loss: 1.3162  valid_loss_mov_avg: 1.9497
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4336  valid loss: 1.3063  valid_loss_mov_avg: 1.9433
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4100  valid loss: 1.3254  valid_loss_mov_avg: 1.9371
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.3970  valid loss: 1.3095  valid_loss_mov_avg: 1.9308
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4120  valid loss: 1.3033  valid_loss_mov_avg: 1.9246
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3972  valid loss: 1.3048  valid_loss_mov_avg: 1.9184
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3782  valid loss: 1.3133  valid_loss_mov_avg: 1.9123
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3739  valid loss: 1.3126  valid_loss_mov_avg: 1.9063
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3782  valid loss: 1.3049  valid_loss_mov_avg: 1.9003
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3943  valid loss: 1.3031  valid_loss_mov_avg: 1.8943
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3797  valid loss: 1.3017  valid_loss_mov_avg: 1.8884
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3664  valid loss: 1.3166  valid_loss_mov_avg: 1.8827
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3550  valid loss: 1.3056  valid_loss_mov_avg: 1.8769
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3592  valid loss: 1.3123  valid_loss_mov_avg: 1.8713
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3474  valid loss: 1.3047  valid_loss_mov_avg: 1.8656
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3575  valid loss: 1.3009  valid_loss_mov_avg: 1.8600
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3444  valid loss: 1.3068  valid_loss_mov_avg: 1.8544
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3633  valid loss: 1.3028  valid_loss_mov_avg: 1.8489
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3566  valid loss: 1.3090  valid_loss_mov_avg: 1.8435
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3387  valid loss: 1.3172  valid_loss_mov_avg: 1.8383
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3529  valid loss: 1.3057  valid_loss_mov_avg: 1.8329
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3365  valid loss: 1.3048  valid_loss_mov_avg: 1.8276
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3366  valid loss: 1.3093  valid_loss_mov_avg: 1.8225
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3436  valid loss: 1.3105  valid_loss_mov_avg: 1.8173
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3248  valid loss: 1.3159  valid_loss_mov_avg: 1.8123
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3504  valid loss: 1.3215  valid_loss_mov_avg: 1.8074
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3473  valid loss: 1.3157  valid_loss_mov_avg: 1.8025
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3263  valid loss: 1.3048  valid_loss_mov_avg: 1.7975
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3283  valid loss: 1.3150  valid_loss_mov_avg: 1.7927
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3364  valid loss: 1.3074  valid_loss_mov_avg: 1.7878
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3210  valid loss: 1.3185  valid_loss_mov_avg: 1.7832
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3127  valid loss: 1.3131  valid_loss_mov_avg: 1.7785
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3338  valid loss: 1.3166  valid_loss_mov_avg: 1.7738
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3120  valid loss: 1.3220  valid_loss_mov_avg: 1.7693
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3347  valid loss: 1.3124  valid_loss_mov_avg: 1.7647
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3184  valid loss: 1.3194  valid_loss_mov_avg: 1.7603
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3268  valid loss: 1.3182  valid_loss_mov_avg: 1.7559
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3223  valid loss: 1.3257  valid_loss_mov_avg: 1.7516
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3014  valid loss: 1.3156  valid_loss_mov_avg: 1.7472
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3158  valid loss: 1.3181  valid_loss_mov_avg: 1.7429
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3057  valid loss: 1.3105  valid_loss_mov_avg: 1.7386
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3092  valid loss: 1.3197  valid_loss_mov_avg: 1.7344
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2968  valid loss: 1.3182  valid_loss_mov_avg: 1.7302
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3150  valid loss: 1.3258  valid_loss_mov_avg: 1.7262
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3064  valid loss: 1.3267  valid_loss_mov_avg: 1.7222
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.3007  valid loss: 1.3269  valid_loss_mov_avg: 1.7183
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3108  valid loss: 1.3112  valid_loss_mov_avg: 1.7142
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2911  valid loss: 1.3251  valid_loss_mov_avg: 1.7103
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.3060  valid loss: 1.3260  valid_loss_mov_avg: 1.7064

 ---------------- Grad-Step 400 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 401 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 402 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 403 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 404 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 405 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 406 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 407 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 408 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 409 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.47          |
| Data-EnvSampler-Poli... | 2.32          |
| Data-EnvTrajs-Averag... | 314           |
| Data-EnvTrajs-MaxReturn | 583           |
| Data-EnvTrajs-MinReturn | -62.2         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 304           |
| Data-TimeEnvSampleProc  | 0.000601      |
| Data-TimeEnvSampling    | 3.97          |
| Iteration               | 40            |
| ItrTime                 | 107           |
| LossAfter               | -0.015849704  |
| LossBefore              | -8.106232e-09 |
| Model-TimeModelFit      | 43.4          |
| ModelSampler-n_times... | 1640000       |
| Policy-AverageAbsPol... | 0.9418114     |
| Policy-AverageDiscou... | 211           |
| Policy-AveragePolicyStd | 0.30030486    |
| Policy-AverageReturn    | 583           |
| Policy-MaxReturn        | 643           |
| Policy-MinReturn        | 516           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 33.3          |
| Policy-TimeAlgoOpt      | 1.01          |
| Policy-TimeSampleProc   | 0.141         |
| Policy-TimeSampling     | 58.6          |
| Policy-TimeStep         | 59.7          |
| Time                    | 4.1e+03       |
| n_timesteps             | 41000         |
-------------------------------------------

 ---------------- Iteration 41 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4756  valid loss: 1.3407  valid_loss_mov_avg: 2.0043
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4417  valid loss: 1.3172  valid_loss_mov_avg: 1.9975
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4144  valid loss: 1.3263  valid_loss_mov_avg: 1.9908
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4031  valid loss: 1.3245  valid_loss_mov_avg: 1.9841
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.3951  valid loss: 1.3321  valid_loss_mov_avg: 1.9776
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3838  valid loss: 1.3339  valid_loss_mov_avg: 1.9711
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3949  valid loss: 1.3337  valid_loss_mov_avg: 1.9648
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3811  valid loss: 1.3206  valid_loss_mov_avg: 1.9583
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3771  valid loss: 1.3336  valid_loss_mov_avg: 1.9521
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3778  valid loss: 1.3184  valid_loss_mov_avg: 1.9457
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3650  valid loss: 1.3236  valid_loss_mov_avg: 1.9395
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3810  valid loss: 1.3307  valid_loss_mov_avg: 1.9334
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3760  valid loss: 1.3381  valid_loss_mov_avg: 1.9275
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3635  valid loss: 1.3329  valid_loss_mov_avg: 1.9215
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3564  valid loss: 1.3409  valid_loss_mov_avg: 1.9157
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3505  valid loss: 1.3284  valid_loss_mov_avg: 1.9099
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3646  valid loss: 1.3353  valid_loss_mov_avg: 1.9041
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3532  valid loss: 1.3395  valid_loss_mov_avg: 1.8985
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3477  valid loss: 1.3249  valid_loss_mov_avg: 1.8927
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3469  valid loss: 1.3288  valid_loss_mov_avg: 1.8871
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3387  valid loss: 1.3357  valid_loss_mov_avg: 1.8816
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3527  valid loss: 1.3315  valid_loss_mov_avg: 1.8761
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3528  valid loss: 1.3316  valid_loss_mov_avg: 1.8706
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3440  valid loss: 1.3369  valid_loss_mov_avg: 1.8653
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3330  valid loss: 1.3378  valid_loss_mov_avg: 1.8600
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3516  valid loss: 1.3188  valid_loss_mov_avg: 1.8546
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3282  valid loss: 1.3389  valid_loss_mov_avg: 1.8494
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3301  valid loss: 1.3438  valid_loss_mov_avg: 1.8444
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3347  valid loss: 1.3412  valid_loss_mov_avg: 1.8394
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3279  valid loss: 1.3407  valid_loss_mov_avg: 1.8344
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3283  valid loss: 1.3318  valid_loss_mov_avg: 1.8293
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3214  valid loss: 1.3345  valid_loss_mov_avg: 1.8244
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3170  valid loss: 1.3447  valid_loss_mov_avg: 1.8196
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3117  valid loss: 1.3334  valid_loss_mov_avg: 1.8147
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3127  valid loss: 1.3448  valid_loss_mov_avg: 1.8100
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3203  valid loss: 1.3452  valid_loss_mov_avg: 1.8054
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3311  valid loss: 1.3373  valid_loss_mov_avg: 1.8007
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.2994  valid loss: 1.3537  valid_loss_mov_avg: 1.7962
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3206  valid loss: 1.3428  valid_loss_mov_avg: 1.7917
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3005  valid loss: 1.3474  valid_loss_mov_avg: 1.7873
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3117  valid loss: 1.3489  valid_loss_mov_avg: 1.7829
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3092  valid loss: 1.3419  valid_loss_mov_avg: 1.7785
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.3020  valid loss: 1.3469  valid_loss_mov_avg: 1.7742
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3002  valid loss: 1.3449  valid_loss_mov_avg: 1.7699
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2977  valid loss: 1.3586  valid_loss_mov_avg: 1.7658
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3083  valid loss: 1.3497  valid_loss_mov_avg: 1.7616
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2972  valid loss: 1.3478  valid_loss_mov_avg: 1.7575
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3034  valid loss: 1.3525  valid_loss_mov_avg: 1.7534
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.3039  valid loss: 1.3503  valid_loss_mov_avg: 1.7494
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2921  valid loss: 1.3499  valid_loss_mov_avg: 1.7454

 ---------------- Grad-Step 410 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 411 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 412 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 413 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 414 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 415 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 416 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 417 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 418 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 419 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
------------------------------------------
| Data-EnvSampler-EnvE... | 0.371        |
| Data-EnvSampler-Poli... | 1.76         |
| Data-EnvTrajs-Averag... | 245          |
| Data-EnvTrajs-MaxReturn | 605          |
| Data-EnvTrajs-MinReturn | -34          |
| Data-EnvTrajs-NumTrajs  | 5            |
| Data-EnvTrajs-StdReturn | 273          |
| Data-TimeEnvSampleProc  | 0.000494     |
| Data-TimeEnvSampling    | 3.11         |
| Iteration               | 41           |
| ItrTime                 | 102          |
| LossAfter               | -0.012173568 |
| LossBefore              | 2.503395e-09 |
| Model-TimeModelFit      | 40.2         |
| ModelSampler-n_times... | 1680000      |
| Policy-AverageAbsPol... | 0.92259747   |
| Policy-AverageDiscou... | 241          |
| Policy-AveragePolicyStd | 0.29093623   |
| Policy-AverageReturn    | 655          |
| Policy-MaxReturn        | 722          |
| Policy-MinReturn        | 578          |
| Policy-NumTrajs         | 20           |
| Policy-StdReturn        | 33.6         |
| Policy-TimeAlgoOpt      | 0.909        |
| Policy-TimeSampleProc   | 0.148        |
| Policy-TimeSampling     | 57.4         |
| Policy-TimeStep         | 58.5         |
| Time                    | 4.21e+03     |
| n_timesteps             | 42000        |
------------------------------------------

 ---------------- Iteration 42 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4511  valid loss: 1.3493  valid_loss_mov_avg: 2.0172
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4370  valid loss: 1.3419  valid_loss_mov_avg: 2.0105
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4215  valid loss: 1.3419  valid_loss_mov_avg: 2.0038
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.4128  valid loss: 1.3459  valid_loss_mov_avg: 1.9972
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.4016  valid loss: 1.3463  valid_loss_mov_avg: 1.9907
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.4073  valid loss: 1.3300  valid_loss_mov_avg: 1.9841
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.4013  valid loss: 1.3187  valid_loss_mov_avg: 1.9775
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3772  valid loss: 1.3297  valid_loss_mov_avg: 1.9710
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3757  valid loss: 1.3428  valid_loss_mov_avg: 1.9647
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3739  valid loss: 1.3350  valid_loss_mov_avg: 1.9584
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3817  valid loss: 1.3356  valid_loss_mov_avg: 1.9522
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3610  valid loss: 1.3442  valid_loss_mov_avg: 1.9461
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3688  valid loss: 1.3393  valid_loss_mov_avg: 1.9400
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3652  valid loss: 1.3369  valid_loss_mov_avg: 1.9340
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3649  valid loss: 1.3467  valid_loss_mov_avg: 1.9281
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3601  valid loss: 1.3376  valid_loss_mov_avg: 1.9222
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3596  valid loss: 1.3388  valid_loss_mov_avg: 1.9164
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3636  valid loss: 1.3422  valid_loss_mov_avg: 1.9106
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3455  valid loss: 1.3377  valid_loss_mov_avg: 1.9049
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3481  valid loss: 1.3384  valid_loss_mov_avg: 1.8992
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3394  valid loss: 1.3380  valid_loss_mov_avg: 1.8936
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3298  valid loss: 1.3367  valid_loss_mov_avg: 1.8881
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3420  valid loss: 1.3350  valid_loss_mov_avg: 1.8825
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3380  valid loss: 1.3400  valid_loss_mov_avg: 1.8771
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3403  valid loss: 1.3386  valid_loss_mov_avg: 1.8717
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3306  valid loss: 1.3433  valid_loss_mov_avg: 1.8664
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3365  valid loss: 1.3393  valid_loss_mov_avg: 1.8612
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3229  valid loss: 1.3513  valid_loss_mov_avg: 1.8561
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3265  valid loss: 1.3538  valid_loss_mov_avg: 1.8510
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3348  valid loss: 1.3488  valid_loss_mov_avg: 1.8460
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3322  valid loss: 1.3520  valid_loss_mov_avg: 1.8411
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3235  valid loss: 1.3394  valid_loss_mov_avg: 1.8361
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3130  valid loss: 1.3494  valid_loss_mov_avg: 1.8312
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3057  valid loss: 1.3535  valid_loss_mov_avg: 1.8264
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3206  valid loss: 1.3552  valid_loss_mov_avg: 1.8217
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3255  valid loss: 1.3544  valid_loss_mov_avg: 1.8170
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3100  valid loss: 1.3555  valid_loss_mov_avg: 1.8124
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3212  valid loss: 1.3503  valid_loss_mov_avg: 1.8078
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3155  valid loss: 1.3522  valid_loss_mov_avg: 1.8032
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.3165  valid loss: 1.3502  valid_loss_mov_avg: 1.7987
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3089  valid loss: 1.3511  valid_loss_mov_avg: 1.7942
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3020  valid loss: 1.3499  valid_loss_mov_avg: 1.7898
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2930  valid loss: 1.3603  valid_loss_mov_avg: 1.7855
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.3070  valid loss: 1.3555  valid_loss_mov_avg: 1.7812
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.3008  valid loss: 1.3610  valid_loss_mov_avg: 1.7770
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3030  valid loss: 1.3596  valid_loss_mov_avg: 1.7728
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2941  valid loss: 1.3609  valid_loss_mov_avg: 1.7687
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.3022  valid loss: 1.3582  valid_loss_mov_avg: 1.7646
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2959  valid loss: 1.3621  valid_loss_mov_avg: 1.7606
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2875  valid loss: 1.3545  valid_loss_mov_avg: 1.7565

 ---------------- Grad-Step 420 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 421 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 422 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 423 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 424 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 425 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 426 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 427 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 428 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 429 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.38          |
| Data-EnvSampler-Poli... | 1.8           |
| Data-EnvTrajs-Averag... | 369           |
| Data-EnvTrajs-MaxReturn | 586           |
| Data-EnvTrajs-MinReturn | 193           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 174           |
| Data-TimeEnvSampleProc  | 0.000567      |
| Data-TimeEnvSampling    | 3.16          |
| Iteration               | 42            |
| ItrTime                 | 104           |
| LossAfter               | -0.0118497405 |
| LossBefore              | 8.34465e-09   |
| Model-TimeModelFit      | 37.5          |
| ModelSampler-n_times... | 1720000       |
| Policy-AverageAbsPol... | 0.9391694     |
| Policy-AverageDiscou... | 244           |
| Policy-AveragePolicyStd | 0.28047344    |
| Policy-AverageReturn    | 666           |
| Policy-MaxReturn        | 740           |
| Policy-MinReturn        | 578           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 44.6          |
| Policy-TimeAlgoOpt      | 1.22          |
| Policy-TimeSampleProc   | 0.173         |
| Policy-TimeSampling     | 62.3          |
| Policy-TimeStep         | 63.7          |
| Time                    | 4.31e+03      |
| n_timesteps             | 43000         |
-------------------------------------------

 ---------------- Iteration 43 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4523  valid loss: 1.3667  valid_loss_mov_avg: 2.0432
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4238  valid loss: 1.3422  valid_loss_mov_avg: 2.0362
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4008  valid loss: 1.3487  valid_loss_mov_avg: 2.0293
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.3950  valid loss: 1.3471  valid_loss_mov_avg: 2.0225
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.3759  valid loss: 1.3538  valid_loss_mov_avg: 2.0158
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3806  valid loss: 1.3541  valid_loss_mov_avg: 2.0092
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3775  valid loss: 1.3414  valid_loss_mov_avg: 2.0025
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3686  valid loss: 1.3415  valid_loss_mov_avg: 1.9959
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3575  valid loss: 1.3410  valid_loss_mov_avg: 1.9893
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3745  valid loss: 1.3376  valid_loss_mov_avg: 1.9828
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3665  valid loss: 1.3415  valid_loss_mov_avg: 1.9764
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3570  valid loss: 1.3435  valid_loss_mov_avg: 1.9701
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3449  valid loss: 1.3470  valid_loss_mov_avg: 1.9639
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3605  valid loss: 1.3482  valid_loss_mov_avg: 1.9577
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3469  valid loss: 1.3491  valid_loss_mov_avg: 1.9516
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3439  valid loss: 1.3406  valid_loss_mov_avg: 1.9455
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3365  valid loss: 1.3441  valid_loss_mov_avg: 1.9395
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3369  valid loss: 1.3422  valid_loss_mov_avg: 1.9335
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3276  valid loss: 1.3482  valid_loss_mov_avg: 1.9277
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3274  valid loss: 1.3549  valid_loss_mov_avg: 1.9219
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3480  valid loss: 1.3448  valid_loss_mov_avg: 1.9162
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3482  valid loss: 1.3605  valid_loss_mov_avg: 1.9106
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3470  valid loss: 1.3550  valid_loss_mov_avg: 1.9050
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3210  valid loss: 1.3501  valid_loss_mov_avg: 1.8995
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3154  valid loss: 1.3588  valid_loss_mov_avg: 1.8941
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3322  valid loss: 1.3418  valid_loss_mov_avg: 1.8886
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3169  valid loss: 1.3486  valid_loss_mov_avg: 1.8832
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3145  valid loss: 1.3619  valid_loss_mov_avg: 1.8780
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3239  valid loss: 1.3606  valid_loss_mov_avg: 1.8728
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3129  valid loss: 1.3626  valid_loss_mov_avg: 1.8677
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3206  valid loss: 1.3747  valid_loss_mov_avg: 1.8628
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3295  valid loss: 1.3576  valid_loss_mov_avg: 1.8577
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3084  valid loss: 1.3669  valid_loss_mov_avg: 1.8528
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3017  valid loss: 1.3623  valid_loss_mov_avg: 1.8479
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.2999  valid loss: 1.3654  valid_loss_mov_avg: 1.8431
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3158  valid loss: 1.3657  valid_loss_mov_avg: 1.8383
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3158  valid loss: 1.3520  valid_loss_mov_avg: 1.8334
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.3065  valid loss: 1.3556  valid_loss_mov_avg: 1.8286
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.3048  valid loss: 1.3588  valid_loss_mov_avg: 1.8239
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.2907  valid loss: 1.3636  valid_loss_mov_avg: 1.8193
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.3067  valid loss: 1.3664  valid_loss_mov_avg: 1.8148
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.3099  valid loss: 1.3586  valid_loss_mov_avg: 1.8103
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2972  valid loss: 1.3581  valid_loss_mov_avg: 1.8057
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2973  valid loss: 1.3716  valid_loss_mov_avg: 1.8014
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2858  valid loss: 1.3771  valid_loss_mov_avg: 1.7971
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3087  valid loss: 1.3715  valid_loss_mov_avg: 1.7929
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2930  valid loss: 1.3634  valid_loss_mov_avg: 1.7886
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2954  valid loss: 1.3677  valid_loss_mov_avg: 1.7844
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2790  valid loss: 1.3718  valid_loss_mov_avg: 1.7803
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2958  valid loss: 1.3738  valid_loss_mov_avg: 1.7762

 ---------------- Grad-Step 430 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 431 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 432 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 433 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 434 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 435 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 436 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 437 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 438 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 439 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
------------------------------------------
| Data-EnvSampler-EnvE... | 0.48         |
| Data-EnvSampler-Poli... | 2.32         |
| Data-EnvTrajs-Averag... | 411          |
| Data-EnvTrajs-MaxReturn | 593          |
| Data-EnvTrajs-MinReturn | -13          |
| Data-EnvTrajs-NumTrajs  | 5            |
| Data-EnvTrajs-StdReturn | 220          |
| Data-TimeEnvSampleProc  | 0.000535     |
| Data-TimeEnvSampling    | 3.92         |
| Iteration               | 43           |
| ItrTime                 | 104          |
| LossAfter               | -0.01403412  |
| LossBefore              | 3.695488e-09 |
| Model-TimeModelFit      | 39.1         |
| ModelSampler-n_times... | 1760000      |
| Policy-AverageAbsPol... | 0.91731924   |
| Policy-AverageDiscou... | 270          |
| Policy-AveragePolicyStd | 0.27153903   |
| Policy-AverageReturn    | 715          |
| Policy-MaxReturn        | 772          |
| Policy-MinReturn        | 665          |
| Policy-NumTrajs         | 20           |
| Policy-StdReturn        | 26.7         |
| Policy-TimeAlgoOpt      | 1.1          |
| Policy-TimeSampleProc   | 0.175        |
| Policy-TimeSampling     | 59.2         |
| Policy-TimeStep         | 60.5         |
| Time                    | 4.41e+03     |
| n_timesteps             | 44000        |
------------------------------------------

 ---------------- Iteration 44 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4404  valid loss: 1.3760  valid_loss_mov_avg: 2.0571
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4225  valid loss: 1.3708  valid_loss_mov_avg: 2.0502
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4059  valid loss: 1.3751  valid_loss_mov_avg: 2.0435
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.3887  valid loss: 1.3843  valid_loss_mov_avg: 2.0369
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.3798  valid loss: 1.3696  valid_loss_mov_avg: 2.0302
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3795  valid loss: 1.3754  valid_loss_mov_avg: 2.0236
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3744  valid loss: 1.3794  valid_loss_mov_avg: 2.0172
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3704  valid loss: 1.3743  valid_loss_mov_avg: 2.0108
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3585  valid loss: 1.3826  valid_loss_mov_avg: 2.0045
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3584  valid loss: 1.3754  valid_loss_mov_avg: 1.9982
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3454  valid loss: 1.3686  valid_loss_mov_avg: 1.9919
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3474  valid loss: 1.3796  valid_loss_mov_avg: 1.9858
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3523  valid loss: 1.3793  valid_loss_mov_avg: 1.9797
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3418  valid loss: 1.3847  valid_loss_mov_avg: 1.9738
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3437  valid loss: 1.3793  valid_loss_mov_avg: 1.9678
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3398  valid loss: 1.3788  valid_loss_mov_avg: 1.9619
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3248  valid loss: 1.3803  valid_loss_mov_avg: 1.9561
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3353  valid loss: 1.3804  valid_loss_mov_avg: 1.9504
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3368  valid loss: 1.3834  valid_loss_mov_avg: 1.9447
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3369  valid loss: 1.3710  valid_loss_mov_avg: 1.9389
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3164  valid loss: 1.3872  valid_loss_mov_avg: 1.9334
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3258  valid loss: 1.3791  valid_loss_mov_avg: 1.9279
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3156  valid loss: 1.3802  valid_loss_mov_avg: 1.9224
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3123  valid loss: 1.3809  valid_loss_mov_avg: 1.9170
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3143  valid loss: 1.3849  valid_loss_mov_avg: 1.9117
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3044  valid loss: 1.3685  valid_loss_mov_avg: 1.9062
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3029  valid loss: 1.3808  valid_loss_mov_avg: 1.9010
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3128  valid loss: 1.3913  valid_loss_mov_avg: 1.8959
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3112  valid loss: 1.3932  valid_loss_mov_avg: 1.8909
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3128  valid loss: 1.3935  valid_loss_mov_avg: 1.8859
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3136  valid loss: 1.3793  valid_loss_mov_avg: 1.8808
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3035  valid loss: 1.3874  valid_loss_mov_avg: 1.8759
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3012  valid loss: 1.3945  valid_loss_mov_avg: 1.8711
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3151  valid loss: 1.3948  valid_loss_mov_avg: 1.8663
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.2989  valid loss: 1.3907  valid_loss_mov_avg: 1.8616
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.2974  valid loss: 1.3848  valid_loss_mov_avg: 1.8568
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.2992  valid loss: 1.3895  valid_loss_mov_avg: 1.8521
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.2900  valid loss: 1.3902  valid_loss_mov_avg: 1.8475
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.2904  valid loss: 1.3932  valid_loss_mov_avg: 1.8430
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.2951  valid loss: 1.3854  valid_loss_mov_avg: 1.8384
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.2955  valid loss: 1.3950  valid_loss_mov_avg: 1.8339
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.2910  valid loss: 1.4092  valid_loss_mov_avg: 1.8297
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2914  valid loss: 1.3861  valid_loss_mov_avg: 1.8253
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2940  valid loss: 1.3983  valid_loss_mov_avg: 1.8210
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2887  valid loss: 1.3915  valid_loss_mov_avg: 1.8167
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.3003  valid loss: 1.3940  valid_loss_mov_avg: 1.8125
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2851  valid loss: 1.3988  valid_loss_mov_avg: 1.8083
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2828  valid loss: 1.3919  valid_loss_mov_avg: 1.8042
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2793  valid loss: 1.3990  valid_loss_mov_avg: 1.8001
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2905  valid loss: 1.4006  valid_loss_mov_avg: 1.7961

 ---------------- Grad-Step 440 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 441 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 442 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 443 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 444 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 445 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 446 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 447 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 448 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 449 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
------------------------------------------
| Data-EnvSampler-EnvE... | 0.382        |
| Data-EnvSampler-Poli... | 1.83         |
| Data-EnvTrajs-Averag... | 602          |
| Data-EnvTrajs-MaxReturn | 642          |
| Data-EnvTrajs-MinReturn | 542          |
| Data-EnvTrajs-NumTrajs  | 5            |
| Data-EnvTrajs-StdReturn | 41.3         |
| Data-TimeEnvSampleProc  | 0.000492     |
| Data-TimeEnvSampling    | 3.27         |
| Iteration               | 44           |
| ItrTime                 | 98.1         |
| LossAfter               | -0.014080424 |
| LossBefore              | 1.66893e-08  |
| Model-TimeModelFit      | 36.5         |
| ModelSampler-n_times... | 1800000      |
| Policy-AverageAbsPol... | 0.9052126    |
| Policy-AverageDiscou... | 238          |
| Policy-AveragePolicyStd | 0.2629532    |
| Policy-AverageReturn    | 636          |
| Policy-MaxReturn        | 681          |
| Policy-MinReturn        | 562          |
| Policy-NumTrajs         | 20           |
| Policy-StdReturn        | 32.7         |
| Policy-TimeAlgoOpt      | 1.07         |
| Policy-TimeSampleProc   | 0.182        |
| Policy-TimeSampling     | 57           |
| Policy-TimeStep         | 58.3         |
| Time                    | 4.51e+03     |
| n_timesteps             | 45000        |
------------------------------------------

 ---------------- Iteration 45 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4072  valid loss: 1.3885  valid_loss_mov_avg: 2.0759
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4092  valid loss: 1.3968  valid_loss_mov_avg: 2.0691
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.3794  valid loss: 1.3906  valid_loss_mov_avg: 2.0623
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.3691  valid loss: 1.3817  valid_loss_mov_avg: 2.0555
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.3603  valid loss: 1.3818  valid_loss_mov_avg: 2.0487
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3601  valid loss: 1.3764  valid_loss_mov_avg: 2.0420
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3489  valid loss: 1.4008  valid_loss_mov_avg: 2.0356
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3555  valid loss: 1.3893  valid_loss_mov_avg: 2.0291
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3417  valid loss: 1.3907  valid_loss_mov_avg: 2.0228
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3388  valid loss: 1.3952  valid_loss_mov_avg: 2.0165
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3370  valid loss: 1.3946  valid_loss_mov_avg: 2.0103
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3305  valid loss: 1.3912  valid_loss_mov_avg: 2.0041
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3328  valid loss: 1.3898  valid_loss_mov_avg: 1.9979
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3462  valid loss: 1.3970  valid_loss_mov_avg: 1.9919
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3243  valid loss: 1.3916  valid_loss_mov_avg: 1.9859
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3247  valid loss: 1.3921  valid_loss_mov_avg: 1.9800
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3216  valid loss: 1.4078  valid_loss_mov_avg: 1.9743
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3143  valid loss: 1.3931  valid_loss_mov_avg: 1.9685
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3124  valid loss: 1.3949  valid_loss_mov_avg: 1.9627
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3084  valid loss: 1.4034  valid_loss_mov_avg: 1.9571
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3093  valid loss: 1.4063  valid_loss_mov_avg: 1.9516
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3152  valid loss: 1.4073  valid_loss_mov_avg: 1.9462
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3024  valid loss: 1.3933  valid_loss_mov_avg: 1.9406
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3035  valid loss: 1.3997  valid_loss_mov_avg: 1.9352
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3385  valid loss: 1.4193  valid_loss_mov_avg: 1.9301
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3237  valid loss: 1.3918  valid_loss_mov_avg: 1.9247
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3112  valid loss: 1.4027  valid_loss_mov_avg: 1.9195
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3057  valid loss: 1.4053  valid_loss_mov_avg: 1.9143
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3047  valid loss: 1.4071  valid_loss_mov_avg: 1.9093
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3055  valid loss: 1.3955  valid_loss_mov_avg: 1.9041
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.2937  valid loss: 1.3999  valid_loss_mov_avg: 1.8991
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3121  valid loss: 1.4037  valid_loss_mov_avg: 1.8941
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3020  valid loss: 1.4062  valid_loss_mov_avg: 1.8892
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.2959  valid loss: 1.4057  valid_loss_mov_avg: 1.8844
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.2904  valid loss: 1.4011  valid_loss_mov_avg: 1.8796
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.2829  valid loss: 1.4011  valid_loss_mov_avg: 1.8748
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.2983  valid loss: 1.4221  valid_loss_mov_avg: 1.8703
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.2925  valid loss: 1.4042  valid_loss_mov_avg: 1.8656
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.2861  valid loss: 1.4118  valid_loss_mov_avg: 1.8611
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.2783  valid loss: 1.4096  valid_loss_mov_avg: 1.8566
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.2841  valid loss: 1.4106  valid_loss_mov_avg: 1.8521
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.2660  valid loss: 1.4074  valid_loss_mov_avg: 1.8476
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2771  valid loss: 1.4156  valid_loss_mov_avg: 1.8433
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2843  valid loss: 1.4169  valid_loss_mov_avg: 1.8391
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2796  valid loss: 1.4119  valid_loss_mov_avg: 1.8348
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2792  valid loss: 1.4129  valid_loss_mov_avg: 1.8306
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2757  valid loss: 1.4167  valid_loss_mov_avg: 1.8264
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2854  valid loss: 1.4103  valid_loss_mov_avg: 1.8223
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2680  valid loss: 1.4195  valid_loss_mov_avg: 1.8182
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2623  valid loss: 1.4190  valid_loss_mov_avg: 1.8143

 ---------------- Grad-Step 450 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 451 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 452 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 453 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 454 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 455 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 456 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 457 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 458 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 459 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.381          |
| Data-EnvSampler-Poli... | 1.81           |
| Data-EnvTrajs-Averag... | 616            |
| Data-EnvTrajs-MaxReturn | 662            |
| Data-EnvTrajs-MinReturn | 538            |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 44.5           |
| Data-TimeEnvSampleProc  | 0.000531       |
| Data-TimeEnvSampling    | 3.16           |
| Iteration               | 45             |
| ItrTime                 | 105            |
| LossAfter               | -0.015961826   |
| LossBefore              | -1.6331672e-08 |
| Model-TimeModelFit      | 37.9           |
| ModelSampler-n_times... | 1840000        |
| Policy-AverageAbsPol... | 0.9303099      |
| Policy-AverageDiscou... | 278            |
| Policy-AveragePolicyStd | 0.25182402     |
| Policy-AverageReturn    | 739            |
| Policy-MaxReturn        | 783            |
| Policy-MinReturn        | 695            |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 19.4           |
| Policy-TimeAlgoOpt      | 1.19           |
| Policy-TimeSampleProc   | 0.171          |
| Policy-TimeSampling     | 63             |
| Policy-TimeStep         | 64.4           |
| Time                    | 4.62e+03       |
| n_timesteps             | 46000          |
--------------------------------------------

 ---------------- Iteration 46 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4516  valid loss: 1.3926  valid_loss_mov_avg: 2.0820
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4145  valid loss: 1.3893  valid_loss_mov_avg: 2.0751
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.4041  valid loss: 1.3861  valid_loss_mov_avg: 2.0682
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.3924  valid loss: 1.3795  valid_loss_mov_avg: 2.0613
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.3851  valid loss: 1.3802  valid_loss_mov_avg: 2.0545
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3772  valid loss: 1.3729  valid_loss_mov_avg: 2.0477
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3644  valid loss: 1.3786  valid_loss_mov_avg: 2.0410
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3646  valid loss: 1.3822  valid_loss_mov_avg: 2.0344
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3467  valid loss: 1.3844  valid_loss_mov_avg: 2.0279
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3700  valid loss: 1.3790  valid_loss_mov_avg: 2.0214
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3577  valid loss: 1.3772  valid_loss_mov_avg: 2.0150
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3510  valid loss: 1.3722  valid_loss_mov_avg: 2.0085
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3455  valid loss: 1.3965  valid_loss_mov_avg: 2.0024
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3459  valid loss: 1.3767  valid_loss_mov_avg: 1.9962
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3489  valid loss: 1.3751  valid_loss_mov_avg: 1.9900
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3393  valid loss: 1.3735  valid_loss_mov_avg: 1.9838
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3276  valid loss: 1.3957  valid_loss_mov_avg: 1.9779
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3323  valid loss: 1.3862  valid_loss_mov_avg: 1.9720
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3321  valid loss: 1.3849  valid_loss_mov_avg: 1.9661
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3303  valid loss: 1.3723  valid_loss_mov_avg: 1.9602
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3307  valid loss: 1.3825  valid_loss_mov_avg: 1.9544
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3329  valid loss: 1.3817  valid_loss_mov_avg: 1.9487
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3218  valid loss: 1.3723  valid_loss_mov_avg: 1.9429
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3130  valid loss: 1.3811  valid_loss_mov_avg: 1.9373
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3056  valid loss: 1.3930  valid_loss_mov_avg: 1.9318
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3344  valid loss: 1.3775  valid_loss_mov_avg: 1.9263
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3309  valid loss: 1.3930  valid_loss_mov_avg: 1.9210
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3177  valid loss: 1.3994  valid_loss_mov_avg: 1.9158
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3225  valid loss: 1.3875  valid_loss_mov_avg: 1.9105
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.3072  valid loss: 1.3869  valid_loss_mov_avg: 1.9052
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.2974  valid loss: 1.3801  valid_loss_mov_avg: 1.9000
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.3036  valid loss: 1.3865  valid_loss_mov_avg: 1.8949
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.2961  valid loss: 1.3975  valid_loss_mov_avg: 1.8899
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.3102  valid loss: 1.3814  valid_loss_mov_avg: 1.8848
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3006  valid loss: 1.3964  valid_loss_mov_avg: 1.8799
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.3109  valid loss: 1.3823  valid_loss_mov_avg: 1.8749
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.3017  valid loss: 1.3886  valid_loss_mov_avg: 1.8701
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.2874  valid loss: 1.3948  valid_loss_mov_avg: 1.8653
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.2933  valid loss: 1.3926  valid_loss_mov_avg: 1.8606
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.2968  valid loss: 1.3899  valid_loss_mov_avg: 1.8559
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.2904  valid loss: 1.3884  valid_loss_mov_avg: 1.8512
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.2813  valid loss: 1.3948  valid_loss_mov_avg: 1.8466
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2790  valid loss: 1.3994  valid_loss_mov_avg: 1.8422
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2828  valid loss: 1.3931  valid_loss_mov_avg: 1.8377
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2900  valid loss: 1.3970  valid_loss_mov_avg: 1.8333
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2977  valid loss: 1.3945  valid_loss_mov_avg: 1.8289
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2745  valid loss: 1.3949  valid_loss_mov_avg: 1.8245
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2797  valid loss: 1.3943  valid_loss_mov_avg: 1.8202
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2788  valid loss: 1.3900  valid_loss_mov_avg: 1.8159
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2831  valid loss: 1.3954  valid_loss_mov_avg: 1.8117

 ---------------- Grad-Step 460 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 461 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 462 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 463 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 464 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 465 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 466 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 467 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 468 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 469 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.398          |
| Data-EnvSampler-Poli... | 1.9            |
| Data-EnvTrajs-Averag... | 314            |
| Data-EnvTrajs-MaxReturn | 573            |
| Data-EnvTrajs-MinReturn | -58.2          |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 240            |
| Data-TimeEnvSampleProc  | 0.000814       |
| Data-TimeEnvSampling    | 3.3            |
| Iteration               | 46             |
| ItrTime                 | 110            |
| LossAfter               | -0.011769668   |
| LossBefore              | -8.5830685e-09 |
| Model-TimeModelFit      | 46.3           |
| ModelSampler-n_times... | 1880000        |
| Policy-AverageAbsPol... | 0.9018926      |
| Policy-AverageDiscou... | 283            |
| Policy-AveragePolicyStd | 0.24462795     |
| Policy-AverageReturn    | 748            |
| Policy-MaxReturn        | 800            |
| Policy-MinReturn        | 712            |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 20.4           |
| Policy-TimeAlgoOpt      | 1.05           |
| Policy-TimeSampleProc   | 0.155          |
| Policy-TimeSampling     | 59.5           |
| Policy-TimeStep         | 60.7           |
| Time                    | 4.73e+03       |
| n_timesteps             | 47000          |
--------------------------------------------

 ---------------- Iteration 47 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.3957  valid loss: 1.3895  valid_loss_mov_avg: 2.0774
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.3931  valid loss: 1.3753  valid_loss_mov_avg: 2.0704
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.3893  valid loss: 1.3665  valid_loss_mov_avg: 2.0633
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.3676  valid loss: 1.3752  valid_loss_mov_avg: 2.0564
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.3570  valid loss: 1.3628  valid_loss_mov_avg: 2.0495
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3453  valid loss: 1.3729  valid_loss_mov_avg: 2.0427
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3379  valid loss: 1.3709  valid_loss_mov_avg: 2.0360
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3477  valid loss: 1.3726  valid_loss_mov_avg: 2.0294
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3370  valid loss: 1.3676  valid_loss_mov_avg: 2.0228
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3287  valid loss: 1.3773  valid_loss_mov_avg: 2.0163
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3280  valid loss: 1.3681  valid_loss_mov_avg: 2.0098
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3255  valid loss: 1.3743  valid_loss_mov_avg: 2.0035
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3310  valid loss: 1.3759  valid_loss_mov_avg: 1.9972
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3212  valid loss: 1.3713  valid_loss_mov_avg: 1.9909
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3112  valid loss: 1.3731  valid_loss_mov_avg: 1.9848
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3099  valid loss: 1.3741  valid_loss_mov_avg: 1.9787
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3101  valid loss: 1.3807  valid_loss_mov_avg: 1.9727
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3107  valid loss: 1.3696  valid_loss_mov_avg: 1.9666
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3023  valid loss: 1.3793  valid_loss_mov_avg: 1.9608
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3140  valid loss: 1.3762  valid_loss_mov_avg: 1.9549
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3276  valid loss: 1.3712  valid_loss_mov_avg: 1.9491
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3043  valid loss: 1.3685  valid_loss_mov_avg: 1.9433
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.3071  valid loss: 1.3774  valid_loss_mov_avg: 1.9376
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3092  valid loss: 1.3745  valid_loss_mov_avg: 1.9320
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3026  valid loss: 1.3727  valid_loss_mov_avg: 1.9264
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.2927  valid loss: 1.3738  valid_loss_mov_avg: 1.9209
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.3064  valid loss: 1.3907  valid_loss_mov_avg: 1.9156
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3039  valid loss: 1.3785  valid_loss_mov_avg: 1.9102
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.2996  valid loss: 1.3826  valid_loss_mov_avg: 1.9049
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.2813  valid loss: 1.3818  valid_loss_mov_avg: 1.8997
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.2917  valid loss: 1.3744  valid_loss_mov_avg: 1.8944
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.2869  valid loss: 1.3820  valid_loss_mov_avg: 1.8893
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.2771  valid loss: 1.3854  valid_loss_mov_avg: 1.8843
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.2785  valid loss: 1.3893  valid_loss_mov_avg: 1.8793
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.2795  valid loss: 1.3825  valid_loss_mov_avg: 1.8744
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.2766  valid loss: 1.3930  valid_loss_mov_avg: 1.8695
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.2803  valid loss: 1.3913  valid_loss_mov_avg: 1.8648
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.2935  valid loss: 1.3920  valid_loss_mov_avg: 1.8600
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.2901  valid loss: 1.3838  valid_loss_mov_avg: 1.8553
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.2822  valid loss: 1.3892  valid_loss_mov_avg: 1.8506
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.2770  valid loss: 1.3851  valid_loss_mov_avg: 1.8460
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.2791  valid loss: 1.3836  valid_loss_mov_avg: 1.8413
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2787  valid loss: 1.3849  valid_loss_mov_avg: 1.8368
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2835  valid loss: 1.3870  valid_loss_mov_avg: 1.8323
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2759  valid loss: 1.3846  valid_loss_mov_avg: 1.8278
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2748  valid loss: 1.3882  valid_loss_mov_avg: 1.8234
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2664  valid loss: 1.3888  valid_loss_mov_avg: 1.8190
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2712  valid loss: 1.3900  valid_loss_mov_avg: 1.8148
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2589  valid loss: 1.3840  valid_loss_mov_avg: 1.8105
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2704  valid loss: 1.3948  valid_loss_mov_avg: 1.8063

 ---------------- Grad-Step 470 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 471 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 472 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 473 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 474 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 475 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 476 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 477 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 478 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 479 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
------------------------------------------
| Data-EnvSampler-EnvE... | 0.387        |
| Data-EnvSampler-Poli... | 1.87         |
| Data-EnvTrajs-Averag... | 567          |
| Data-EnvTrajs-MaxReturn | 694          |
| Data-EnvTrajs-MinReturn | 292          |
| Data-EnvTrajs-NumTrajs  | 5            |
| Data-EnvTrajs-StdReturn | 143          |
| Data-TimeEnvSampleProc  | 0.00153      |
| Data-TimeEnvSampling    | 3.25         |
| Iteration               | 47           |
| ItrTime                 | 105          |
| LossAfter               | -0.014278792 |
| LossBefore              | 4.768372e-10 |
| Model-TimeModelFit      | 40.5         |
| ModelSampler-n_times... | 1920000      |
| Policy-AverageAbsPol... | 0.91643596   |
| Policy-AverageDiscou... | 276          |
| Policy-AveragePolicyStd | 0.23849723   |
| Policy-AverageReturn    | 728          |
| Policy-MaxReturn        | 768          |
| Policy-MinReturn        | 669          |
| Policy-NumTrajs         | 20           |
| Policy-StdReturn        | 22.8         |
| Policy-TimeAlgoOpt      | 1            |
| Policy-TimeSampleProc   | 0.199        |
| Policy-TimeSampling     | 60           |
| Policy-TimeStep         | 61.2         |
| Time                    | 4.83e+03     |
| n_timesteps             | 48000        |
------------------------------------------

 ---------------- Iteration 48 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4243  valid loss: 1.3805  valid_loss_mov_avg: 2.0639
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.4024  valid loss: 1.3829  valid_loss_mov_avg: 2.0571
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.3905  valid loss: 1.3669  valid_loss_mov_avg: 2.0502
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.3677  valid loss: 1.3627  valid_loss_mov_avg: 2.0433
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.3661  valid loss: 1.3669  valid_loss_mov_avg: 2.0365
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3610  valid loss: 1.3673  valid_loss_mov_avg: 2.0298
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3465  valid loss: 1.3635  valid_loss_mov_avg: 2.0232
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3517  valid loss: 1.3564  valid_loss_mov_avg: 2.0165
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3486  valid loss: 1.3667  valid_loss_mov_avg: 2.0100
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3321  valid loss: 1.3606  valid_loss_mov_avg: 2.0035
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3421  valid loss: 1.3630  valid_loss_mov_avg: 1.9971
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3346  valid loss: 1.3697  valid_loss_mov_avg: 1.9908
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3405  valid loss: 1.3670  valid_loss_mov_avg: 1.9846
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3282  valid loss: 1.3653  valid_loss_mov_avg: 1.9784
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3307  valid loss: 1.3664  valid_loss_mov_avg: 1.9723
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3234  valid loss: 1.3659  valid_loss_mov_avg: 1.9662
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3176  valid loss: 1.3676  valid_loss_mov_avg: 1.9602
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3075  valid loss: 1.3700  valid_loss_mov_avg: 1.9543
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3099  valid loss: 1.3689  valid_loss_mov_avg: 1.9485
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3254  valid loss: 1.3660  valid_loss_mov_avg: 1.9426
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3093  valid loss: 1.3741  valid_loss_mov_avg: 1.9370
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3143  valid loss: 1.3687  valid_loss_mov_avg: 1.9313
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.2989  valid loss: 1.3740  valid_loss_mov_avg: 1.9257
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3108  valid loss: 1.3678  valid_loss_mov_avg: 1.9201
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.2908  valid loss: 1.3747  valid_loss_mov_avg: 1.9147
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3106  valid loss: 1.3707  valid_loss_mov_avg: 1.9092
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.2973  valid loss: 1.3715  valid_loss_mov_avg: 1.9038
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.2925  valid loss: 1.3725  valid_loss_mov_avg: 1.8985
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.2970  valid loss: 1.3739  valid_loss_mov_avg: 1.8933
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.2955  valid loss: 1.3677  valid_loss_mov_avg: 1.8880
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.2974  valid loss: 1.3740  valid_loss_mov_avg: 1.8829
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.2876  valid loss: 1.3757  valid_loss_mov_avg: 1.8778
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.2964  valid loss: 1.3748  valid_loss_mov_avg: 1.8728
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.2896  valid loss: 1.3845  valid_loss_mov_avg: 1.8679
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.2940  valid loss: 1.3830  valid_loss_mov_avg: 1.8631
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.2862  valid loss: 1.3811  valid_loss_mov_avg: 1.8582
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.2821  valid loss: 1.3824  valid_loss_mov_avg: 1.8535
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.2884  valid loss: 1.3739  valid_loss_mov_avg: 1.8487
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.2681  valid loss: 1.3856  valid_loss_mov_avg: 1.8441
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.2918  valid loss: 1.3768  valid_loss_mov_avg: 1.8394
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.2795  valid loss: 1.3794  valid_loss_mov_avg: 1.8348
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.2695  valid loss: 1.3858  valid_loss_mov_avg: 1.8303
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2724  valid loss: 1.3949  valid_loss_mov_avg: 1.8259
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2806  valid loss: 1.3826  valid_loss_mov_avg: 1.8215
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2732  valid loss: 1.3905  valid_loss_mov_avg: 1.8172
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2737  valid loss: 1.3792  valid_loss_mov_avg: 1.8128
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2758  valid loss: 1.3878  valid_loss_mov_avg: 1.8086
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2681  valid loss: 1.3856  valid_loss_mov_avg: 1.8043
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2716  valid loss: 1.3924  valid_loss_mov_avg: 1.8002
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2731  valid loss: 1.3904  valid_loss_mov_avg: 1.7961

 ---------------- Grad-Step 480 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 481 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 482 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 483 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 484 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 485 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 486 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 487 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 488 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 489 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.382         |
| Data-EnvSampler-Poli... | 1.82          |
| Data-EnvTrajs-Averag... | 257           |
| Data-EnvTrajs-MaxReturn | 679           |
| Data-EnvTrajs-MinReturn | -30.5         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 336           |
| Data-TimeEnvSampleProc  | 0.000572      |
| Data-TimeEnvSampling    | 3.18          |
| Iteration               | 48            |
| ItrTime                 | 113           |
| LossAfter               | -0.010592577  |
| LossBefore              | -2.425909e-08 |
| Model-TimeModelFit      | 39.8          |
| ModelSampler-n_times... | 1960000       |
| Policy-AverageAbsPol... | 0.95854324    |
| Policy-AverageDiscou... | 247           |
| Policy-AveragePolicyStd | 0.23107608    |
| Policy-AverageReturn    | 670           |
| Policy-MaxReturn        | 736           |
| Policy-MinReturn        | 619           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 33.1          |
| Policy-TimeAlgoOpt      | 1.08          |
| Policy-TimeSampleProc   | 0.141         |
| Policy-TimeSampling     | 69            |
| Policy-TimeStep         | 70.2          |
| Time                    | 4.95e+03      |
| n_timesteps             | 49000         |
-------------------------------------------

 ---------------- Iteration 49 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.4214  valid loss: 1.3555  valid_loss_mov_avg: 2.0265
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.3911  valid loss: 1.3514  valid_loss_mov_avg: 2.0197
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.3814  valid loss: 1.3859  valid_loss_mov_avg: 2.0134
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.3878  valid loss: 1.3471  valid_loss_mov_avg: 2.0067
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.3661  valid loss: 1.3618  valid_loss_mov_avg: 2.0003
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3650  valid loss: 1.3643  valid_loss_mov_avg: 1.9939
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3586  valid loss: 1.3574  valid_loss_mov_avg: 1.9875
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3585  valid loss: 1.3588  valid_loss_mov_avg: 1.9812
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3532  valid loss: 1.3508  valid_loss_mov_avg: 1.9749
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3337  valid loss: 1.3545  valid_loss_mov_avg: 1.9687
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3388  valid loss: 1.3491  valid_loss_mov_avg: 1.9625
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3387  valid loss: 1.3600  valid_loss_mov_avg: 1.9565
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3384  valid loss: 1.3599  valid_loss_mov_avg: 1.9505
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3532  valid loss: 1.3605  valid_loss_mov_avg: 1.9446
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3324  valid loss: 1.3489  valid_loss_mov_avg: 1.9387
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3411  valid loss: 1.3591  valid_loss_mov_avg: 1.9329
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3212  valid loss: 1.3553  valid_loss_mov_avg: 1.9271
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.3239  valid loss: 1.3555  valid_loss_mov_avg: 1.9214
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3054  valid loss: 1.3591  valid_loss_mov_avg: 1.9158
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3174  valid loss: 1.3622  valid_loss_mov_avg: 1.9102
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3094  valid loss: 1.3556  valid_loss_mov_avg: 1.9047
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.3052  valid loss: 1.3621  valid_loss_mov_avg: 1.8993
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.2995  valid loss: 1.3594  valid_loss_mov_avg: 1.8939
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.3088  valid loss: 1.3599  valid_loss_mov_avg: 1.8885
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.3057  valid loss: 1.3675  valid_loss_mov_avg: 1.8833
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.3201  valid loss: 1.3681  valid_loss_mov_avg: 1.8782
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.2994  valid loss: 1.3694  valid_loss_mov_avg: 1.8731
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.3050  valid loss: 1.3600  valid_loss_mov_avg: 1.8679
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.3017  valid loss: 1.3683  valid_loss_mov_avg: 1.8630
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.2945  valid loss: 1.3650  valid_loss_mov_avg: 1.8580
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.3021  valid loss: 1.3657  valid_loss_mov_avg: 1.8531
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.2874  valid loss: 1.3650  valid_loss_mov_avg: 1.8482
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.3071  valid loss: 1.3696  valid_loss_mov_avg: 1.8434
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.2922  valid loss: 1.3778  valid_loss_mov_avg: 1.8387
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.3088  valid loss: 1.3662  valid_loss_mov_avg: 1.8340
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.2900  valid loss: 1.3629  valid_loss_mov_avg: 1.8293
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.2794  valid loss: 1.3730  valid_loss_mov_avg: 1.8247
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.2853  valid loss: 1.3735  valid_loss_mov_avg: 1.8202
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.2897  valid loss: 1.3641  valid_loss_mov_avg: 1.8157
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.2805  valid loss: 1.3744  valid_loss_mov_avg: 1.8112
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.2893  valid loss: 1.3751  valid_loss_mov_avg: 1.8069
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.2959  valid loss: 1.3788  valid_loss_mov_avg: 1.8026
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2813  valid loss: 1.3663  valid_loss_mov_avg: 1.7982
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2757  valid loss: 1.3764  valid_loss_mov_avg: 1.7940
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2795  valid loss: 1.3815  valid_loss_mov_avg: 1.7899
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2856  valid loss: 1.3788  valid_loss_mov_avg: 1.7858
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2809  valid loss: 1.3756  valid_loss_mov_avg: 1.7817
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2697  valid loss: 1.3697  valid_loss_mov_avg: 1.7776
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2793  valid loss: 1.3846  valid_loss_mov_avg: 1.7736
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2826  valid loss: 1.3785  valid_loss_mov_avg: 1.7697

 ---------------- Grad-Step 490 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 491 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 492 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 493 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 494 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 495 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 496 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 497 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 498 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 499 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.419         |
| Data-EnvSampler-Poli... | 2.02          |
| Data-EnvTrajs-Averag... | 294           |
| Data-EnvTrajs-MaxReturn | 592           |
| Data-EnvTrajs-MinReturn | -29.9         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 240           |
| Data-TimeEnvSampleProc  | 0.000803      |
| Data-TimeEnvSampling    | 3.51          |
| Iteration               | 49            |
| ItrTime                 | 128           |
| LossAfter               | -0.0069371634 |
| LossBefore              | 1.9550324e-08 |
| Model-TimeModelFit      | 47            |
| ModelSampler-n_times... | 2000000       |
| Policy-AverageAbsPol... | 1.0010096     |
| Policy-AverageDiscou... | 303           |
| Policy-AveragePolicyStd | 0.22406791    |
| Policy-AverageReturn    | 818           |
| Policy-MaxReturn        | 862           |
| Policy-MinReturn        | 770           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 23.2          |
| Policy-TimeAlgoOpt      | 1.39          |
| Policy-TimeSampleProc   | 0.198         |
| Policy-TimeSampling     | 75.4          |
| Policy-TimeStep         | 77            |
| Time                    | 5.07e+03      |
| n_timesteps             | 50000         |
-------------------------------------------

 ---------------- Iteration 50 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Training NNDynamicsModel - finished epoch 0 -- train loss: 0.3762  valid loss: 1.3580  valid_loss_mov_avg: 2.0302
Training NNDynamicsModel - finished epoch 1 -- train loss: 0.3670  valid loss: 1.3618  valid_loss_mov_avg: 2.0235
Training NNDynamicsModel - finished epoch 2 -- train loss: 0.3576  valid loss: 1.3532  valid_loss_mov_avg: 2.0168
Training NNDynamicsModel - finished epoch 3 -- train loss: 0.3436  valid loss: 1.3682  valid_loss_mov_avg: 2.0103
Training NNDynamicsModel - finished epoch 4 -- train loss: 0.3547  valid loss: 1.3565  valid_loss_mov_avg: 2.0038
Training NNDynamicsModel - finished epoch 5 -- train loss: 0.3364  valid loss: 1.3574  valid_loss_mov_avg: 1.9973
Training NNDynamicsModel - finished epoch 6 -- train loss: 0.3319  valid loss: 1.3544  valid_loss_mov_avg: 1.9909
Training NNDynamicsModel - finished epoch 7 -- train loss: 0.3313  valid loss: 1.3562  valid_loss_mov_avg: 1.9846
Training NNDynamicsModel - finished epoch 8 -- train loss: 0.3262  valid loss: 1.3604  valid_loss_mov_avg: 1.9783
Training NNDynamicsModel - finished epoch 9 -- train loss: 0.3171  valid loss: 1.3626  valid_loss_mov_avg: 1.9722
Training NNDynamicsModel - finished epoch 10 -- train loss: 0.3206  valid loss: 1.3543  valid_loss_mov_avg: 1.9660
Training NNDynamicsModel - finished epoch 11 -- train loss: 0.3240  valid loss: 1.3722  valid_loss_mov_avg: 1.9600
Training NNDynamicsModel - finished epoch 12 -- train loss: 0.3111  valid loss: 1.3567  valid_loss_mov_avg: 1.9540
Training NNDynamicsModel - finished epoch 13 -- train loss: 0.3214  valid loss: 1.3624  valid_loss_mov_avg: 1.9481
Training NNDynamicsModel - finished epoch 14 -- train loss: 0.3089  valid loss: 1.3563  valid_loss_mov_avg: 1.9422
Training NNDynamicsModel - finished epoch 15 -- train loss: 0.3000  valid loss: 1.3658  valid_loss_mov_avg: 1.9364
Training NNDynamicsModel - finished epoch 16 -- train loss: 0.3137  valid loss: 1.3635  valid_loss_mov_avg: 1.9307
Training NNDynamicsModel - finished epoch 17 -- train loss: 0.2991  valid loss: 1.3674  valid_loss_mov_avg: 1.9250
Training NNDynamicsModel - finished epoch 18 -- train loss: 0.3006  valid loss: 1.3593  valid_loss_mov_avg: 1.9194
Training NNDynamicsModel - finished epoch 19 -- train loss: 0.3029  valid loss: 1.3654  valid_loss_mov_avg: 1.9139
Training NNDynamicsModel - finished epoch 20 -- train loss: 0.3024  valid loss: 1.3697  valid_loss_mov_avg: 1.9084
Training NNDynamicsModel - finished epoch 21 -- train loss: 0.2886  valid loss: 1.3723  valid_loss_mov_avg: 1.9030
Training NNDynamicsModel - finished epoch 22 -- train loss: 0.2983  valid loss: 1.3641  valid_loss_mov_avg: 1.8977
Training NNDynamicsModel - finished epoch 23 -- train loss: 0.2975  valid loss: 1.3663  valid_loss_mov_avg: 1.8923
Training NNDynamicsModel - finished epoch 24 -- train loss: 0.2884  valid loss: 1.3730  valid_loss_mov_avg: 1.8872
Training NNDynamicsModel - finished epoch 25 -- train loss: 0.2936  valid loss: 1.3643  valid_loss_mov_avg: 1.8819
Training NNDynamicsModel - finished epoch 26 -- train loss: 0.2855  valid loss: 1.3774  valid_loss_mov_avg: 1.8769
Training NNDynamicsModel - finished epoch 27 -- train loss: 0.2898  valid loss: 1.3650  valid_loss_mov_avg: 1.8718
Training NNDynamicsModel - finished epoch 28 -- train loss: 0.2776  valid loss: 1.3730  valid_loss_mov_avg: 1.8668
Training NNDynamicsModel - finished epoch 29 -- train loss: 0.2868  valid loss: 1.3789  valid_loss_mov_avg: 1.8619
Training NNDynamicsModel - finished epoch 30 -- train loss: 0.2879  valid loss: 1.3767  valid_loss_mov_avg: 1.8570
Training NNDynamicsModel - finished epoch 31 -- train loss: 0.2928  valid loss: 1.3708  valid_loss_mov_avg: 1.8522
Training NNDynamicsModel - finished epoch 32 -- train loss: 0.2873  valid loss: 1.3734  valid_loss_mov_avg: 1.8474
Training NNDynamicsModel - finished epoch 33 -- train loss: 0.2731  valid loss: 1.3782  valid_loss_mov_avg: 1.8427
Training NNDynamicsModel - finished epoch 34 -- train loss: 0.2775  valid loss: 1.3717  valid_loss_mov_avg: 1.8380
Training NNDynamicsModel - finished epoch 35 -- train loss: 0.2642  valid loss: 1.3756  valid_loss_mov_avg: 1.8334
Training NNDynamicsModel - finished epoch 36 -- train loss: 0.2865  valid loss: 1.3711  valid_loss_mov_avg: 1.8287
Training NNDynamicsModel - finished epoch 37 -- train loss: 0.2808  valid loss: 1.3724  valid_loss_mov_avg: 1.8242
Training NNDynamicsModel - finished epoch 38 -- train loss: 0.2712  valid loss: 1.3790  valid_loss_mov_avg: 1.8197
Training NNDynamicsModel - finished epoch 39 -- train loss: 0.2760  valid loss: 1.3902  valid_loss_mov_avg: 1.8154
Training NNDynamicsModel - finished epoch 40 -- train loss: 0.2964  valid loss: 1.3716  valid_loss_mov_avg: 1.8110
Training NNDynamicsModel - finished epoch 41 -- train loss: 0.2706  valid loss: 1.3788  valid_loss_mov_avg: 1.8067
Training NNDynamicsModel - finished epoch 42 -- train loss: 0.2608  valid loss: 1.3747  valid_loss_mov_avg: 1.8024
Training NNDynamicsModel - finished epoch 43 -- train loss: 0.2610  valid loss: 1.3809  valid_loss_mov_avg: 1.7981
Training NNDynamicsModel - finished epoch 44 -- train loss: 0.2678  valid loss: 1.3812  valid_loss_mov_avg: 1.7940
Training NNDynamicsModel - finished epoch 45 -- train loss: 0.2711  valid loss: 1.3813  valid_loss_mov_avg: 1.7898
Training NNDynamicsModel - finished epoch 46 -- train loss: 0.2634  valid loss: 1.3794  valid_loss_mov_avg: 1.7857
Training NNDynamicsModel - finished epoch 47 -- train loss: 0.2581  valid loss: 1.3919  valid_loss_mov_avg: 1.7818
Training NNDynamicsModel - finished epoch 48 -- train loss: 0.2682  valid loss: 1.3814  valid_loss_mov_avg: 1.7778
Training NNDynamicsModel - finished epoch 49 -- train loss: 0.2577  valid loss: 1.3834  valid_loss_mov_avg: 1.7739

 ---------------- Grad-Step 500 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 501 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 502 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 503 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 504 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 505 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 506 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 507 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 508 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 509 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.507         |
| Data-EnvSampler-Poli... | 2.5           |
| Data-EnvTrajs-Averag... | 562           |
| Data-EnvTrajs-MaxReturn | 801           |
| Data-EnvTrajs-MinReturn | -46.6         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 313           |
| Data-TimeEnvSampleProc  | 0.00181       |
| Data-TimeEnvSampling    | 4.27          |
| Iteration               | 50            |
| ItrTime                 | 139           |
| LossAfter               | -0.012690794  |
| LossBefore              | 8.5830685e-09 |
| Model-TimeModelFit      | 53.9          |
| ModelSampler-n_times... | 2040000       |
| Policy-AverageAbsPol... | 1.0164012     |
| Policy-AverageDiscou... | 319           |
| Policy-AveragePolicyStd | 0.21979094    |
| Policy-AverageReturn    | 859           |
| Policy-MaxReturn        | 903           |
| Policy-MinReturn        | 806           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 26.3          |
| Policy-TimeAlgoOpt      | 1.52          |
| Policy-TimeSampleProc   | 0.174         |
| Policy-TimeSampling     | 79.1          |
| Policy-TimeStep         | 80.8          |
| Time                    | 5.21e+03      |
| n_timesteps             | 51000         |
-------------------------------------------
Training finished
