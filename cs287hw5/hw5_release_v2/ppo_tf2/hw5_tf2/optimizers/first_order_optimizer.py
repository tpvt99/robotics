from hw5_tf2.logger import loggerfrom hw5_tf2.utils.serializable import Serializableimport numpy as npimport tensorflow as tffrom hw5_tf2.utils import utilsclass FirstOrderOptimizer(Serializable):    """    Optimizer for first order methods (SGD, Adam)    Args:        tf_optimizer_cls (tf.train.optimizer): desired tensorflow optimzier for training        tf_optimizer_args (dict or None): arguments for the optimizer        learning_rate (float): learning rate        max_epochs: number of maximum epochs for training        tolerance (float): tolerance for early stopping. If the loss fucntion decreases less than the specified tolerance        after an epoch, then the training stops.        num_minibatches (int): number of mini-batches for performing the gradient step. The mini-batch size is        batch size//num_minibatches.        verbose (bool): Whether to log or not the optimization process    """    def __init__(            self,            tf_optimizer_cls=tf.keras.optimizers.Adam,            tf_optimizer_args=None,            learning_rate=1e-3,            max_epochs=1,            tolerance=1e-6,            num_minibatches=1,            verbose=False    ):        Serializable.quick_init(self, locals())        self._policy = None        if tf_optimizer_args is None:            tf_optimizer_args = dict()        tf_optimizer_args['learning_rate'] = learning_rate        self._tf_optimizer = tf_optimizer_cls(**tf_optimizer_args)        self._max_epochs = max_epochs        self._tolerance = tolerance        self._verbose = verbose        self._num_minibatches = num_minibatches        self._loss_object = None    def build_optimizer(self, loss_object, target, *args, **kwargs):        '''        :param loss_object: minimization objective        :param target: policy        :param args:        :param kwargs:        :return:        '''        self._loss_object = loss_object        self._policy = target    def loss(self, data):        """        Computes the value of the loss for given inputs        Args:            input_val_dict (dict): dict containing the values to be fed into the computation graph        Returns:            (float): value of the loss        """        return self._loss_object(data)    '''    def build_graph(self, loss, target, input_ph_dict, *args, **kwargs):        """        Sets the objective function and target weights for the optimize function        Args:            loss (tf_op) : minimization objective            target (Policy) : Policy whose values we are optimizing over            input_ph_dict (dict) : dict containing the placeholders of the computation graph corresponding to loss        """        assert isinstance(loss, tf.Tensor)        assert hasattr(target, 'get_params')        assert isinstance(input_ph_dict, dict)        self._target = target        self._input_ph_dict = input_ph_dict        self._loss = loss        self._train_op = self._tf_optimizer.minimize(loss, var_list=target.get_params())    '''    def optimize(self, data):        """        Carries out the optimization step        Args:        Returns:            (float) loss before optimization        """        # Todo: reimplement minibatches        loss_before_opt = None        # Eventhough we use .values(), the Graph can do the gradients on both self.mean_var.trainable_variables and        # self.policy_params because we already store trainable_variables into policy_params        for epoch in range(self._max_epochs):            if self._verbose:                logger.log("Epoch %d" % epoch)            with tf.GradientTape() as tape:                loss = self._loss_object(data)            gradients = tape.gradient(loss, self._policy.get_params().values()) # policy_params in an OrderedDict. Hence we have to use .values() here            self._tf_optimizer.apply_gradients(zip(gradients, self._policy.get_params().values()))            if not loss_before_opt:                loss_before_opt = loss        return loss_before_opt